[{"id":"db8c465178376c09ae44121bab3afad6","title":"flamingo论文精读","content":"Introduction1. 核心挑战与现有方法的局限性多模态机器学习领域面临一个核心的开放性挑战：如何构建能够仅凭少数几个标注样本就迅速适应新任务的模型。 这种快速学习能力是智能的一个关键特征，即在接收到简短指令后能学会执行新任务。 \n目前，计算机视觉领域最广泛应用的范式仍然是“预训练-微调”（pre-training and fine-tuning）。 这个过程通常包括两个阶段：\n\n预训练阶段：在一个大规模的、有监督的数据集上预先训练模型。\n微调阶段：在目标任务的特定数据集上对预训练好的模型进行参数微调。 \n\n然而，这种主流范式存在显著的缺点：\n\n数据依赖性强：成功的微调往往需要成千上万个为特定任务标注的数据点，获取这些数据成本高昂。\n\n高昂的调优成本：针对每个新任务，都需要进行仔细的超参数调整，这是一个繁琐且耗费计算资源的过程。\n\n资源密集：整个微调过程需要大量的计算资源。\n\n\n为了克服微调的限制，近年来出现了一些新的方法，但它们同样有其局限性：\n\n对比学习模型 (Contrastive Models)：像CLIP这样的多模态视觉语言模型，通过对比学习目标进行训练，实现了对新任务的“零样本”适应，无需微调。 它们的工作原理是为文本和图像学习一个共享的嵌入空间，并计算两者之间的相似度分数。 这种机制限制了它们的应用场景，主要适用于分类等选择题式的任务，即从一个预先给定的有限选项中做出选择。 关键的缺陷在于，这类模型无法生成语言，这使得它们不适用于更开放式的任务，例如视觉问答（VQA）或图像描述（Captioning），因为这些任务需要模型生成自由形式的文本答案。 \n\n视觉条件下的语言生成模型 (Visually-conditioned Language Generation Models)：虽然有一些研究探索了根据视觉输入生成文本的模型，但这些模型在数据量较少（即小样本）的情况下，尚未展现出足够好的性能。 \n\n\n图1：从Flamingo-80B获得的输入和输出精选示例。Flamingo可通过少样本提示快速适应各种图像/视频理解任务。此外，Flamingo原生支持多图像视觉对话。\n2. Flamingo模型的提出与核心能力为了解决上述挑战，本研究引入了一个名为Flamingo的视觉语言模型系列。 Flamingo的核心突破在于其卓越的小样本学习（few-shot learning）能力。 \n\n工作范式：Flamingo通过“提示（prompting）”来适应新任务。用户只需向模型提供几个包含输入/输出对的样本（例如，&lt;图片，对应描述&gt; 或 &lt;图片，问答对&gt;），模型就能理解任务要求并对新的查询图片或视频生成相应的文本输出。 1这种方式极大地降低了对大量标注数据的依赖。\n\n卓越的性能：在一个包含16个不同的视觉和语言任务的广泛评测中，Flamingo展现了最先进的小样本学习性能。 更为引人注目的是，在其中的6个任务上，Flamingo仅使用极少数的样本（例如32个），其性能就超越了在数千甚至数万倍任务专属数据上进行微调的现有最先进模型。 \n\n\n\n3. 设计思想：将大语言模型的能力扩展至多模态领域Flamingo的设计灵感主要来源于近年来在大型语言模型领域取得的巨大成功，例如GPT-3等模型展现出的强大的小样本学习能力。 \n\nLMs的小样本学习机制：一个强大的大型语言模型能够仅通过其文本接口来执行多种任务。具体做法是，将任务的几个示例（examples）和新的查询输入（query input）一起打包成一个文本提示（prompt），然后模型会自回归地生成一个续写，这个续写就是对查询的预测输出。 \n\n将该机制迁移至视觉任务：本研究证明，同样的方法论可以被成功地应用于图像和视频理解任务。 像分类、描述生成、视觉问答等任务，都可以被重新定义和构建为以视觉输入为条件的文本预测问题。 \n\n与纯语言模型的关键区别：与仅处理文本的LM不同，视觉任务需要模型能够处理一个多模态的提示（multimodal prompt），这个提示中包含了与文本交错在一起的图像或视频。 Flamingo模型的核心能力之一就是处理这种任意交错的视觉和文本序列。 \n\n\n4. Flamingo的架构理念与关键组件Flamingo模型是一个能够接收文本、图像、视频交错序列作为输入，并生成自由文本作为输出的视觉条件自回归文本生成模型。 其架构设计的核心理念是有效地连接和利用两个强大的、预训练好的互补模型，同时保留它们在各自领域学到的丰富知识。\n\n利用预训练模型：Flamingo架构的基础是两个预训练且被冻结（frozen）的模型：\n\n一个视觉模型，负责“感知”视觉场景。 \n\n一个大型语言模型，负责执行基本的推理和文本生成。 \n\n\n\n创新的桥接设计：在冻结的视觉和语言模型之间，引入了全新设计的、从零开始训练的架构组件。 这种“桥接”方式至关重要，因为它可以在不破坏原有模型知识（防止灾难性遗忘）的前提下，高效地将视觉信息融入到语言模型的处理流程中。 \n\n高效处理高分辨率视觉输入：为了处理高分辨率的图像或视频，Flamingo采用了一个基于Perceiver的架构。 该模块可以将视觉编码器产生的大量、可变数量的视觉特征，压缩并重采样成一小组固定数量的“视觉令牌（visual tokens）”。 这一设计极大地提高了处理效率。\n\n\n\n5. 训练策略的关键性大型语言模型的强大性能很大程度上归功于其在海量文本数据上的训练，这赋予了它们通用的文本生成能力，从而在接收到任务提示时表现出色。 \n\n训练数据的重要性：与此类似，本研究证明，Flamingo模型的训练方式对其最终的性能至关重要。 \n\n独特的训练数据：模型在一个精心挑选的、大规模、多模态的网络语料库上进行训练。这些数据的一个关键特征是包含了任意交错的文本和图像，这与传统的成对（image-text pair）数据有本质区别。 \n\n实现小样本能力的关键：正是这种在真实网络页面数据上的训练，才赋予了Flamingo强大的上下文小样本学习（in-context few-shot learning）能力。 经过这样的训练后，模型无需任何针对特定任务的微调，就能直接通过小样本提示的方式适应新的视觉任务。 \n\n\nApproach本节详细阐述了Flamingo模型的架构设计、工作原理与训练策略。Flamingo是一个视觉语言模型，其核心功能是接收以任意方式穿插的文本与图像/视频序列作为输入，并以自回归的方式生成自由形式的文本作为输出。模型设计的指导思想是高效地桥接两个强大的、已预训练好的独立模型——一个用于视觉感知的模型和一个用于语言理解与生成的模型，从而在不破坏各自预训练知识的前提下，实现强大的多模态处理能力。\n2.1 视觉处理与Perceiver Resampler模块视觉信息的处理是整个模型的第一步，其目标是将原始的像素输入转化为紧凑且固定长度的、可供语言模型利用的表征。\n\n视觉编码器 (Vision Encoder)：\n\n模型选择：采用了一个预训练好且在整个Flamingo训练过程中保持冻结的Normalizer-Free ResNet (NFNet) 模型（具体为F6版本）。冻结视觉编码器可以保留其强大的、泛化的视觉特征提取能力，并节省大量计算资源。\n预训练方式：该视觉编码器是在大规模图文对数据集上通过对比学习目标独立预训练的。这种训练方式旨在让模型学习到一种通用的视觉表示，使其能够理解图像内容并与文本描述对齐。\n特征输出：对于图像输入，编码器输出其网络末端的一个二维空间特征图。这个特征图保留了图像的空间信息，随后被展平为一维的特征序列。对于视频输入，首先以1帧/秒的速率对视频进行采样，然后独立地对每一帧进行编码，得到一系列的帧特征。为了融入时序信息，模型会为这些帧特征添加一个可学习的时序嵌入，形成一个三维时空特征网格。最后，这个三维特征同样被展平为一维序列，准备送入下一模块。\n\n\nPerceiver Resampler (感知器重采样器)：\n\n核心功能与目的：该模块是连接视觉编码器和冻结语言模型的关键桥梁。它的核心任务是接收来自视觉编码器的大量且可变长度的视觉特征（无论是来自高分辨率图像还是长视频），并将其压缩成少数固定数量的视觉令牌，在本研究中固定为64个。\n解决的痛点：直接将高维度的视觉特征输入到大型语言模型中进行注意力计算，其计算成本会非常高昂。Perceiver Resampler通过显著减少视觉令牌的数量，极大地降低了后续视觉-文本交叉注意力（cross-attention）的计算复杂度，使得整个模型更加高效。\n工作机制：其设计借鉴了Perceiver和DETR等模型的思想。它定义了一组预设数量的、可学习的潜在输入查询（latent input queries）。这些查询向量作为“信息汇总器”，通过一个Transformer结构中的交叉注意力机制，去“观察”和“查询”由视觉编码器生成的大量视觉特征。通过这个过程，它们将视觉特征中的核心信息“蒸馏”并吸收到自身中。最终，这些经过信息蒸馏的查询向量的输出，就构成了那一小组固定数量的视觉令牌。实验证明，这种方法比使用简单的多层感知机或标准的Transformer来进行特征池化效果更优。\n\n\n\n2.2 在视觉表征上对冻结语言模型进行条件化模型的文本生成能力由一个强大的、预训练好的冻结语言模型提供。为了让这个LM能够“看到”图像内容，需要将Perceiver Resampler产生的视觉令牌有效地融入其处理流程中。\n\n\nGATED XATTN-DENSE（门控交叉注意力-稠密连接）层：\n\n非侵入式集成：为了保留LM强大的预训练知识并防止“灾难性遗忘”（即模型在学习新知识时忘记旧知识），Flamingo不直接微调LM的参数。相反，它在原始LM的各个预训练层之间，插入了若干个全新的、从零开始训练的模块，即GATED XATTN-DENSE层。\n结构与功能：\n交叉注意力 (XATTN)：这是实现视觉与语言融合的核心。在这一层中，注意力机制的查询（Query, Q）来自于前一个冻结LM层的文本表征，而键（Key, K）和值（Value, V）则来自于Perceiver Resampler输出的视觉令牌。这使得在生成每一个文本词元时，模型都能够有针对性地“关注”视觉输入中的相关区域。\n稠密连接 (DENSE)：交叉注意力层之后连接一个标准的前馈神经网络（Feed-Forward Network），进行进一步的特征转换。\n门控机制 (GATED)：这是一个对训练稳定性和最终性能至关重要的设计。新添加模块的输出并不会直接与原始的文本表示相加。相反，它会经过一个tanh门控机制。该机制通过一个可学习的标量参数  来控制新模块的输出贡献。这个  被初始化为0。\n\n\n初始化优势：由于在初始化时 ，所以 tanh(α) 也为0，导致整个新添加模块的输出在训练开始时为零。这意味着，在训练初期，整个Flamingo模型的输出与那个未经改动的、冻结的LM完全相同。这保证了训练的稳定启动，避免了随机初始化的新层对强大预训练模型的干扰。随着训练的进行，模型会逐渐学习调整  的值，从而平滑地、自适应地将视觉信息融合进来。\n\n\n模型规模：研究团队基于Chinchilla系列语言模型构建了三种不同规模的Flamingo：Flamingo-3B, Flamingo-9B, 和 Flamingo-80B。在扩大模型尺寸时，主要是增加了冻结LM的参数量以及可训练的GATED XATTN-DENSE模块的数量，而视觉编码器和Perceiver Resampler的尺寸在不同规模的模型间保持不变。\n\n\n2.3 多视觉输入支持：逐图像/视频的注意力掩码为了让模型能够处理包含多个视觉输入的提示（例如，在小样本学习场景下，提示中包含多个&lt;图像, 文本&gt;对），并正确地将文本与对应的视觉输入关联起来，Flamingo采用了一种精巧的注意力掩码策略。\n\n工作机制：在进行文本到图像的交叉注意力计算时，模型采用了因果掩码（causal masking）。具体来说，当模型正在预测某个文本词元时，它的交叉注意力模块被限制为只能关注（attend to）在交错序列中紧邻于它之前的那个图像/视频所对应的视觉令牌。模型不能直接通过交叉注意力“回顾”更早出现的其他所有图像。\n信息流的保持：尽管交叉注意力被限制在单个最近的图像上，但对先前所有图像的信息依赖性并不会丢失。这些信息是通过语言模型内部的自注意力机制来间接维持的。当模型处理完一个&lt;图像, 文本&gt;对后，视觉信息已经影响了生成的文本，这些文本作为历史信息存储在LM的状态中。在后续步骤里，LM可以通过自注意力机制访问和利用这些包含了早前视觉信息的状态。\n核心优势：这种“单次只看一张图”的交叉注意力方案，不仅计算高效，更重要的是赋予了模型极佳的泛化能力。它使得模型可以无缝地处理任意数量的视觉输入，即使在训练时接触的图像数量有限（例如，训练时最多使用5张图），在推理时也能从多达32个图文对中获益。\n\n2.4 在混合视觉与语言数据集上的训练Flamingo的小样本学习能力严重依赖于其独特的训练数据和策略。模型在一个混合了三种从网络上爬取的数据集上进行训练，未使用任何专为机器学习目的而人工标注的数据。\n\n数据集构成：\n\nM3W (MultiModal MassiveWeb)：这是一个包含约4300万个网页的交错图文数据集。通过解析网页的DOM树结构，将图像以&lt;image&gt;标签的形式插入到其在原文中相应位置的文本流中，从而构建出自然的、图文混排的训练样本。这是训练模型理解上下文和进行小样本学习的关键。\n图文对数据集：这部分由两块组成，一是公开的ALIGN数据集（18亿图文对），二是团队自己收集的、描述更长更优质的LTIP数据集（3.12亿图文对）。\n视频文本对数据集 (VTP)：一个包含2700万个短视频及其文本描述的数据集。\n\n\n语法对齐与多目标训练：\n\n为了统一训练格式，所有图文对和视频文本对数据都被处理成与M3W相似的语法，即在文本描述前后分别加上&lt;image&gt;和&lt;EOC&gt;（end of chunk）特殊标记。\n论文通过最小化给定视觉输入时每个数据集的文本期望负对数似然加权和来训练模型： , 其中是输入文本的第个语言标记，是前面的标记集合，是交错序列中先于标记的图像/视频集合，和分别表示第个数据集及其权重。调整每个数据集的权重是提升性能的关键。我论文对所有数据集的梯度进行累积，实验表明该方法优于“轮询”策略。\n在优化策略上，团队发现梯度累积的方式，即计算完所有数据集的梯度后再进行一次统一的参数更新，比轮流在单个数据集上训练的“round-robin”方法效果更佳。\n\n\n\n2.5 利用小样本上下文学习进行任务适配模型一旦训练完成，就可以通过上下文学习（in-context learning）的方式快速适应新的视觉任务，而无需任何参数更新。\n\n提示构建 (Prompting)：为了解决一个新任务，用户需要构建一个多模态提示。这个提示由若干个“支持样本”和一个“查询样本”组成。例如，可以这样构建提示：&lt;图片1&gt; &lt;答案1&gt; &lt;图片2&gt; &lt;答案2&gt; ... &lt;查询图片&gt;。模型在看到这个提示后，会续写出针对查询图片的答案。\n评估方式：\n对于开放式任务（如VQA），使用集束搜索解码策略来生成最可能的自由文本答案。\n对于封闭式任务（如多项选择），模型会分别计算每个选项作为答案的对数似然概率（log-likelihood），并选择概率最高的那个作为最终答案。\n\n\n零样本泛化：研究还探索了一种特殊的零样本设置，即在提示中只提供任务的纯文本示例（不附带图像），以测试模型是否能仅从文本描述中理解任务的格式和要求。\n\nExperiments本部分旨在通过一系列广泛的实验来评估Flamingo模型的性能，核心目标是验证其在多样化且具有挑战性的任务上快速适应的能力。\n实验设置与评估策略\n\n评测基准：为了全面评估模型，实验覆盖了16个当前流行的多模态基准数据集。这些数据集涵盖了图像和视频理解的多个方面，包括图像描述、视频描述、视觉问答、视频问答、视觉对话以及多项选择题等。\n\n开发集与留出集（Held-out Set）：为了保证评估的公正性和科学性，实验将这16个基准分为了两组：\n\n开发集（DEV Set）：包含5个基准（COCO, OKVQA, VQAv2, MSVDQA, VATEX）。这组数据集在研究过程中被用来验证模型的设计决策和调整超参数。研究者承认，由于模型在开发阶段“看到”了这些任务，其在这些基准上的最终性能评估可能存在偏向性（即可能被高估），但这种做法在领域内是普遍的。\n留出集：包含其余的11个基准。这组数据集在模型设计和超参数选择的整个过程中都未被使用。它们仅在最后阶段被用来评估模型的最终性能，从而为模型的小样本学习能力提供一个无偏的、更可信的估计。\n\n\n数据划分：为了在开发集上进行严谨的评估并避免数据泄露，每个开发集基准都被划分为四个子集：验证集的支持样本（用于开发阶段构建提示）、验证集的查询样本（用于开发阶段评估）、测试集的支持样本（用于最终报告构建提示）、测试集的查询样本（用于最终报告评估）。对于留出集，则只需要测试集的支持和查询样本。\n\n评估超参数：为了证明模型的通用性，所有评估用的超参数（如解码策略等）在全部16个基准上都保持固定。根据任务的性质（例如是生成式还是选择式），会采用四种预设的提示模板中的一种。\n\n\n3.1 小样本学习性能这是实验的核心部分，旨在衡量模型在仅有少量标注样本的情况下学习新任务的能力。\n\n与先前小样本方法的对比：实验结果表明，在所有16个评测基准上，Flamingo的性能都显著超越了以往所有已发表的零样本或小样本方法。这一成就仅需每个任务提供极少数（例如4个）的示例即可实现，充分展示了模型高效且实用的任务适应能力。\n\n\n\n与完全微调（Fine-tuned）方法的对比：更引人注目的是，Flamingo的性能不仅在小样本领域领先，甚至能与那些在成千上万、乃至数十万个任务专属标注数据上进行完全微调的当前最先进（SOTA）模型相媲美。在其中的6个任务上，Flamingo仅凭一个通用的、未经微调的模型和32个任务样本，就超越了这些经过大量数据微调的SOTA模型。\n\n泛化能力验证：模型在11个留出基准上的强劲表现，证实了其设计和训练方法的泛化能力。这表明模型的优异性能并非过拟合于开发集，而是具备广泛适用性的。\n\n规模效应分析（Scaling Analysis）：\n\n模型参数规模：性能随着模型参数量的增加而稳步提升，即Flamingo-80B优于Flamingo-9B，后者又优于Flamingo-3B。这与在大型语言模型领域观察到的“规模法则”（Scaling Law）相一致，即更大的模型通常具备更强的学习能力。\n样本数量（Number of Shots）：对于同一个模型，其性能随着在提示中提供的上下文样本（shots）数量的增加而提高。\n协同效应：研究发现，最大的模型（Flamingo-80B）能更好地利用更多的上下文样本。这表明模型规模和上下文信息量之间存在协同效应，更大的模型能更有效地从示例中学习。\n架构灵活性的体现：一个非常关键的发现是，尽管模型在训练阶段最多只接触过包含5张图像的序列，但在推理（评估）时，它却能有效利用多达32张图像或视频的上下文信息并持续提升性能。这有力地证明了Flamingo架构（特别是其逐图像的注意力掩码机制）的灵活性和卓越的泛化能力。\n\n\n\n3.2 Flamingo作为预训练模型进行微调的性能尽管小样本学习是核心焦点，本部分也探索了当有充足标注数据时，将Flamingo作为预训练模型进行传统微调的潜力。\n\n微调方法：研究团队对最大规模的Flamingo模型，在有大量标注数据的任务上进行微调。微调过程采用了一个较短的训练周期和较小的学习率。一个关键的改动是，在微调期间解冻了视觉主干网络。这使得模型的视觉部分也能针对特定任务进行调整，例如适应更高分辨率的图像输入。\n\n微调结果：通过微调，模型的性能在之前小样本学习的基础上得到了进一步提升。在那些之前通过小样本学习未能达到SOTA的9个任务中，微调后的Flamingo在其中的5个任务上（VQAv2, VATEX, VizWiz, MSRVTTQA, HatefulMemes）刷新了最先进记录（SOTA）。\n\n\n\n3.3 消融研究为了理解模型各个组件和设计选择的重要性，研究者进行了一系列消融实验。这些实验主要在较小的Flamingo-3B模型上进行，以节省计算成本。\n\n训练数据组合的重要性：\n\n移除交错图文数据集会导致模型性能出现超过17%的灾难性下降，这证明了在自然混排的图文数据上进行训练对于培养小样本学习能力至关重要。\n移除传统的图文对数据集同样会使性能下降近10%，这表明交错数据和成对数据是互补的，两者都不可或缺。\n移除视频-文本数据集会对所有视频相关任务的性能产生负面影响。\n\n\n视觉条件化架构的关键设计：\n\n门控机制：在融合视觉和文本信息时使用的tanh门控机制至关重要。移除这个机制不仅导致性能下降4.2%，还会引发训练过程的不稳定。\n交叉注意力架构：实验证明，本文提出的GATED XATTN-DENSE架构优于其他可替代的方案，如vanilla交叉注意力或一些“嫁接”（grafting）方法。\n\n\n计算与性能的权衡：\n\n交叉注意力频率：在语言模型的每一层之间都插入新的交叉注意力模块能获得最佳性能，但这会显著增加训练的参数量和时间复杂度。实验发现，每隔4层插入一个模块是一个极佳的权衡点，它能将训练速度提升66%，而整体性能仅有1.9%的微小下降。基于这个发现，更大的模型采用了这种稀疏插入策略以在硬件限制下实现最优配置。\nResampler架构：Perceiver Resampler在性能和速度上均优于使用普通MLP或Transformer作为重采样器的替代方案。\n\n\n视觉编码器的影响：使用一个强大的视觉编码器非常重要。团队自己预训练的NFNet-F6编码器显著优于公开的CLIP ViT-L/14模型，证明高质量的视觉特征是模型性能的基石。\n\n冻结语言模型的必要性：这是整个方法论中最关键的发现之一。\n\n如果从零开始训练语言模型部分，性能会暴跌12.9%。\n更重要的是，即便是微调（而非冻结）预训练好的语言模型，也会导致8%的显著性能下降。这清晰地揭示了“灾难性遗忘”现象：在适应新的多模态目标时，语言模型会忘记其预训练阶段学到的宝贵的、通用的语言知识。因此，冻结语言模型是保证性能的必要手段，是一种比将纯文本预训练数据混入多模态训练中更优的策略。\n\n\n\n\nRelated Work本节将Flamingo模型置于现有研究的广阔背景之下，阐述其与相关工作的联系与区别，主要涵盖三个领域：语言建模与小样本适应、视觉与语言的交叉研究，以及网络规模的训练数据集。\n1. 语言建模与小样本适应\n技术背景：近年来，基于Transformer架构的语言模型取得了巨大进步，“预训练-再适应”已成为标准范式。Flamingo正是构建在这一坚实基础之上，其核心语言模块采用了强大的Chinchilla 70B大型语言模型。\n\n模型适应技术的多样性：将预训练好的大型语言模型适配到新任务上有多种技术路径：\n\n适配器模块（Adapter Modules）：在预训练模型的层与层之间插入一些小型的、可训练的神经网络模块，在适配新任务时只训练这些适配器，而保持主干模型冻结。\n部分参数微调：只微调模型中一小部分的参数（例如，只微调偏置项 bias），在保持大部分参数不变的情况下实现任务适配。\n提示优化（Prompt Optimization）：保持模型完全不变，而是通过梯度下降等方法来学习最优的、能够引导模型产生正确输出的输入提示。\n上下文学习（In-context Learning）：这是Flamingo采用的思路，其灵感直接来源于GPT-3等模型。这种方法无需任何梯度更新或参数修改，仅通过在模型的输入中提供几个任务示例，就能引导模型在推理时执行新任务。\n\n\nFlamingo的选择与定位：相较于其他需要复杂梯度优化的方法，Flamingo选择了更为简洁的上下文学习路径。它避开了基于度量学习（metric learning，旨在学习一个好的样本间相似度函数）或元学习（meta-learning，即“学会如何学习”，旨在让模型能从少量数据中快速学习）等更为复杂的少样本学习框架，转而将纯文本领域的上下文学习成功地推广到了多模态领域。\n\n\n2. 当语言与视觉相遇\nBERT的深远影响：语言模型（尤其是BERT）的成功极大地启发了视觉语言领域的研究。大量V-L模型借鉴了BERT的架构，使用Transformer来融合视觉和文本特征。然而，这些模型与Flamingo的一个根本区别在于，它们大多需要在下游任务上进行微调才能获得良好性能，而Flamingo则专注于无需微调的小样本学习。\n\n对比学习模型：这是V-L领域的另一大分支（如CLIP）。这类模型通过对比学习来对齐图像和文本的表示，从而计算它们之间的相似度。虽然Flamingo的视觉编码器本身是使用对比学习预训练的，但Flamingo模型整体的功能远超于此。对比学习模型只能进行“打分”，无法生成自由形式的文本，而Flamingo的核心能力之一正是生成性。\n\n自回归视觉语言模型：Flamingo属于能够自回归生成文本的VLM家族。在它出现的同时期，也有其他一些工作探索了将多种视觉任务统一表述为文本生成问题的范式。\n\n基于冻结语言模型的研究趋势：为了防止在多模态训练中破坏大型语言模型预训练好的强大能力（即“灾难性遗忘”），近期的一系列工作开始探索冻结语言模型的方案。Flamingo正是这一思想的践行者和集大成者。\n\nFlamingo的核心创新：尽管存在上述种种相关工作，Flamingo的独特性和核心创新在于，它是首个能够处理任意交错（arbitrarily interleaved）的图像、视频和文本序列的语言模型。这与之前大多数只能处理单个图像/视频与文本对的模型相比，是一个质的飞跃，使其能够处理更复杂、更自然的真实世界多模态场景。\n\n\n3. 网络规模的视觉与语言训练数据集\n数据瓶颈：高质量、人工标注的视觉语言数据集（如COCO, VQA）规模通常在数万到数十万级别，获取成本高昂，这限制了模型的扩展性。\n网络数据的利用：为了突破这一瓶颈，许多研究转向从互联网上自动爬取海量的、自然存在的图文对数据。\nFlamingo的贡献：Flamingo不仅利用了这种大规模的图文对数据，还进一步证明了训练数据的形态至关重要。它的一个关键贡献是证明了在包含交错图文的完整网页上进行训练的巨大价值。这种将网页视为一个单一、连贯的多模态序列的训练方式，是其强大上下文学习能力的关键来源。\n与同期工作的比较：同期的CM3模型也使用了网页数据进行训练。但两者目标不同：CM3旨在生成HTML标记语言，而Flamingo将任务简化为生成纯文本；在评估上，CM3更侧重于语言任务，而Flamingo的重点是验证在各类视觉任务上的小样本学习能力。\n\n","slug":"flamingo论文精读","date":"2025-07-05T02:32:50.000Z","categories_index":"","tags_index":"多模态,小样本","author_index":"犬夜叉"},{"id":"5cac6105e8bc407ec91f6896ed602917","title":"ALBEF论文精读","content":"\n\n\n\n\n\n摘要\n大规模视觉语言表示学习已在各类视觉语言任务上展现出显著改进。现有大多数方法采用基于Transformer的多模态编码器来联合建模视觉标记（基于区域的图像特征）和文本标记。由于视觉标记与文本标记处于未对齐的空间，多模态编码器难以学习图像-文本的交互关系。在本文中，我们提出一种对比损失，用于在通过跨模态注意力融合图像和文本表示之前先进行对齐（即ALBEF），这使得视觉语言表示学习更具语义根基。与大多数现有方法不同，我们的方法无需边界框标注，也不需要高分辨率图像。为提升从噪声Web数据中学习的能力，我们提出动量蒸馏（Momentum Distillation），这是一种自训练方法，通过动量模型生成的伪目标进行学习。我们从互信息最大化的视角对ALBEF进行了理论分析，表明不同训练任务可解释为为图像-文本对生成“视图”的不同方式。ALBEF在多个下游视觉语言任务上实现了SOTA性能：在图像-文本检索任务中，ALBEF超越了在数据规模大若干数量级的数据集上预训练的方法；在VQA和NLVR2任务中，ALBEF相比SOTA方法分别实现了2.37%和3.84%的绝对性能提升，同时推理速度更快。代码和模型见https://github.com/salesforce/ALBEF。\n\n\n论文链接：[https://arxiv.org/abs/2107.07651]\nIntroduction1. 视觉-语言预训练（VLP）的现状与挑战视觉-语言预训练（VLP）旨在通过大规模图像-文本对学习多模态表示，以提升下游任务性能。现有主流方法（如LXMERT、UNITER、OSCAR）依赖预训练的物体检测器提取基于区域的图像特征，并通过多模态编码器融合图像与文本标记。然而，这类方法存在以下关键局限性：  \n\n（1）模态特征未对齐：图像区域特征与文本标记处于不同语义空间，多模态编码器难以有效建模跨模态交互关系。  \n（2）物体检测器的高成本：预训练需边界框标注，推理时依赖高分辨率图像。  \n（3）噪声数据过拟合：主流图像-文本数据集收集自网络，存在大量噪声，传统预训练目标（如掩码语言建模MLM）易过拟合，导致模型泛化能力下降。  \n\n2. ALBEF框架的核心解决方案针对上述问题，论文提出 ALign BEfore Fuse（ALBEF）框架，其核心思路为：  \n\n（1）对齐先行（ALign）：在融合图像-文本特征前，通过图像-文本对比学习（ITC）对齐单模态表示。  \n使用无检测器的图像编码器（ViT）和文本编码器（BERT）独立编码，通过对比损失学习跨模态相似性函数，将图像与文本映射到共享语义空间，降低多模态编码器的建模难度。  \n引入动量编码器维护历史特征队列，通过InfoNCE损失最大化正样本互信息，并利用对比硬负样本挖掘提升样本多样性。  \n\n\n（2）动量蒸馏（Momentum Distillation, MoD）：应对噪声数据过拟合问题。  \n通过动量模型（参数指数滑动平均）生成软伪标签，作为额外监督信号，允许模型学习与噪声标注不同但合理的输出，增强鲁棒性。  \n损失函数结合原始监督信号与伪标签的KL散度，公式为： , 其中，为蒸馏权重。  \n\n\n\n3. 理论分析：互信息最大化视角从互信息（MI）最大化角度解释ALBEF的有效性：  \n\nITC和MLM的视图生成：ITC将图像和文本视为同一数据对的两种“视图”，通过模态分离最大化视图间互信息；MLM通过掩码文本生成视图，利用图像和上下文预测原词，增强跨模态依赖。  \n动量蒸馏的视图增强：动量模型生成语义相似的新视图，迫使基础模型学习对语义保持不变性的表示，进一步提升互信息下界。  \n\n4. 实验效果与创新点\n性能突破：ALBEF在多个下游任务中超越现有SOTA方法：  \n图像-文本检索：在Flickr30K和COCO上超越CLIP、ALIGN，零样本迁移能力显著。  \nVQA和NLVR2：相比SOTA方法VILLA，绝对性能提升分别达2.37%和3.84%，且推理速度快10倍以上。  \n弱监督视觉定位：在RefCOCO+上通过Grad-CAM可视化验证，模型可准确定位物体、属性和关系。  \n\n\n创新优势：  \n无检测器设计：摆脱对物体检测器的依赖，支持低分辨率输入（预训练256×256，微调384×384），降低计算成本。  \n噪声鲁棒性：动量蒸馏有效利用大规模噪声Web数据，提升模型泛化能力。  \n统一单模态与多模态能力：结合对比学习（CLIP类方法）和跨模态推理（Transformer类方法），兼顾检索与复杂推理任务。  \n\n\n\n5. 结论与资源ALBEF通过“对齐-融合”框架、动量蒸馏和互信息理论，实现了视觉-语言表示学习的高效性与鲁棒性，为大规模噪声数据的利用提供了新范式。\nrelated work2.1 视觉-语言表示学习（Vision-Language Representation Learning）现有视觉语言表示学习方法主要分为两类：  \n\n基于多模态编码器的方法  \n代表方法：LXMERT、UNITER、OSCAR等，采用Transformer架构联合建模图像区域特征与文本标记，通过掩码语言建模（MLM）、图像-文本匹配（ITM）等任务学习跨模态交互。  \n优势：在需要复杂推理的任务（如VQA、NLVR²）中表现优异。  \n局限性：依赖预训练物体检测器提取图像区域特征，需高分辨率图像（如600×1000），计算成本高；部分方法（如ViLT）虽移除检测器但性能下降。  \n\n\n基于单模态编码器的对比学习方法  \n代表方法：CLIP、ALIGN，通过对比损失在大规模噪声数据中学习图像和文本的独立嵌入空间，擅长图像-文本检索任务。  \n优势：无需物体检测器，可处理大规模Web数据，零样本迁移能力强。  \n局限性：缺乏对图像-文本复杂交互的建模能力，难以应对需要细粒度推理的任务。  \n\n\n\nALBEF的定位：  \n\n融合两类方法的优势，通过单模态编码器对比对齐（ITC损失）和多模态编码器交互建模（MLM、ITM损失），实现检索与推理任务的平衡。  \n无需物体检测器，采用ViT作为图像编码器，降低计算成本，同时支持低分辨率输入（预训练256×256，微调384×384）。  \n\n2.2 知识蒸馏（Knowledge Distillation）\n传统知识蒸馏：从预训练的“教师模型”向“学生模型”迁移知识，通常通过匹配输出概率（如KL散度）实现，适用于模型压缩或跨任务迁移。  \n在线蒸馏：同时训练多个模型，利用模型集合作为教师，如深度互学习。  \n动量蒸馏（MoD）的创新：  \n属于在线自蒸馏，通过维护模型参数的指数滑动平均（动量模型）生成伪标签，作为额外监督信号。  \n与半监督学习（如Mean Teacher）和对比学习（如MoCo）相关，但首次将动量蒸馏应用于视觉-语言预训练，缓解噪声数据过拟合问题。  \n理论与实验表明，动量蒸馏可提升模型在噪声数据中的鲁棒性，且适用于多种下游任务（包括干净标注数据）。 \n\n\n\nALBEF Pre-training3.1 模型架构ALBEF的架构由三个核心模块组成，如图所示：\n\n图像编码器：\n结构：采用12层视觉Transformer（ViT-B/16），输入图像尺寸为256×256（预训练）或384×384（微调），通过16×16的Patch划分，最终输出序列嵌入，其中是用于全局表示的[CLS]标记，是Patch嵌入。  \n初始化：权重来自ImageNet-1k预训练的ViT模型，提升图像语义理解能力\n\n\n文本编码器：\n结构：6层Transformer，基于BERT-base的前6层初始化，输入文本通过Tokenization生成标记序列，输出嵌入，其中  为文本全局表示。\n\n\n多模态编码器：\n结构：6层Transformer，基于BERT-base的后6层初始化，接收图像编码器的输出和文本编码器的输出，通过 跨注意力机制（Cross-Attention）逐层融合图像与文本特征，实现跨模态交互。\n核心操作：在每一层，图像特征作为键（Key）和值（Value），文本特征作为查询（Query），通过跨注意力学习文本对图像的细粒度对齐（如定位图像中的物体对应文本描述）。\n\n\n\n\n\n\n\n\n\n\n为什么文本编码器使用 BERT-base 的前六层初始化，而多模态编码器使用 BERT-base 后六层来初始化？\nALBEF 的文本编码器和多模态编码器的分层初始化策略，旨在分离单模态理解与跨模态交互的能力，具体原因如下：\n\nBERT 的层功能分工：\nBERT 的前几层更擅长捕捉单模态文本的局部语义和语法结构（如词级关联、短语结构），适合作为文本编码器的初始化，专注于文本单模态表示的学习。\nBERT 的后几层更擅长建模全局语义依赖和跨标记交互（如句子级语义整合），与多模态编码器需要处理图像 - 文本跨模态交互的需求更匹配。\n\n\n单模态与多模态的功能解耦：\n文本编码器：仅处理文本输入，需强化单模态文本理解能力，因此复用 BERT 前六层的文本建模能力。\n多模态编码器：需融合图像与文本特征，其核心是跨注意力机制（Cross-Attention），而 BERT 后六层的自注意力机制（Self-Attention）已具备建模复杂交互的能力，微调后可适配跨模态场景。\n\n\n参数效率与迁移学习：\n利用 BERT 预训练的权重初始化，避免从头训练带来的不稳定，同时通过分模块初始化（前六层 vs. 后六层）实现单模态到多模态的渐进式能力扩展。\n实验表明，这种分层初始化策略在 VQA、NLVR² 等需要跨模态推理的任务中，比随机初始化或全层复用性能更优。\n\n\n\n\n\n3.2 预训练目标（Pre-training Objectives）ALBEF采用三个预训练目标，联合优化单模态对齐与多模态交互：  \n3.2.1 图像-文本对比学习（Image-Text Contrastive Learning, ITC）\n目标：在融合前对齐图像和文本的单模态表示，学习跨模态相似性函数，其中  是将全局嵌入映射到256维空间的线性层。  \n动量编码器与队列机制：  \n维护两个队列存储动量编码器的历史特征（受MoCo启发），动量编码器参数通过指数滑动平均（EMA）更新：, 其中  为动量系数，为当前模型参数，为动量模型参数。  \n队列中存储最近  个图像-文本对的动量特征  和 。  \n\n\n相似度计算：  \n图像到文本相似度：\n文本到图像相似度：\n\n\n对比损失（InfoNCE）：\n\n\n\n其中为one-hot真实标签，为可学习的温度参数，为交叉熵损失。 \n3.2.2 掩码语言建模（Masked Language Modeling, MLM）\n目标：利用图像信息预测文本中被掩码的标记，增强跨模态语义关联。  \n操作：随机掩码文本标记（15%概率），其中80%用[MASK]替换，10%用随机词替换，10%保持不变。  \n损失函数：其中为掩码文本，为模型预测的标记概率，为真实标记的one-hot向量。  \n\n3.2.3 图像-文本匹配（Image-Text Matching, ITM）\n目标：判断图像-文本对是否匹配，引入对比硬负样本挖掘提升训练难度。  \n硬负样本挖掘：在批次内根据对比相似度分布采样硬负样本：对每个图像，选择与该图像相似度最高的非匹配文本作为硬负样本；对每个文本，选择相似度最高的非匹配图像作为硬负样本。  \n损失函数： , 其中为匹配概率，为真实标签。  \n\n总损失函数：\n3.3 动量蒸馏（Momentum Distillation, MoD）\n动机：缓解Web数据噪声导致的过拟合，利用动量模型生成软伪标签作为额外监督。  \n核心思想：当前模型输出与动量模型的伪标签对齐，通过KL散度最小化实现知识蒸馏。  \n公式推导：  \nITC的动量蒸馏损失： , 其中为动量模型计算的软伪概率，为蒸馏权重。  \nMLM的动量蒸馏损失： , 其中 为动量模型预测的掩码标记概率。  \n\n\n效果:  伪标签允许模型学习与噪声标注不同但语义合理的输出，增强鲁棒性（如图所示，伪标签覆盖了真实文本未描述的视觉概念）。\n\n\n3.4 预训练数据集（Pre-training Datasets）\n数据集组成：  \nWeb数据：Conceptual Captions (295万图像)、SBU Captions (86万图像)、Conceptual12M (1006万图像，噪声较大)。  \n领域内数据：COCO (11.3万图像)、Visual Genome (10万图像)。  \n\n\n数据规模：  \n基础数据集：400万图像，510万图像-文本对。  \n扩展数据集：1410万图像（含Conceptual12M），用于验证模型对大规模噪声数据的适应性。  3.5 实现细节（Implementation Details）\n\n\n训练配置：  \n8块NVIDIA A100 GPU，批次大小512，训练30 epoch。  \n优化器：AdamW，权重衰减0.02，学习率余弦衰减（初始，热身1000步）。  \n\n\n图像预处理：随机裁剪256×256，应用RandAugment（不含颜色变换，避免与文本颜色信息冲突）。  \n微调细节：图像分辨率提升至384×384，通过插值适配ViT的位置编码。  \n\nA mutual Information Maximization Perspective4.1 互信息（MI）与视觉-语言表示学习核心思想：互信息衡量两个随机变量的依赖程度，最大化互信息可使模型学习到更具语义关联的跨模态表示。在视觉-语言任务中，图像-文本对的不同“视图”（View）可视为随机变量，通过最大化视图间的互信息，模型能捕捉更鲁棒的语义不变性。数学定义：设随机变量  和  为图像-文本对的两种视图，互信息  表示为：  最大化  可通过最小化InfoNCE损失实现，该损失是互信息的一个下界：  其中  为视图  和  的相似度得分， 包含正样本  和负样本集合。\n4.2 ITC损失的互信息解释ITC的视图定义：将图像  和文本  视为同一对的两个独立视图，通过对比学习最大化它们的互信息。公式推导：ITC损失可重写为对称的InfoNCE损失：  \n\n正样本：真实匹配的图像-文本对 。  \n负样本：队列中存储的  个不匹配样本  和 。  \n物理意义：通过对比学习，迫使匹配对的视图在特征空间中接近，非匹配对远离，从而最大化图像与文本视图的互信息。4.3 MLM损失的互信息解释MLM的视图定义：将“掩码词”与“图像+掩码文本”视为两种视图，通过预测掩码词最大化其依赖关系。公式推导：MLM损失可重构为：  \n查询向量：掩码词的真实标记  通过嵌入层映射为 。  \n键向量：多模态编码器输出的掩码上下文表示 ，包含图像和未掩码文本的信息。  \n物理意义：将掩码词预测视为从上下文视图中检索正确标记，通过最大化掩码词与上下文的互信息，增强跨模态语义关联。4.4 动量蒸馏（MoD）的互信息增强核心机制：动量模型生成的伪目标相当于引入新的“语义相似视图”，迫使基础模型学习对视图变化不变的表示。公式分析：以ITC的动量蒸馏损失为例：   , 其中伪目标  由动量模型的相似度  计算得到：  \n新视图生成：动量模型通过历史参数平均生成更平滑的特征表示， 对应不同于当前模型的视图相似度，引入语义等价但细节不同的负样本。\n互信息提升：最小化  迫使当前模型匹配动量模型的视图分布，相当于最大化当前模型与动量模型生成的新视图之间的互信息，增强表示的不变性。4.5 统一理论视角：视图生成与不变性学习\n\n\n\n\n\n任务\n视图生成方式\n互信息优化目标\n\n\n\n\nITC\n模态分离（图像vs.文本）\n最大化单模态视图间的互信息\n\n\nMLM\n词掩码（完整文本vs.掩码文本+图像）\n最大化掩码词与上下文视图的互信息\n\n\nMoD\n动量模型生成语义相似视图\n最大化当前模型与动量视图的互信息\n\n\n\n\n下游视觉-语言任务与实验结果ALBEF预训练模型被适配到五类下游任务，展示了其在检索、推理、生成和定位任务中的泛化能力。以下是各任务的详细分析：\n5.1 图像-文本检索（Image-Text Retrieval）\n任务目标：实现图像与文本的相互检索，包括图像到文本检索（TR）和文本到图像检索（IR）。  \n数据集：  \nFlickr30K：29k训练图像，1k验证/测试图像，每图像对应5句文本。  \nCOCO：113k训练图像，5k验证/测试图像，每图像对应5句文本。  \n\n\n模型调整：  \n微调时联合优化 ITC损失（单模态对齐）和 ITM损失（多模态匹配）。  \n推理时先用ITC计算特征相似度筛选Top-k候选，再用ITM进行细粒度排序，大幅提升推理速度（仅计算少量候选的ITM分数）。  \n\n\n关键结果：  \n零样本迁移：在Flickr30K上，仅用COCO训练的模型实现TR/IR平均召回率94.1%/82.8%，超越CLIP和ALIGN。  \n微调性能：14M预训练图像下，Flickr30K的TR/IR平均召回率达98.70%/94.07%，COCO上达97.2%/90.5%，显著优于UNITER、OSCAR等方法。  \n\n\n\n\n5.2 视觉蕴含（Visual Entailment, SNLI-VE）任务\n任务目标：判断图像与文本的关系（蕴含、中立、矛盾），属于三分类问题。\n数据集：SNLI-VE，基于SNLI文本和Flickr30K图像构建，含29.8k训练样本。\n模型调整：使用多模态编码器的[CLS]标记输出，通过多层感知机（MLP）分类。\n关键结果：\n14M预训练图像下，测试集准确率达80.91%，相比VILLA提升1.88%，验证细粒度视觉推理能力。\n\n\n\n\n5.3 视觉问答（Visual Question Answering, VQA）\n任务目标：给定图像和问题，生成自然语言答案。\n数据集：VQA2.0，含83k训练图像，41k验证图像，81k测试图像，问题涉及物体、属性、关系等。\n模型调整：\n附加6层Transformer解码器，通过自回归生成答案，解码器权重由多模态编码器初始化。\n限制答案生成范围为3,192个候选词，确保与现有方法可比。\n\n\n关键结果：\n14M预训练图像下，test-std准确率76.04%，相比VILLA提升2.37%，推理速度快10倍以上。\nGrad-CAM可视化显示，模型能准确聚焦问题相关区域。\n\n\n\n\n5.4 自然语言视觉推理（NLVR²）\n任务目标：判断文本是否同时描述两张图像，属于二分类问题。  \n数据集：NLVR²，含50k训练图像对，8.5k验证/测试图像对。  \n模型调整：  \n多模态编码器每层复制两个Transformer块，分别处理两张图像的嵌入，共享跨注意力参数。  \n预训练阶段新增文本分配（TA）任务：将文本匹配到两张图像之一或都不匹配，提升图像对推理能力。  \n\n\n关键结果：  \n14M预训练图像下，test-P准确率83.14%，相比VILLA提升3.84%，验证多图像推理的有效性。  5.5 弱监督视觉定位（Weakly-Supervised Visual Grounding）\n\n\n任务目标：在无边界框标注的情况下，定位图像中与文本描述对应的区域。  \n数据集：RefCOCO+，含19,992张COCO图像，141,564条指代表达（如“穿绿衬衫的女孩”）。  \n模型调整：\n仅使用图像-文本对监督，通过Grad-CAM生成注意力热图，定位文本提及的区域。  \nITC vs. ITM对比：  \nITC：基于单模态相似度，定位物体整体（如“大象”）。  \nITM：基于多模态交互，定位细粒度属性和关系（如“卷鼻子的大象”）。  \n\n\n\n\n\n\n\n关键结果：\nALBEF-ITM在RefCOCO+的TestB子集准确率46.25%，远超ARN、CCL等基线方法（32.13%/33.56%）。\n可视化显示，模型能区分“大行李箱”与“小背包”等细微差异，证明跨模态交互的重要性。\n\n\n\n\n","slug":"ALBEF论文精读","date":"2025-07-03T03:20:45.000Z","categories_index":"","tags_index":"多模态,VLP","author_index":"犬夜叉"},{"id":"8c60d49e88f3e150291142683c6b267b","title":"ViLT 论文精读","content":"摘要\n\n\n\n\n\n\n\n\n视觉语言预训练已提升了各类视觉 - 语言联合下游任务的性能。当前 VLP 方法高度依赖图像特征提取流程，其中大部分涉及区域监督（如目标检测）和卷积架构（如 ResNet）。尽管现有文献未予重视，但我们发现这一模式存在两方面问题：（1）效率 / 速度层面，仅输入特征提取所需的计算量就远超多模态交互步骤；（2）表达能力层面，其上限受限于视觉嵌入器的表达能力及其预定义的视觉词汇表。本文提出一种极简 VLP 模型 —— 视觉语言 Transformer（ViLT），其核心在于将视觉输入的处理大幅简化为与文本输入相同的无卷积模式。实验表明，ViLT 的速度可达以往 VLP 模型的数十倍，同时在下游任务中具备相当或更优的性能。我们的代码和预训练权重可从https://github.com/dandelin/vilt获取。\n论文介绍1. 视觉语言预训练（VLP）的现状与挑战\n主流方法的依赖与问题： 现有 VLP 模型高度依赖基于卷积神经网络（CNN）的图像特征提取（如 ResNet）和区域监督（如目标检测），导致两大核心问题：\n效率瓶颈：特征提取的计算量远超多模态交互步骤（如 UNITER 模型中视觉处理耗时占比超 90%）。\n表达能力受限：依赖预定义的视觉词汇表（如 Visual Genome 的 1600 个物体类别），难以泛化未知物体或场景。\n\n\n学术研究与实际应用的脱节： 学术实验中常通过预缓存区域特征减轻训练时的计算负担，但在实际应用中，实时输入仍需经历耗时的特征提取流程，限制了模型的部署效率。\n\n2. ViLT 的核心创新：极简架构与统一处理\n无卷积的视觉特征嵌入： 受 ViT（Vision Transformer）启发，ViLT 将图像分割为固定大小的补丁（如 32×32 像素），通过线性投影层直接生成视觉嵌入，完全摒弃 CNN 和目标检测模块。这一设计使视觉处理耗时仅为～0.4 ms，参数仅 2.4M，远低于 ResNet 等传统 backbone。\n统一的 Transformer 架构： 视觉和文本输入均通过 Transformer 进行多模态交互，首次实现 模态特定组件计算量（VE+TE）&lt;多模态交互计算量（MI） 的架构（如图 1 中 ViLT 的 MI 耗时～15 ms，远超 VE 的 0.4 ms）。这种 轻嵌入、重交互 的设计显著提升计算效率，同时保持下游任务性能。\n\n3. 性能与效率对比\n速度优势： ViLT 的总推理时间～15 ms，比基于区域特征的模型（如 UNITER，~900 ms）快 60 倍以上，比基于网格特征的 Pixel-BERT（~60 ms）快 4 倍。\n任务表现： 在 NLVR2（视觉推理）、F30K 检索等任务中，ViLT 的准确率与传统模型相当或更优（如 NLVR2 测试集准确率 74.57%，接近 UNITER 的 75.8%），证明无需复杂视觉 backbone 仍可实现有效跨模态建模。\n\n4. 关键贡献与意义\n架构革新： 首次证明 VLP 模型可完全脱离卷积和区域监督，为轻量化多模态模型设计提供新范式。\n方法创新： 引入全词掩码（Whole Word Masking）和图像增强（RandAugment），提升预训练效率和下游任务泛化能力。\n研究启示： 呼吁 VLP 领域从 “单模态特征增强” 转向 “多模态交互优化”，为后续模型（如更大规模的 ViLT-L/H）奠定基础。\n\n\n从论文中的图片可以看出，对于传统模型，一般的处理流程包括：\n\n图像输入后，先通过卷积神经网络提取网格特征，再经目标检测模块生成区域建议（RoI），并通过非极大值抑制（NMS）和 RoI 头处理，最终得到区域特征。\n直接使用 CNN backbone（如 ResNet）输出的网格特征，跳过目标检测步骤，通过线性嵌入输入 Transformer。在这片论文中，图像直接分割为固定大小的补丁（如 32×32 像素），通过线性投影层（Linear Embedding）将每个补丁转换为特征向量，无需 CNN 或目标检测模块。\n\n\n效率优势：ViLT 的总运行时间（~15 ms）仅为 UNITER 的 1.7%，Pixel-BERT 的 25%。\n性能保持：在 NLVR2、F30K 检索等任务中，ViLT 性能接近或超过传统模型，证明去除 CNN 和区域监督不会显著损失表达能力。\n\n研究背景1. 视觉-语言模型的分类体系\n2. 模态交互方式\n单流 vs. 双流：\n单流模型（如 VisualBERT）将图像和文本拼接后输入 Transformer，参数效率更高；\n双流模型（如 ViLBERT）分模态处理，引入额外参数，ViLT 采用单流架构。\n\n\n\n3. 视觉嵌入的瓶颈\n区域特征： 通过 Faster R-CNN 等目标检测器生成，需经历 RPN、NMS 等复杂流程，计算量大（如 UNITER 视觉处理耗时 810 ms），且依赖预定义类别（如 Visual Genome 的 1600 个物体类），泛化能力受限。\n网格特征： 直接使用 CNN 输出（如 Pixel-BERT），虽省去目标检测，但 CNN backbone（如 ResNet）仍为性能瓶颈（Pixel-BERT-R50 视觉处理耗时 45 ms）。\nViLT 的创新： 采用Patch投影，将图像分割为 32×32 像素块，通过线性投影生成嵌入，仅需 0.4 ms 和 2.4M 参数，彻底摒弃 CNN 和目标检测。\n\n4. 关键问题与突破\n传统模型的效率与表达局限： 视觉嵌入的高计算量（如区域特征提取）和预定义视觉词汇限制了模型速度和泛化能力，学术实验中预缓存特征的做法无法解决实际应用中的实时性需求。\nViLT 的设计思路： 受 ViT 启发，利用 Transformer 的自注意力机制直接处理图像补丁和文本 tokens，将计算重点从 “单模态特征提取” 转向 “跨模态深度交互”，实现轻量化与高性能的平衡。\n\nVision-and-Language Transformer一、模型整体架构与核心设计1. 统一的单流Transformer架构ViLT采用单流 Transformer 处理视觉和文本输入，即图像和文本嵌入被拼接为单一序列，通过多层Transformer层进行联合建模。这一设计避免了双流架构的额外参数开销，同时确保深度跨模态交互。\n2. 视觉嵌入：无卷积的Patch投影\n输入处理：图像被分割为固定大小的补丁（如32×32像素），每个补丁被展平为一维向量  为补丁尺寸， 为图像通道数， 为补丁数量）。\n线性投影：通过可学习的线性层  将补丁向量映射到隐藏空间 （H=768 为隐藏层维度），并添加位置嵌入 。\n关键优势：\n仅需 2.4M参数，远小于ResNet-50（25M）等卷积 backbone。\n计算耗时仅 0.4 ms，彻底消除CNN和目标检测的耗时瓶颈。\n\n\n\n3. 文本嵌入：基于BERT的轻量设计\n输入处理：文本通过BERT分词器生成 tokens，包含特殊标记（如[CLS]）和位置嵌入 。\n模态类型嵌入：为区分视觉和文本模态，分别添加模态类型向量  和，避免模态混淆。\n\n4. 多模态交互：Transformer编码器\n输入拼接：视觉嵌入  和文本嵌入  拼接为 。\nTransformer层：通过12层Transformer编码器（每层包含多头自注意力MSA和MLP）进行特征交互，公式如下：， 其中，层归一化采用ViT的“前归一化”策略（先归一化再计算注意力/MLP），与BERT的“后归一化”不同。\n池化输出：最终序列的首个标记  通过线性投影和激活函数生成池化表示 ，用于下游任务分类或检索。\n\n二、与训练目标与技术创新\n\n\n\n\n\n\n实验数据集介绍\n1、 MSCOCO（Microsoft Common Objects in Context）\n\n规模：113K 张图像，567K 条字幕（平均每张图 5 条字幕），字幕长度为 11.81±2.81。\n定位：微软开发的 多任务基准数据集，支持目标检测、场景分割、图像字幕、视觉问答（VQA）等任务。\n\n\n核心特点：\n字幕标注 聚焦场景核心内容（如 “一只狗在公园奔跑”），避免冗余细节，适合训练模型抓重点；\n图像覆盖真实生活场景（如家庭、户外），物体类别丰富（80 类常见物体），是视觉语言任务的 “试金石”。\n\n\n\n2、VG（Visual Genome）\n\n规模：108K 张图像，5.41M 条字幕（平均每张图 50 条 + 字幕），字幕长度为 5.53±1.76。\n定位：斯坦福李飞飞团队推出的 细粒度语义理解数据集，主打 场景图（Scene Graph） 标注（结构化描述物体、属性、关系，如 “人→骑→自行车”）。\n核心特点：\n\n标注极丰富：除字幕，还有 “区域描述”（局部区域的文本解释）、“视觉问答”（针对图像的问答对）；\n分布呈长尾性：常见关系（如 “on” “in”）占比极高，稀有关系极少，后续衍生出 “VG150” 等清洗版本（筛选高频类别）。\n\n3、 GCC（Google Conceptual Captions）\n\n规模：3.01M 张图像，3.01M 条字幕（1:1 配对），字幕长度为 10.66±4.93。\n定位：大规模弱监督数据集，图像和字幕从网络爬取，属于 “弱关联” 标注（无需严格语义对应）。\n核心特点：\n数据极多样：图像风格覆盖更广（如艺术画、表情包），突破 MSCOCO 的场景限制，提升模型泛化能力；\n预训练核心：ViLT 等模型用其预训练，通过海量数据学习 “松散跨模态关联”，弥补强监督数据集的规模不足。\n\n\n\n4、 SBU（SBU Captions）\n\n规模：867K 张图像，867K 条字幕（1:1 配对），字幕长度为 15.0±7.74。\n定位：早期 图像 - 字幕检索数据集，最初用于 “给定图像找匹配字幕” 或反之的检索任务。\n核心特点：\n字幕更长且自由（均值 15 词，长于其他数据集），适合训练模型理解复杂文本描述；\n标注简洁：1 张图对应 1 条字幕，训练成本低，常与其他数据集联合预训练（如 ViLT）。\n\n\n\n\n\n\n\n1. 图像文本匹配\n任务定义: 判断图像-文本对是否匹配，正样本为真实配对，负样本为随机替换图像的配对。\n损失函数: 二元交叉熵损失, 通过线性层将池化表示  映射为匹配概率\n创新: 词-补丁对齐 (Word-Patch Alignment, WPA) 受最有传输理论启发, 引入WPA 目标增强跨模态对齐:\n计算文本子集和视觉子集之间的Wasserstein距离, 使用近似最近点方法优化\n将距离损失按系数0.1加权后加入ITM损失, 提升细粒度语义对齐(如下图的可视化热力图)\n\n\n\n\n\n\n\n\n\n\n词-补丁对齐详解: 基于最优传输的跨模态语义对齐\n\n核心问题：跨模态细粒度对齐的挑战在视觉语言任务中，文本中的单词（如“花朵”）需与图像中的对应区域（如花朵所在的补丁）建立精准关联。传统方法（如点积、注意力）通过特征相似度建模交互，但缺乏对语义结构的显式对齐。例如，文本中的“giraffe”需对应图像中长颈鹿的整体区域，而非零散像素。WPA的目标：通过最优传输理论，将文本单词与图像补丁视为两个分布，计算它们之间的结构化对齐成本，强制模型学习语义级别的跨模态对应关系。\n理论基础：最优传输与Wasserstein距离最优传输（Optimal Transport, OT）：起源于运输问题，旨在找到两个概率分布之间的最优映射路径，使总运输成本最小。例如，将一堆沙子（源分布）移动到另一堆沙子（目标分布）的最小工作量。Wasserstein距离（推土机距离）：衡量两个分布之间的最小运输成本，公式为： ， 其中， 和  分别为文本和视觉特征分布，为两者的联合分布， 为单个样本的运输成本（如特征向量距离）。关键价值：Wasserstein距离不仅衡量整体分布差异，还能捕捉局部结构对应（如单词与补丁的一一映射），适合细粒度语义对齐。\nWPA的技术实现：基于IPOT的近似优化论文采用 近似最近点方法（IPOT） 高效计算Wasserstein距离，避免传统OT的高复杂度。具体步骤如下：步骤1：特征子集提取文本子集 ：从Transformer最后一层输出中提取文本 tokens 的特征（不包含特殊标记如[CLS]）。视觉子集 ：提取图像补丁的特征（不包含补丁分类标记[VCLS]）。形状：假设文本有  个单词，视觉有  个补丁，则特征矩阵分别为 和 为隐藏层维度）。步骤2：运输成本矩阵构建计算每个单词与补丁之间的成对距离，形成成本矩阵 ：  （归一化欧氏距离），分母  为缩放因子，确保数值稳定性。步骤3：IPOT迭代优化通过IPOT算法求解最优传输计划 ，其中  表示单词  与补丁  的对齐强度（概率值）。初始化：均匀分布 。迭代更新：, 其中， 为温度参数，控制对齐的平滑度；迭代次数 （与训练阶段一致）。约束：确保每行（单词）和每列（补丁）的概率和为1，即 ， 。步骤4：Wasserstein距离计算近似Wasserstein距离为：  , 即运输计划与成本矩阵的内积，反映整体对齐成本。步骤5：损失加权与反向传播将Wasserstein距离按系数  加权后加入图像文本匹配（ITM）损失：  ， 该损失引导模型调整Transformer参数，使语义相关的单词与补丁之间的运输成本降低（即对齐强度增加）。\n可视化与效果：语义对齐的直观呈现论文通过上面的热图可视化WPA的对齐结果。例如，文本“flowers”对应图像中花朵区域的补丁，“wall”对应墙壁区域的补丁，透明度越高表示对齐强度）越大。关键发现：  \n\n\n传统模型（如UNITER）依赖目标检测框标注，只能对齐预定义类别；  \nViLT通过WPA实现无显式标注的语义对齐，可泛化至任意物体。消融实验：移除WPA后，NLVR2准确率下降约0.8%，证明其对跨模态推理的重要性。\n\n\n为什么WPA有效？结构化对齐：区别于简单特征融合（如CLIP的点积），WPA显式建模单词与补丁的一对多关系，捕捉复杂语义关联（如“giraffe”对应多个身体部位补丁）。抗局部最优：IPOT的迭代优化避免陷入浅层交互的局部最优解，迫使模型学习全局一致的跨模态映射。轻量化开销：WPA仅在预训练阶段引入约0.4 ms计算量，远低于目标检测的数百毫秒耗时，适合端到端优化。\n\n\n\n2. 掩码语言建模（Masked Language Modeling, MLM）\n任务定义：随机掩码文本 tokens（15%概率），预测原始标记。\n论文创新：全词掩码（whole world masking）\n掩码整个单词的所有子词单元（如“giraffe”掩码为[gi, [MASK], ##fe]），迫使模型依赖图像信息而非局部文本上下文。\n实验表明，全词掩码显著提升NLVR2等推理任务性能（+0.96% test-P准确率）。\n\n\n\n\n3. 图像增强（RandAugment）\n应用场景：仅在微调阶段使用，避免预训练时缓存特征的限制。\n策略选择：采用RandAugment中的非颜色反转和非裁剪策略（保留颜色信息和小物体），超参数设为 。\n效果：结合全词掩码和更长训练步数（200K步），ViLT在VQAv2测试集准确率提升至71.26%。\n\n\n\n\n\n\n\n\n表格解释\n\n视觉嵌入方式：\nRegion：基于目标检测的区域特征。\nGrid：基于 CNN 网格特征。\nLinear：ViLT 的线性补丁投影。\n\n\n任务：\nVQAv2：视觉问答，准确率越高越好。\nNLVR2：视觉推理（二元分类），dev/test-P 准确率越高越好。\n\n\n关键标注：\n†：预训练额外使用 GQA、VQAv2、VG-QA 数据集。\n‡：额外使用 Open Images 数据集。\na：微调时应用 RandAugment 数据增强。\n+：预训练步数延长至 200K 步（默认 100K 步）。\n\n\n\n\n\n三、关键技术对比与性能优势1. 与传统VLP模型的对比\n\n\n\n维度\n传统模型（如UNITER）\nViLT\n\n\n\n\n视觉嵌入\nCNN+目标检测（如ResNet+Faster R-CNN）\n线性补丁投影（无卷积/检测）\n\n\n计算量\n视觉处理占90%以上耗时（~810 ms）\n视觉处理仅0.4 ms，总耗时~15 ms\n\n\n参数规模\n150M+（含CNN权重）\n87.4M（仅Transformer+线性层）\n\n\n下游任务性能\nVQAv2 test-dev 72.7%\nVQAv2 test-dev 71.26%（+RandAugment）\n\n\n推理效率\n需预缓存特征（训练时高效，推理时慢）\n端到端高效（无需预计算）\n\n\n\n\n2. 效率与性能的平衡\n速度优势：ViLT比UNITER快60倍以上，比Pixel-BERT（网格特征）快4倍。\n泛化能力：无需预定义视觉词汇（如Visual Genome的1600类），通过自注意力学习任意图像补丁与文本的关联，适用于未知物体场景。\n\n","slug":"ViLT论文精读","date":"2025-07-02T09:37:29.000Z","categories_index":"","tags_index":"多模态,VLP","author_index":"犬夜叉"},{"id":"64c2453c0ac7d584fda9903cba47649e","title":"CLIP 论文精读","content":"CLIP论文精读CLIP通过对比学习从大量的图像-文本中学习视觉概念，实现了强大的零样本图像分类能力\n\n\n\n\n\n\n\n\n\n论文地址：Learning Transferable Visual Models From Natural Language Supervision\n\n\n\n\n\n\n论文创新点\n\n它不使用传统计算机视觉任务中常见的、带有固定类别（如“猫”、“狗”）的标签，而是直接从互联网上收集的（图像，文本）对中学习。这种文本描述提供了比单一标签丰富得多的监督信号，涵盖了几乎无限的视觉概念。\n为了让模型理解图像与文本的关联，CLIP采用了一种名为“对比学习”的自监督方法。其核心思想是：\n构建双编码器架构：CLIP包含一个图像编码器（Image Encoder，如ResNet或Vision Transformer）和一个文本编码器（Text Encoder，如Transformer）。\n学习多模态嵌入空间：在训练过程中，模型会接收一批图像和文本。图像编码器将图像转换为特征向量，文本编码器将文本转换为特征向量。CLIP的目标是在这个共享的多模态嵌入空间中，拉近真实的“图像-文本”对的特征向量（正样本），同时推远不匹配的“图像-文本”对的特征向量（负样本）。\n高效的代理任务：通过判断哪个文本与哪个图像配对，这个看似简单的“代理任务”却极其高效地迫使模型学习图像内容和文本语义之间的深刻联系。\nCLIP最令人瞩目的成果是其强大的零样本学习能力。传统的模型在面对一个新的分类任务时，通常需要进行微调，即在新任务的标注数据上进行再训练，而经过预训练的CLIP无需任何微调即可直接应用于新的视觉分类任务。其实现方式为：\n动态构建分类器：对于一个给定的分类任务（例如，区分“猫”和“狗”的图片），CLIP会将类别名称（”cat”, “dog”）转换成标准的提示语，如 “a photo of a cat” 和 “a photo of a dog”。\n相似度匹配预测：将这些提示语通过文本编码器生成文本特征向量。当输入一张待分类的图像时，图像编码器会生成其图像特征向量。最后，模型会计算该图像特征向量与所有类别提示语的文本特征向量之间的余弦相似度，相似度最高者即为预测的类别。\n\n\n\n\n\n1. 核心方法：对比学习CLIP的目标不是像传统模型那样预测一个固定的类别，而是学习一个多模态的嵌入空间，在这个空间里，匹配的图像和文本对的特征向量距离很近，而不匹配的则很远。具体实现如下：\n\n构建批次：在一个训练批次中，包含 N 个匹配的对。\n双塔编码：\n图像编码器 (Image Encoder)：将 N 个图像编码成 N 个图像特征向量 \n文本编码器 (Text Encoder)：将 N 个文本编码成 N 个文本特征向量 \n\n\n计算相似度：计算所有可能的 对的余弦相似度，形成一个 N×N 的相似度矩阵\n定义损失：在这个矩阵中，对角线上的 N 个元素是正样本（匹配的图文对），其余的  个元素都是负样本。CLIP的优化目标是一个对称的交叉熵损失函数，即同时在行和列的方向上进行优化：对于每个图像，模型需要从 N 个文本中找出正确的那个；反之，对于每个文本，也需要从N个图像中找出正确的那个。\n\n\n2. 核心能力：零样本迁移这是CLIP方法论的直接应用，也是其价值的主要体现\n\n动态构建分类器：对于任何一个分类任务（比如对ImageNet的1000类进行分类），CLIP不需要重新训练。而是通过“提示工程”为每个类别创建一个或多个描述性文本。例如，对于类别 “dog”，可以生成文本 “A photo of a dog.”。\n\n\n\n推理与匹配：将待分类的图像输入Image Encoder得到其特征 。然后，将所有类别的提示文本输入Text Encoder得到一组类别特征 ,… 。最后，计算  与每个类别特征  的余弦相似度，相似度最高者即为预测类别。\n提示工程的重要性：精心设计的提示语至关重要。它能解决词义模糊问题（如”boxer”是狗还是拳击手）并提升性能。论文中通过集成80个不同的提示模板，在ImageNet上的准确率提升了3.5%。这一技巧的有效性，使得CLIP的性能得到了显著增强。\n\n\n3. 关键决策：追求最高的训练效率在论文中，作者强调，由于计算资源是有限的，选择一个计算效率最高的预训练方法至关重要。他们对比了三种方法：\n\n方法一：Transformer语言模型 (预测文本)：类似VirTex，用图像作为上下文，生成描述文本。这种方法表现力强，但任务难度大，学习效率最低。\n方法二：词袋模型：不要求生成完整句子，只要求预测文本中的单词。效率比方法一高3倍，但仍不够理想。\n方法三：对比学习：只要求判断图文是否匹配，任务最简单。其训练效率比词袋模型还要高4倍。\n\n\n这个实验结论是CLIP成功的关键之一：在超大规模数据下，一个更简单、更高计算效率的训练目标，能让模型在有限时间内学到更好的表征。\n4. 模型与数据规模\n数据集：OpenAI构建了一个名为WebImageText的私有数据集，包含从互联网上收集的4亿个图文对。\n图像编码器：论文中测试了两种架构：\nResNet-D：对标准ResNet进行了一些修改，如用注意力池化层替换全局平均池化层。共测试了5个不同规模的ResNet。\nVision Transformer (ViT)：共测试了3个不同规模的ViT。实验发现ViT的计算效率比ResNet更高。最终性能最好的模型是 ViT-L/14，并在336x336的分辨率下进行了额外的微调。\n\n\n文本编码器：一个标准的63M参数、12层、512宽、8个注意力头的Transformer模型。\n\n5. 实验结果与分析CLIP的实验部分非常详尽，覆盖了超过30个不同的数据集，主要结论如下：\n\n与全监督模型匹敌：在ImageNet上，Zero-shot CLIP的准确率可以达到76.2%，与一个在ImageNet上经过完整监督训练的ResNet-50效果相当。\n超强的鲁棒性：CLIP最令人印象深刻的是其在自然分布漂移上的表现。在ImageNet-V2, Rendition, Sketch, Adversarial等更具挑战性的数据集上，其性能远超监督模型。最极端的例子是在ImageNet-A上，ResNet101的准确率从76.2%骤降至2.7%，而CLIP仍能达到77.1%，展现了惊人的泛化能力。\n\n\n\n数据重叠检查：为了验证性能不是来自于训练集和测试集的重叠（数据泄露），作者进行了详尽的检查，发现重合率中位数仅为2.2%，且移除这些重叠样本后，模型性能没有显著变化，证明了其强大的泛化能力是真实有效的。\n优秀的特征表示：即使不用于零样本分类，CLIP学习到的特征本身也极为优秀。在标准的线性探查（linear probe）评测中，CLIP的特征在性能和计算效率方面均优于当时的其他自监督方法。\n\n\n上面图片中，左图估算了线性分类器达到零样本 CLIP 性能所需的每类标注样本数，范围从不足 1 到 184，中位数 5.4，均值 20.8。这表明zero - shot transfer 数据效率差异大，部分任务需大量标注，部分几乎无需。图 8 显示zero - shot 与线性探针性能正相关，但zero - shot 普遍低 10%-25%，仅 5 个数据集接近。这说明 CLIP zero - shot 能力与表征质量相关，但仍有提升空间，多数任务距最优有差距。\n\n上图对比了 CLIP 模型与 EfficientNet、MoCo、ViT 等先进计算机视觉模型的线性探针性能。左图为 Kornblith 等研究的 12 个数据集平均分数，右图为 27 个更多样分布数据集的平均分数。结果显示，CLIP 模型，尤其是 ViT 架构的 CLIP-L/14@336px，在两类数据集上均表现出色，其最佳模型平均得分超过现有模型，且 Vision Transformer 比 ResNet 更高效。虚线表示微调或高分辨率评估的模型，体现了 CLIP 在表征学习上的优势与高效性。\n\n","slug":"CLIP论文精读","date":"2025-07-02T08:41:41.000Z","categories_index":"","tags_index":"多模态,对比学习","author_index":"犬夜叉"},{"id":"290cb712ccbf87b18cb10d019f08a741","title":"ViT 论文精读","content":"Vision Transformer的出现标志着 Transformer 架构成功应用于计算机视觉领域，挑战了卷积神经网络在该领域的主导地位。ViT 通过将图像分割成小块 (patches)，并将这些图像块视为序列输入到标准的 Transformer 编码器中，从而实现了对图像的有效处理。这一进展不仅在图像分类等任务上取得了最先进的成果，更重要的是，它为视觉和语言（以及其他模态）提供了一种通用的架构语言——Transformer。这种架构上的统一极大地促进了后续多模态模型（如CLIP、LLaVA等，它们通常采用ViT或其变体作为视觉编码器）的设计和发展，使得不同模态的基于token的表示可以在相似的计算框架内进行交互和融合。 \n\n从数据图中可以看出，在较小的数据集上，Vision Transformer比计算成本相当的ResNets更容易过拟合。例如，ViT-B/32比ResNet50稍快；它在9M子集上的表现要差得多，但在90M+子集上表现更好。ResNet152x2和ViT-L/16也是如此。这个结果强化了这样一种直觉：卷积的归纳偏置对于较小的数据集是有用的，但对于更大的数据集，直接从数据中学习相关模式是足够的，甚至是有益的。\nI. 摘要尽管 Transformer 架构已成为自然语言处理（NLP）任务事实上的标准，但其在计算机视觉领域的应用仍然有限。在视觉领域，注意力机制要么与卷积网络（CNN）结合使用，要么用于替代卷积网络中的某些组件，但整体结构仍然保留。我们证明了这种对 CNN 的依赖并非必要，一个直接应用于图像块序列的纯 Transformer 模型可以在图像分类任务上表现得非常好。 当在大量数据上进行预训练，并迁移到多个中等或小型图像识别基准测试（如 ImageNet, CIFAR-100, VTAB 等）时，Vision Transformer (ViT) 相比于最先进的卷积网络取得了优异的结果，同时训练所需的计算资源也大幅减少。\nII. 创新点范式革新：将图像视为序列处理论文首次证明了一个纯粹的、标准的 Transformer 模型可以直接用于图像分类，而无需依赖卷积神经网络。传统视觉任务长期由 CNN 主导，这篇论文打破了这一惯例。它通过将图像分割成固定大小的图块，并将这些图块的线性嵌入序列作为 Transformer 的输入，成功地将 NLP 领域的成功范式迁移到了视觉领域。\n数据量胜于归纳偏置论文发现，当在超大规模数据集（如 JFT-300M，包含3亿张图片）上进行预训练时，Vision Transformer (ViT) 的性能超越了当前最先进的卷积网络。这揭示了一个重要现象：CNN 中固有的（如局部性、平移不变性）的归纳偏置在数据量较小时非常有效，但当数据量足够大时，模型可以从数据中直接学习到这些空间关系，强大的模型容量和更少的先验限制反而成为优势。\n卓越的计算效率和可扩展性与性能相当的 SOTA 卷积网络相比，ViT 在达到同等甚至更高精度时，所需的预训练计算资源要少得多。例如，ViT-L/16 在 JFT-300M 数据集上预训练后，其性能优于在同一数据集上训练的 BiT-L (一个大型 ResNet 模型)，而训练成本却显著降低。这证明了 Transformer 架构在可扩展性上的巨大潜力。\n简洁而有效的模型设计论文的设计理念是尽可能少地修改原始的 Transformer 架构，使其可以直接利用 NLP 领域成熟的高效实现和可扩展架构。这种简洁性不仅体现在模型结构上，也体现在对图像的处理上，除了初始的图块划分和用于适应不同分辨率的位置编码插值外，几乎没有引入图像特有的归纳偏置。\nIII. 网络原理详解ViT模型概览ViT模型的核心思想是将图像转换为一个序列，然后用标准的Transformer Encoder来处理这个序列。\n图像分块处理 (Image Patching)Transformer接受的输入数据格式是一维的词嵌入序列，为了处理二维图像数据，论文将图像  重塑为一个扁平化的二维图块序列 ，其中：\n\n(H, W) 是原始图像的分辨率\nC 是通道数\n(P, P) 是每个图像图块的分辨率\n 是最终得到的图块数量， 也作为 Transformer 的有效输入序列长度。\n\n\nTransformer 在其所有层中使用恒定的潜在向量大小 D，因此论文将图块扁平化，并通过一个可训练的线性投影映射到 D 维。这个投影的输出称为图块嵌入 (Patch Embeddings)。具体的分块实现可以使用卷积来实现，例如对于 224x224x3 的图像，可使用卷积核大小为 16x16、步长为 16，卷积核数量为 768，将原图像输出为 14x14x768，再将前两个维度展平，得到了最终的 196x768 的张量。\n\n可学习的分类嵌入 (Class Token)原论文在嵌入图块序列的前面添加一个类似于BERT的可学习的嵌入 [CLS] Token，用于对图像进行分类。在进行物体分类任务时，如果不添加Class token，直接把 196 x 768 维的张量输入到编码器（Encode）中，编码器输出的同样是 196 x 768 维的张量，也就是196个 1 x 768 维的向量。但此时面临一个难题：难以确定该选取哪个向量作为最终的输出向量来进行物体分类。为了解决上述问题，在将数据输入编码器之前，添加一个 1 x 768 维的向量，也就是Class token。这个向量会被放置在 196 x 768 维向量的前面。这样一来，编码器输出的向量维度就变成了 197 x 768。之后，只需通过切片操作获取第一个 1 x 768 维的向量，再把它送入分类头进行分类即可。\n融合位置编码 (Positional Encoding)若不添加类似Transformer中的位置编码，那么ViT对于不同顺序的图块会得到相同的结果，这是违反直觉的。Transformer中使用的是正弦位置编码，ViT原始论文中使用的是一维可学习的位置嵌入，因为论文通过实验没有观察到使用二维感知位置嵌入会带来显著的性能提升。在得到经过操作后的 197 x 768 维的张量（由 1 x 768 维的Class token和 196 x 768 维的张量x拼接而成）后，会加上一个维度同样为 197 x 768 的位置编码向量 position Embedding。由于二者维度相同，所以可以直接进行逐元素相加操作，这样就将位置信息融入到了嵌入向量中。最终的输入数据为：\nTransformer EncoderViT的 Encoder 使用的就是 Transformer Encoder 的结构，经过L个encoder结构后，输入维度没有发生变换，仍为 197*768 维。Transformer 的 Encoder 接收输入序列后，先通过词嵌入和位置编码融合语义与位置信息，随后经过多层处理。每层先通过多头自注意力机制计算词与词之间的关联权重，动态聚合上下文信息，再经残差连接和层归一化稳定训练；接着通过前馈神经网络非线性变换特征，同样伴随残差与归一化。最终输出富含上下文信息的序列表示，核心在于自注意力的全局交互和层堆叠的渐进特征抽象。\nMLP Head (分类头)经过encoder结构后，输出的维度为 197*768，此时我们会通过切片的方式提取出Class token的信息，其维度为 1*768。接着会拿这个 1*768 维的Class token经过MLP Head层。ViT中的MLP Head结构非常简洁，它的设计目标是作为一个简单的线性分类器，将从 [CLS] Token中提取到的高度浓缩的图像特征映射到最终的分类结果上。在最常见的实现中，MLP Head仅仅由一个线性层构成。输入维度等于Transformer模型内部的隐藏维度，输出维度等于任务所需的类别总数。在某些论文或实现中（特别是在预训练阶段），这个MLP Head可能会稍微复杂一点，比如包含一个 tanh 激活函数和一个线性层，即 Linear(tanh(Input))。但在将预训练好的模型应用于下游任务时，通常会丢弃预训练的MLP Head，换上一个全新的、符合新任务类别数量的单线性层。ViT整体结构如下图所示：\n\nIV. ViT代码复现1. Patch Embedding1234567891011121314151617181920212223242526272829303132import torchimport torch.nn as nnfrom functools import partialfrom collections import OrderedDictclass PatchEmbed(nn.Module):    \"\"\"    将图像分割成块 (patch) 并进行线性嵌入    \"\"\"    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):        # img_size 输入图片大小 | patch_size 图片分块大小 | in_c 输入通道数 | embed_dim 嵌入后的维度        super().__init__()        img_size = (img_size, img_size)        patch_size = (patch_size, patch_size)        self.img_size = img_size        self.patch_size = patch_size        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])        self.num_patches = self.grid_size[0] * self.grid_size[1]        # 使用二维卷积实现分块和嵌入 (B,3,224,224) -&gt; (B,768,14,14)        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()    def forward(self, x):        B, C, H, W = x.shape        assert H == self.img_size[0] and W == self.img_size[1], \\            f\"输入图像大小{H}*{W}与模型期望大小{self.img_size[0]}*{self.img_size[1]}不匹配\"                # (B,768,14,14) --flatten--&gt; (B,768,196) --transpose--&gt; (B,196,768)        x = self.proj(x).flatten(2).transpose(1, 2)        x = self.norm(x)        return x\n2. multi-head123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Attention(nn.Module):    def __init__(self,                 dim,  # 输入的token维度,768                 num_heads = 8, # 注意力头数,为8                 qkv_bias=False, # 生成QKV的时候是否添加偏置                 qk_scale=None, # 用于缩放QK的系数,如果None,则使用1/sqrt(head_dim)                 atte_drop_ration=0., # 注意力分数的dropout比率                 proj_drop_ration=0.): # 最终投影层的dropout比率        super().__init__()        self.num_heads = num_heads # 注意力头数        head_dim = dim // num_heads  # 每个注意力头数的维度        self.scale = qk_scale or head_dim ** -0.5  #qk的缩放因子        self.qkv = nn.Linear(dim,dim*3,bisa=qkv_bias) # 通过全连接层生成QKV,为了并行计算,提高计算效率,参数更少        \"\"\"\"        这是实现多头注意力的一个巧妙且高效的方式。        它用一个全连接层，一次性地将输入 x (维度为 dim) 映射到一个维度为 dim * 3 的张量。        这个 dim * 3 的张量可以被看作是 Q, K, V 三个部分横向拼接在一起的结果，        后续我们只需要对这个大张量进行切分即可。        这样做比定义三个独立的线性层(一个给Q,一个给K,一个给V)在计算上更高效。        \"\"\"        self.atte_drop = nn.Dropout(atte_drop_ration)        self.proj_drop = nn.Dropout(proj_drop_ration)        self.proj = nn.Linear(dim,dim) # 将每个head得到的输出进行concat拼接,然后通过线性变换映射为原本的嵌入dim        def forward(self,x):        B,N,C = x.shape  # 批大小, 图块数+1(这个1为class_token), 通道数        # reshape: B,N,3*C -&gt; B,N,3,num_head,C//num_head        # permute: B,N,3,num_head,C//num_head  -&gt;  3,B,num_heads,N,C//self.num_heads        #这样一来，Q, K, V就被分开了，并且每个头的计算所需的数据（N 和 head_dim）都排列在一起，非常适合进行批处理矩阵运算        qkv = self.qkv(x).reshape(B,N,3,self.num_head,C//self.num_head).permute(2,0,3,1,4)        # 用切片拿到q,k,v. 形状都是: [B, num_heads, N, head_dim]        q,k,v = qkv[0],qkv[1],qkv[2]        # transpose: (B,num_heads,N,C//self.num_heads)  -&gt;  (B,num_heads,C//self.num_heads,N)        # 这是一个批处理矩阵乘法。对于 B 个样本中的每一个和 num_heads 个头中的每一个，        # 我们都计算一个 [N, head_dim] 的 q 矩阵和一个 [head_dim, N] 的 k 转置矩阵的乘积        # 结果是一个 [N, N] 的矩阵，这个矩阵的第 (i, j) 个元素表示序列中第 i 个 token 对第 j 个 token 的注意力分数        attn = (q @ k.transpose(-2,-1))*self.scale  #形状为[B,num_heads,N,N]         attn = attn.softmax(dim=-1) # 对每个头的注意力分数矩阵的 每一行 进行归一化，使其和为1        attn = self.atte_drop(attn) # 应用dropout        # 注意力权重对V进行加权求和        # attn @ V: B,num_heads,N,C//self.num_heads        # transpose(1,2): B,N,num_heads,C//self.num_heads        # reshape(B,N,C): 将最后两个维度信息拼接,合并多个头输出,回到总的嵌入维度        x = (attn @ v).transpose(1,2).reshape(B,N,C)        # 将拼接好的多头输出通过最后一个线性层 self.proj。这一步允许模型学习如何最好地融合来自不同头的信息        x = self.proj(x)        x = self.proj_drop(x) # 应用最后的 dropout，防止过拟合        return x\n3. Block1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class MLP(nn.Module):    def __init__(self,in_features,hidden_features=None,out_features=None,act_layer=nn.GELU,drop=0.):        # in_features输入的维度  hidden_features隐藏层的维度,通常为in_features的4倍  out_features输出的维度,通常与in_features相同        super().__init__()        out_features = out_features or in_features        hidden_features = hidden_features or in_features        self.fc1 = nn.Linear(in_features,hidden_features)   # 第一个全连接层        self.act = act_layer()  # 激活层,默认GELU函数        self.fc2 = nn.Linear(hidden_features,out_features)  # 第二个全连接层        self.drop = nn.Dropout(drop)   # dropout层    def forward(self,x):        x = self.fc1(x)        x = self.act(x)        x = self.drop(x)        x = self.fc2(x)        x = self.drop(x)        return x                def drop_path(x, drop_prob:float = 0., training: bool = False):    \"\"\"    实现DropPath的核心功能。    以 drop_prob 的概率将输入的整个张量 x 置零。    这是 Stochastic Depth 网络中的主要正则化方法。    \"\"\"    # 如果丢弃概率为0，或者当前不是训练模式，则直接返回原始输入x    # 在评估或推理时，我们不希望随机丢弃任何路径    if drop_prob == 0. or not training:        return x    keep_prob = 1-drop_prob   # 计算需要保留的路径的概率 (keep_prob)    # 创建一个生成随即掩码的形状元组，shape会是 (batch_size, 1, 1, ...)，1的数量取决于x的维度    shape = (x.shape[0],) + (1,)*(x.ndim - 1)    # 生成一个随机张量。torch.rand生成[0, 1)之间的均匀分布随机数。    # 加上keep_prob后，random_tensor的范围变为 [keep_prob, 1 + keep_prob)    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.devide)    # floor_(): 对随机张量进行向下取整    # 结果是：原先在 [keep_prob, 1) 区间的数值变为0，原先在 [1, 1 + keep_prob) 区间的数值变为1    # 一个数落在 [1, 1+keep_prob) 的概率恰好是 keep_prob    # 这样，random_tensor就变成了一个二值掩码（0或1）    random_tensor.floor_()    # 将输入x除以keep_prob，然后乘以二值掩码    # 乘以random_tensor：将一部分样本的整个张量置零（实现DropPath）。    # 除以keep_prob：这是一种被称为\"Inverted Dropout\"的技术。通过在训练时放大保留下来的输出，    # 可以保证在推理时（此时keep_prob为1，不做任何操作）网络的期望输出与训练时保持一致，无需在推理阶段进行额外的缩放    output = x.div(keep_prob)*random_tensor    return outputclass DropPath(nn.Module):    def __init__(self, drop_prob=None):        super(DropPath,self).__init__()        self.drop_prob = drop_prob    def forward(self,x):        return drop_path(x,self.drop_prob,self.training)                 class Block(nn.Module):    def __init__(self,                 dim, # 每个token的维度                 num_heads,  #多头自注意力的头数量                 mlp_ratio=4,  #计算hidden_features大小 为输入的四倍                 qkv_bias=False,  # qkv偏置                 qk_scale = None,  # 注意力缩放因子                 drop_ratio=0.,  # 多头自注意力机制的最后dropout比例                 attn_drop_ratio=0.,  # 生成qkv之后的dropout比例                 drop_path_ratio=0., # drop_path比例                 act_layer=nn.GELU,  # 激活函数                 norm_layer=nn.LayerNorm  # 正则化层                 ):        super(Block,self).__init__()        self.norm1 = norm_layer(dim) # transformer encoder 中的第一个norm层        self.attn = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias,qk_scale=qk_scale,                              attn_drop_ratio=attn_drop_ratio,proj_drop_ration=drop_ratio)        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio else nn.Identity()        self.norm2 = norm_layer(dim)  # 定义第二个layer_norm层        mlp_hidden_dim = int(dim*mlp_ratio)        # 定义mlp层        self.mlp = MLP(in_features=dim,hidden_features=mlp_hidden_dim,act_layer=act_layer,drop=drop_ratio)    def forward(self,x):        x = x + self.drop_path(self.attn(self.norm1(x))) # 前向传播部分，输入的x先经过layernorm再经过多头注意力        x = x + self.drop_path(self.mlp(self.norm2(x)))  # 将得到的x一次通过layernorm、mlp、drop_path        \n4. Vision Transformer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180def _init_vit_weights(m):    # 1. 如果是全连接层    if isinstance(m,nn.Linear):        # 使用截断正态分布进行初始化，这是原始ViT论文推荐的方法        # 截断正态分布可以防止权重值离均值太远，让初始状态更稳定        # std=0.02 是一个经验值。带下划线的方法表示这是个in-place（原地）操作        nn.init.trunc_normal_(m.weight,std=0.01)        # 如果该线性层有偏置(bias)项        if m.bias is not None:            # 将偏置初始化为0            nn.init.zeros_(m.bias)    # 2. 如果是2D卷积层    elif isinstance(m,nn.Conv2d):        # 使用Kaiming正态分布初始化。这是一种非常适合带有ReLU激活函数的卷积层的初始化方法        # 它可以有效防止梯度消失或爆炸        # mode=\"fan_out\" 表示根据输出通道数来调整方差        nn.init.kaiming_normal_(m.weight,mode=\"fan_out\")        # 如果该卷积层有偏置项        if m.bias is not None:            # 将偏置项初始化为0            nn.init.zeros_(m.bias)    # 3. 如果是层归一化层    elif isinstance(m,nn.LayerNorm):        # 将偏置初始化为0        nn.init.zeros_(m.bias)        # 将权重初始化为1        nn.init.ones_(m.weight)        # 这样做的目的是，当训练刚开始时，LayerNorm层几乎等同于一个恒等映射，不会改变输入的分布，让训练更稳定class VisionTransformer(nn.Module):    def __init__(self, img_size=224,patch_size=16,in_c=3,num_classes=1000,                 embed_dim=768,depth=12,num_head=12,mlp_ratio=4.0,qkv_bias=True,                 qk_scale=None,representation_size=None,distilled=False,drop_ratio=0.,                 attn_drop_ratio=0.,drop_path_ratio=0.,embed_layer=PatchEmbed,norm_layer=None,                 act_layer=None):        \"\"\"        ViT模型构造函数。        Args:            img_size (int): 输入图像的尺寸            patch_size (int): 每个图像块(patch)的尺寸            in_c (int): 输入图像的通道数            num_classes (int): 最终分类的类别数            embed_dim (int): Token的嵌入维度 (D)            depth (int): Transformer Encoder的总层数            num_head (int): 多头注意力机制中的头数            mlp_ratio (float): Transformer Encoder中MLP层的维度扩展比例            qkv_bias (bool): 是否在Q,K,V生成时使用偏置            qk_scale (float, optional): QK缩放因子，默认为 1/sqrt(head_dim)            representation_size (int, optional): 在最终分类头之前，可选的中间全连接层维度            distilled (bool): 是否使用蒸馏模式 (DeiT模型)            drop_ratio (float): 全局Dropout比率            attn_drop_ratio (float): 注意力权重Dropout比率            drop_path_ratio (float): 随机深度的DropPath比率            embed_layer (nn.Module): 用于生成Patch Embedding的层            norm_layer (nn.Module, optional): 使用的归一化层            act_layer (nn.Module, optional): 使用的激活函数层        \"\"\"        # 调用父类nn.Module的初始化方法        super(VisionTransformer,self).__init__()        self.num_classes = num_classes # 保存分类数        self.num_features = self.embed_dim = embed_dim # 保存嵌入维度        # 判断是否使用蒸馏。如果使用，会多一个distillation token，总共2个特殊token；否则只有cls_token，1个        self.num_tokens = 2 if distilled else 1        # 如果未指定归一化层，则默认使用LayerNorm，并设置eps以增加数值稳定性。        # partial用于创建一个预设了部分参数的新函数        norm_layer = norm_layer or partial(nn.LayerNorm,eps=1e-6)        \"\"\"        partial 来自 Python 内置的 functools 模块，它的作用是将一个函数的某些参数“冻结”住，从而创建一个新的、更简单的函数        此处的意义是将所有nn.LayerNorm的eps固定为1e-6        \"\"\"        act_layer = act_layer or nn.GELU() # 如果未指定激活函数，则默认使用GELU        # 1. Patch Embedding层：将输入的图片(B, C, H, W)转换为一系列token(B, N, D)        self.patch_embed = embed_layer(img_size=img_size,patch_size=patch_size,in_c=in_c,embed_dim=embed_dim)        # 获取patch的数量        num_patches = self.patch_embed.num_patches          # 2. 定义可学习的 [CLS] token。这个token最终的输出将代表整个图像的特征用于分类        # 初始形状为(1, 1, embed_dim)，1个token，维度为embed_dim        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))        # 3. 如果是蒸馏模式，定义可学习的 [DIST] token        self.dist_token = nn.Parameter(torch.zeros(1,1,embed_dim)) if distilled else None        # 4. 定义可学习的位置编码(Positional Embedding)。因为Transformer本身不感知顺序，需要它来提供位置信息        # 长度为 patch数量 + 特殊token数量        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches+self.num_tokens,embed_dim))        # 5. 在位置编码加入后，应用一个Dropout层        self.pos_drop = nn.Dropout(p = drop_ratio)        # 6. 构建随机深度(Stochastic Depth)的衰减率序列        # torch.linspace生成一个从0到drop_path_ratio的等差序列，长度为depth        # 这样，越深的Block，其drop_path_ratio越大，被\"丢弃\"的概率也越高        dpr = [x.item() for x in torch.linspace(0,drop_path_ratio,depth)]        # 7. 构建Transformer Encoder主体，由连续的Block堆叠而成        # 使用nn.Sequential将多个Block串联起来        self.block = nn.Sequential(*[            Block(dim=embed_dim,num_heads=num_head,mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,qk_scale=qk_scale,                  drop_ratio = drop_ratio,attn_drop_ratio=attn_drop_ratio,drop_path_ratio=dpr[i],                  norm_layer=norm_layer,act_layer=act_layer)            for i in range(depth)        ])        # 8. 经过所有Transformer Block后的最后一个LayerNorm层。        self.norm = norm_layer(embed_dim)        # 9. 定义分类头之前的一个可选的\"pre-logits\"层 (常用于从JFT等大数据集预训练后迁移学习)        if representation_size and not distilled:            self.has_logits = True            # 更新最终输出的特征维度            self.num_features = representation_size            # pre_logits是一个包含全连接层和Tanh激活的序列            self.pre_logits = nn.Sequential(OrderedDict([                (\"fc\",nn.Linear(embed_dim,representation_size)),                (\"act\",nn.Tanh())            ]))        else:            self.has_logits=False            # 如果不使用，则用一个恒等映射层代替            self.pre_logits=nn.Identity()        # 10. 定义最终的分类头 (Head)        # 将提取的特征映射到最终的分类数        self.head = nn.Linear(self.num_features,num_classes) if num_classes&gt;0 else nn.Identity()        # 11. 如果是蒸馏模式，为distillation token也定义一个分类头        self.head_dist = None        if distilled:            self.head_dist = nn.Linear(self.embed_dim,self.num_classes) if num_classes&gt;0 else nn.Identity()                # 12. 权重初始化        # 对位置编码、dist_token、cls_token进行截断正态分布初始化        nn.init.trunc_normal_(self.pos_embed,std=0.02)        if self.dist_token is not None:            nn.init.trunc_normal_(self.dist_token,std=0.02)        nn.init.trunc_normal_(self.cls_token,std=0.02)        # 使用self.apply()方法，将_init_vit_weights函数递归地应用到模型的所有子模块上        self.apply(_init_vit_weights)    def forward_features(self,x):        \"\"\"提取特征的前向传播过程，不包括最后的分类头。\"\"\"        # x 初始形状: [B, C, H, W]        # 1. Patch Embedding: [B, C, H, W] -&gt; [B, num_patches, embed_dim]        x = self.patch_embed(x)        # 2. 将cls_token在batch维度上进行扩展，以匹配输入x的batch_size        # expand()是一个高效的操作，它不会实际复制数据        cls_token = self.cls_token.expand(x.shape[0],-1,-1) # [1, 1, D] -&gt; [B, 1, D]        # 3. 将特殊token与patch token拼接在一起        if self.dist_token is None:            # 若没有蒸馏token，拼接: [B, 1, D] 和 [B, N, D] -&gt; [B, N+1, D]            x = torch.cat((cls_token,x),dim=1)        else:            # 蒸馏模式下，拼接cls_token和dist_token            x = torch.cat((cls_token,self.dist_token.expand(x.shape[0],-1,-1),x),dim=1)        # 4. 加上位置编码，然后应用dropout        # pos_embed的[B]维度会自动广播以匹配x        x = self.pos_drop(x+self.pos_embed)        # 5. 通过Transformer Encoder主干网络        x = self.block(x)        # 6. 通过最后的LayerNorm        x = self.norm(x)        # 7. 提取用于分类的token的输出        if self.dist_token is None:            # 只返回cls_token的输出 (在序列的第0个位置)，并通过pre_logits层            return self.pre_logits(x[:,0])        else:            # 返回cls_token和dist_token的输出            return x[:,0],x[:,1]            def forward(self,x):        \"\"\"完整的从输入到输出的前向传播过程。\"\"\"        # 1. 首先通过forward_features提取特征        x = self.forward_features(x)        # 2. 通过最后的分类头得到logits        if self.head_dist is not None:            # 蒸馏模式下，两个token分别通过各自的分类头            # x此时是一个元组 (cls_output, dist_output)            x_cls,x_dist = self.head(x[0]),self.head_dist(x[1])            if self.training and not torch.jit.is_scripting():                # 在训练并且不是在 TorchScript 编译模式下，返回两个头的输出，以便分别计算损失                return x_cls,x_dist            else:                # 在评估时，返回两个头输出的平均值作为最终预测                return (x_cls + x_dist) / 2        else:            # 标准模式下，直接将特征通过分类头            x = self.head(x)        return x        \n\n\n\n\n\n\n\n\n\n蒸馏模式与 dist_token代码里的“蒸馏模式”来源于一篇非常重要的论文 DeiT (Data-efficient Image Transformers)。(1)什么是知识蒸馏 (Knowledge Distillation)？这是一种模型压缩和迁移学习的技术，核心思想是让一个强大而复杂的“教师模型”来指导一个轻量级的“学生模型”进行学习。\n\n教师模型: 通常是一个已经在大规模数据集上训练好的、性能非常强的模型（比如一个超大的 ResNet 或者另一个 ViT）。\n学生模型: 我们当前正在训练的模型（比如这个 VisionTransformer）。指导的方式不仅仅是让学生模型学习正确的标签（比如图片是“猫”），还会让它学习教师模型输出的“软标签”。软标签是指教师模型对所有类别的预测概率分布，例如它可能认为图片是“猫”的概率是85%，是“狗”的概率是10%，是“老虎”的… 这个概率分布包含了教师模型“思考过程”的丰富信息。\n\n(2)为什么 ViT 需要蒸馏？原始的 ViT 需要在海量的数据集（如谷歌内部的 JFT-300M，包含3亿张图片）上预训练才能获得优异的性能。如果只在 ImageNet-1k（约120万张图片）这种“中等”规模的数据集上从头训练，效果往往不如经典的 CNN 模型。DeiT 论文发现，通过知识蒸馏，可以让一个 ViT 在仅使用 ImageNet-1k 的情况下，达到甚至超过在 JFT-300M 上预训练的效果，极大地提高了 ViT 的数据效率。\n(3)dist_token 的作用DeiT 论文提出了一种新颖的蒸馏方式，就是通过添加一个专门用于蒸馏的 distillation token（即 dist_token）。\n\ncls_token 的任务: 和原来一样，它的最终输出用来和真实的标签 (ground-truth label) 计算损失，我们称之为“硬标签损失”。\ndist_token 的任务: 它是一个和 cls_token 地位相同的可学习向量，也被拼接到序列中，通过 Transformer 网络。但它的最终输出是专门用来和教师模型的预测（软标签或硬标签） 计算损失的，我们称之为“蒸馏损失”。\n\n通过这种方式，模型在训练时会同时优化两个目标：\n\n让 cls_token 的输出尽可能接近真实答案。\n让 dist_token 的输出尽可能模仿教师模型的答案。这种双重监督机制被证明非常有效，dist_token 就像一个专门负责从教师那里“偷师学艺”的通道，帮助学生模型学得更好。\n\npre-logits 层的作用pre-logits 层，可以理解为一个在最终分类头（self.head）之前的一个特征处理/映射层。它的主要作用是为了更好地进行迁移学习。想象一个场景：\n\n一个机构（比如谷歌）在一个超级庞大的私有数据集（比如 JFT-300M，有18000个类别）上预训练了一个 ViT 模型。\n这个预训练模型的最终分类头是 nn.Linear(embed_dim, 18000)。\n现在你想把这个模型用到你自己的任务上，比如一个只有10个类别的猫狗分类任务。\n\n显然，那个输出18000个维度的分类头对你来说是没用的。但是，它之前的网络层学到的特征提取能力是非常宝贵的。在这种情况下，pre-logits 层就派上用场了：\n\n它通常是一个 nn.Linear(embed_dim, representation_size) 加上一个激活函数（如 Tanh）。\n在预训练时，模型会先将 cls_token 的输出通过这个 pre-logits 层，得到一个固定维度的“特征表示” (representation)，然后再将这个表示送入最终的分类头。\n当你拿到这个预训练模型进行迁移学习时，你可以丢弃掉原有的最终分类头，但保留 pre-logits 层。然后，你只需要在 pre-logits 层的输出后面接上你自己任务的分类头，例如 nn.Linear(representation_size, 10)。\n\n5. 创建应用层123456789def vit_base_patch16_224(num_classes:int =100, pretrained=False):    model = VisionTransformer(img_size=224,                              patch_size=16,                              embed_dim=768,                              depth=12,                              num_head=12,                              representation_size=None,                              num_classes=num_classes)    return model\n参考笔记：ViT讲解\n","slug":"ViT论文精读","date":"2025-07-02T03:32:00.000Z","categories_index":"","tags_index":"Vision-Transformer,深度学习","author_index":"犬夜叉"}]