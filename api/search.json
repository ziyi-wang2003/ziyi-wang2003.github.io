[{"id":"8c1dec7477178f4dfc0f70f9822eceb4","title":"BLIP论文精读","content":"\n\n\n\n\n\n摘要\n视觉语言预训练（VLP）已推动许多视觉语言任务的性能提升。然而，大多数现有预训练模型仅在基于理解的任务或基于生成的任务中表现优异。此外，性能提升在很大程度上通过扩大从网络收集的含噪图像-文本对数据集实现，而这类数据作为监督来源并非最优。本文提出BLIP，一种可灵活迁移至视觉语言理解与生成任务的新型VLP框架。BLIP通过引导字幕生成（由字幕生成器生成合成字幕，过滤器去除噪声字幕）有效利用含噪网络数据。我们在图像-文本检索（平均召回率@1提升2.7%）、图像字幕生成（CIDEr提升2.8%）和视觉问答（VQA分数提升1.6%）等广泛视觉语言任务上取得了最先进的结果。BLIP在以零样本方式直接迁移至视频语言任务时也展现出强大的泛化能力。相关代码、模型和数据集已开源。\n\n\n\n1. Introduction1.1. 现有视觉语言预训练（VLP）的局限性视觉语言预训练在推动众多视觉语言任务（如图像-文本检索、视觉问答、图像字幕生成等）的发展上取得了显著的成功。 然而，当前主流的预训练方法存在两大核心局限性：\n\n模型架构的局限性 (Model Perspective):\n\n现有的模型通常采用两种主流架构：一种是基于编码器的模型（Encoder-based），例如 CLIP 和 ALBEF。 这类模型通过将图像和文本编码到同一个特征空间，非常擅长处理基于“理解”的任务，如图像-文本检索和匹配，因为它们能有效地学习图像和文本之间的对齐关系。 但是，由于其结构天然不包含生成模块，将它们直接应用于文本生成任务（如图像字幕生成）时，会显得非常不自然和低效，通常需要额外增加解码器或进行复杂的改造。\n另一种是基于编码器-解码器的模型（Encoder-Decoder），例如 SimVLM。 这种架构天生适合处理“生成”类任务，因为解码器可以直接根据图像信息生成文本序列。 然而，研究发现，这类模型在需要对图像和文本进行全局特征匹配的检索类任务上，表现却不尽如人意。\n核心矛盾: 这种架构上的分化导致了没有一个单一的模型能够同时在“理解”和“生成”两大类任务上都达到顶尖水平。模型的设计往往需要在两者之间做出取舍。\n\n\n训练数据的局限性 (Data Perspective):\n\n为了获得强大的泛化能力，最先进的VLP模型（如 CLIP, ALBEF, SimVLM）普遍依赖于从网络上自动爬取的大规模图像-文本对作为训练数据。这些数据集的规模动辄千万甚至上亿，通过扩大数据规模确实带来了性能的提升。\n然而，这种方式获取的文本数据（通常是网页图片的“alt-text”标签）充满了噪声。这些文本往往与图像内容不完全相关，可能包含无关的广告信息、上下文描述，或者描述非常笼统、不准确。 例如，一张“日落时分公园里的面包店”的图片，其网络文本可能仅仅是“blue sky bakery in sunset park”，非常简洁但缺乏视觉细节。\n核心问题: 这种充满噪声的文本是一种“次优”的监督信号（suboptimal source of supervision）。 直接使用这些噪声数据进行训练，会严重影响模型学习精确的图文对齐关系，从而限制了模型性能的上限。\n\n\n\n1.2. BLIP 的提出与核心贡献为了系统性地解决上述模型和数据两个层面的挑战，该研究提出了 BLIP (Bootstrapping Language-Image Pre-training) 框架。 BLIP 的目标是创建一个既能统一理解与生成任务，又能有效利用并净化网络噪声数据的VLP新范式。 其核心贡献体现在以下两个方面：\n\n(a) 模型创新：多模态编码器-解码器混合模型 (Multimodal mixture of Encoder-Decoder, MED)\n\n这是一种全新的、为多任务预训练和灵活迁移学习设计的模型架构。 MED 的精妙之处在于它能够根据不同的任务需求，灵活地切换其工作模式，主要有三种形态：\n单模态编码器 (Unimodal Encoder): 独立地对图像和文本进行编码，用于学习两者各自的表征。\n图像接地的文本编码器 (Image-grounded Text Encoder): 在标准的文本编码器基础上，通过交叉注意力机制（Cross-Attention）将图像信息注入到文本表征中，从而生成深度融合的多模态表征，专门用于理解任务。\n图像接地的文本解码器 (Image-grounded Text Decoder): 同样以图像信息为条件，但其文本处理部分采用自回归的方式（causal self-attention），用于生成连贯的文本描述。\n\n\n为了充分发挥 MED 架构的潜力，BLIP 设计了三个协同工作的预训练目标，在一个统一的框架下进行联合优化：\n图文对比学习 (Image-Text Contrastive Learning, ITC): 旨在拉近正样本图文对的特征表示，推远负样本对，从而对齐视觉和语言的特征空间。\n图文匹配 (Image-Text Matching, ITM): 这是一个二分类任务，让模型学习判断一个图文对是否匹配，旨在捕捉更细粒度的图文对应关系。\n图像条件下的语言建模 (Image-conditioned Language Modeling, LM): 训练模型在给定图像的条件下，生成对应的文本描述，赋予模型生成能力。\n\n\n\n\n(b) 数据创新：字幕生成与过滤 (Captioning and Filtering, CapFilt)\n\n这是一种新颖的、用于从噪声图文对中进行“数据自举”（Dataset Bootstrapping）的方法，其核心思想是“以模型净化数据，再用净化后的数据训练更好的模型”。\n该过程包含两个关键模块，它们都是由一个预训练好的 MED 模型微调而来：\n字幕器 (Captioner): 这是一个图像接地的文本解码器，其任务是为网络图片生成高质量的、描述性的合成字幕 (synthetic captions)。例如，对于一张网络图片，除了原有的噪声文本，字幕器会生成一个新的、更详细的描述，如“a chocolate cake with cream frosting and chocolate sprinkles on top”。\n过滤器 (Filter): 这是一个图像接地的文本编码器，其任务是判断图文对是否匹配。它会同时对原始的网络文本和新生成的合成字幕进行筛选，移除所有它认为与图像内容不匹配的“噪声”文本。 这一步确保了最终用于训练的数据质量。\n\n\n通过 CapFilt 机制，BLIP 能够从海量的网络数据中提炼出一个规模可观且质量更高的训练集，从而为训练高性能的 VLP 模型奠定坚实的数据基础。 \n\n\n\n如图所示，字幕器根据图片生成对该图片的合成字幕，之后再通过过滤器来进行评估，是保留原网络字母还是更新为合成的字幕。\n1.3. 关键成果通过模型和数据的双重创新，BLIP 在广泛的视觉语言任务上取得了最先进的（state-of-the-art）成果，全面验证了其框架的有效性和优越性。\n\n显著的性能提升: 在图文检索任务上，平均召回率（Recall@1）提升了 2.7%；在图像字幕生成任务上，CIDEr 分数提升了 2.8%；在视觉问答（VQA）任务上，VQA 分数提升了 1.6%。\n强大的泛化能力: BLIP 不仅在传统的图文任务上表现出色，还展现了极强的零样本（zero-shot）迁移能力，无需任何针对性训练，直接应用于视频-语言任务（如文本到视频检索和视频问答）时，也取得了顶尖性能。 \n\n2. Related Work2.1. 视觉语言预训练 (Vision-language Pre-training, VLP)\n核心目标与数据来源: VLP 的核心思想是在大规模的图文对数据上预训练一个通用模型，然后将这个模型微调应用于各种下游的视觉和语言任务。由于高质量的人工标注图文数据集（如COCO）规模有限且成本高昂，绝大多数现代 VLP 方法都转向使用从网络上自动爬取的数据集，例如 Conceptual Captions, ALIGN, SBU Captions 等。这些数据集通过简单的规则过滤来收集，但文本中普遍存在的噪声问题是不可避免的。\n\n对噪声数据的忽视与本文的贡献: 尽管噪声普遍存在，但其负面影响在很大程度上被通过不断扩大数据集规模所带来的性能增益所掩盖了。许多研究更关注于模型架构的改进，而忽视了数据质量这一根本问题。本研究明确指出，充满噪声的网络文本对于学习视觉-语言对齐是一种次优的选择，并提出了 CapFilt 机制，这是一种更有效、更智能地利用这些海量网络数据的方式，从根本上提升了数据质量。\n\n统一框架的挑战与本文的方案: VLP 领域的一个长期挑战是如何设计一个单一的模型架构，使其能够同时胜任基于“理解”的任务（如检索）和基于“生成”的任务（如字幕生成）。以往的尝试，无论是纯编码器模型、纯编码器-解码器模型，还是简单的统一编码器-解码器模型，都难以两全其美，各有侧重和局限。本文提出的 多模态编码器-解码器混合模型 (MED) 通过其灵活的功能切换能力，以及高效的参数共享策略，成功地在一个统一的框架内实现了对各类下游任务的卓越支持，同时保持了预训练过程的简洁和高效。\n\n\n2.2. 知识蒸馏 (Knowledge Distillation, KD)\n基本概念: 知识蒸馏是一种模型压缩和性能提升技术，其目标是通过让一个（通常较小的）“学生模型”模仿一个（通常较大的）“教师模型”的输出来进行学习，从而将教师模型的知识迁移给学生模型。自蒸馏（Self-distillation）是其中的一种特殊情况，即教师和学生模型的结构和尺寸相同，已被证明在图像分类和近期的VLP领域（如ALBEF）中是有效的。\n\nCapFilt 作为一种新颖的知识蒸馏: 传统的知识蒸馏方法大多是让学生模型模仿教师模型的类别预测概率（soft labels）。本文提出的 CapFilt 可以被看作是在VLP场景下一种更高级、更有效的知识蒸馏形式。它不是传递简单的预测分数，而是传递更丰富的语义知识：\n\n字幕器 (Captioner) 的作用类似于一个“生成式教师”，它将其对图像的理解“蒸馏”成一段段语义丰富的合成字幕。这些字幕相比于简单的类别标签，包含了更复杂的场景、物体和关系知识。\n过滤器 (Filter) 的作用则像一个“判别式教师”，它将其判断图文是否匹配的“知识”“蒸馏”出来，通过筛选行为告诉主模型什么样的图文对是高质量的。\n通过这种“生成”与“过滤”相结合的方式，CapFilt 实现了一种全新的、基于语义内容的知识蒸馏，从而有效地提升了预训练数据的质量。\n\n\n\n2.3. 数据增强 (Data Augmentation, DA)\n背景: 数据增强在计算机视觉领域是一种应用广泛且非常有效的技术。然而，对于自然语言处理（NLP）任务，由于语言的离散性和结构性，进行有效的数据增强更具挑战性。近年来，一个新兴的方向是利用预训练好的生成式语言模型来为各种NLP任务（特别是在数据稀疏的场景下）合成新的训练样本。\n\n本文工作的独特性: 之前利用生成模型进行数据增强的工作主要集中在纯文本、小规模或低资源的NLP任务上。本文首次将这一思想成功地应用到了大规模的视觉-语言预训练场景中。通过生成海量的合成字幕，本研究证明了“生成式数据增强”不仅适用于低资源场景，同样能为超大规模的多模态预训练带来显著的性能优势，为解决VLP中的数据瓶颈问题开辟了新的道路。\n\n\n3. 方法 (Method)该研究提出的BLIP是一个旨在从充满噪声的图文对中学习的统一视觉语言预训练（VLP）框架。 本节将首先详细介绍其创新的模型架构——多模态编码器-解码器混合模型（MED）及其预训练目标，然后阐述用于数据集自举（bootstrapping）的字幕生成与过滤（CapFilt）机制。\n3.1. 模型架构 (Model Architecture)BLIP的模型架构设计旨在创建一个能够同时处理理解与生成任务，并且在两者之间灵活切换的统一模型。\n\n\n图像编码器 (Image Encoder):\n\n模型采用 Vision Transformer 作为图像编码器。 这一选择与近期先进的方法保持一致，相比于早期工作中依赖预训练物体检测器来提取区域特征的方式，ViT在计算上更为友好。\n其工作原理是将输入图像分割成一系列的固定大小的图块（patches），然后将这些图块线性投影后编码为一串嵌入序列（sequence of embeddings）。 在序列的开头，会额外加入一个 [CLS] (classification) token，其在经过Transformer编码后的最终输出嵌入被用作整个图像的全局特征表示。\n\n\n多模态编码器-解码器混合模型 (Multimodal Mixture of Encoder-Decoder, MED):\n\n为了实现理解与生成能力的统一，研究者提出了MED，这是一个能够以三种不同功能模式运行的多任务模型。\n模式1：单模态编码器 (Unimodal Encoder):\n在此模式下，模型分别对图像和文本进行独立编码。\n文本编码器基于BERT架构实现。 在文本输入的起始位置同样会添加一个 [CLS] token，用于在编码后汇总整个句子的信息。该模式主要服务于需要独立图文表征的任务，例如对比学习。\n\n\n模式2：图像接地的文本编码器 (Image-grounded Text Encoder):\n此模式用于深度融合视觉和语言信息，以完成理解类任务。\n其实现方式是在文本编码器的每个Transformer块中，在自注意力（Self-Attention, SA）层和前馈网络（Feed Forward Network, FFN）之间，插入一个额外的交叉注意力（Cross-Attention, CA）层。这个交叉注意力层使得文本token能够“关注”并融合来自图像编码器的视觉特征。\n为了得到最终的图文融合表征，一个任务专用的 [Encode] token会被添加到文本输入中，该token在经过编码后的输出嵌入，即被用作代表该图文对的多模态表征。\n\n\n模式3：图像接地的文本解码器 (Image-grounded Text Decoder):\n此模式用于生成类任务，例如图像字幕。\n其结构与图像接地的文本编码器非常相似，但核心区别在于将编码器中的双向自注意力 (bi-directional self-attention) 层替换为因果自注意力 (causal self-attention) 层。 [102] 这种因果注意力机制确保了在预测当前词元时，模型只能访问到它之前已经生成的词元，这是自回归文本生成的关键。\n一个特殊的 [Decode] token被用作序列开始的信号，引导模型开始生成过程，并以一个序列结束（end-of-sequence）token来标志生成的终点。 [103]\n\n\n\n\n\n\n\n\n\n\n\n\n补充BLIP中使用的注意力机制\n双向自注意力（Bi Self-Att）双向自注意力（Bi-directional Self-Attention）是一种允许模型在处理序列时同时考虑整个上下文的注意力机制。在标准的Transformer自注意力中，每个输入位置可以通过关注序列中所有位置来更新自己的表示。\n\n具体来说：\n给定一个输入序列  ，对于每个位置 ，模型计算它与所有 （ j = 1  到 n ）的注意力权重。\n这些权重通过查询（Query）、键（Key）和值（Value）的点积计算得出，最终生成  的新表示。\n因为没有信息流限制，信息可以在序列中双向传播，这就是“双向”的含义。\n\n\n功能：\n捕捉全局上下文：双向自注意力能让模型综合整个序列的信息。例如，在文本中，一个词的含义可能依赖于它前后的词，双向自注意力可以捕捉这种关系。\n适用于编码任务：这种机制非常适合需要理解完整输入的场景，比如编码器中生成深层表示。\n\n\n在BLIP中的应用：\nText Encoder：BLIP的Text Encoder使用双向自注意力来处理纯文本输入。通过这种方式，模型可以理解文本的全局语义，例如句子的整体意思。\nImage-grounded Text Encoder：在这个模块中，双向自注意力用于处理文本序列，同时通过交叉注意力与图像特征交互，进一步丰富文本表示。\n\n\n\n\n因果自注意力（Causal Self-Att）因果自注意力（Causal Self-Attention）是一种限制信息流的注意力机制，确保模型在处理序列时只能访问当前位置之前的信息。这种机制常见于自回归任务（例如语言生成）。\n\n具体来说：\n对于输入序列 ，在计算  的表示时，模型只能关注  到  的位置，而不能看到  到 。\n实现方法通常是在注意力矩阵中加入掩码（Mask），将未来位置的权重设为零，从而阻止信息从“未来”流向“现在”。\n\n\n功能：\n保持因果性：因果自注意力模拟了序列生成的自然顺序，确保模型在预测下一个元素时不会“偷看”未来的信息。\n适用于解码任务：在Transformer的解码器中，这种机制被用来逐步生成序列，例如从左到右生成文本。\n\n\n在BLIP中的应用：\nImage-grounded Text Decoder：BLIP的这个模块负责生成文本（例如图像描述）。因果自注意力确保在生成每个词时，模型只依赖于之前生成的词和图像特征，而不会提前使用未生成的词。这种设计符合生成任务的逻辑。\n\n\n\n\n交叉注意力（Cross Attention）交叉注意力（Cross Attention）是一种在两个不同序列之间建立联系的注意力机制。与自注意力关注自身序列不同，交叉注意力让一个序列（查询）去关注另一个序列（键和值）。\n\n具体来说：\n假设有两个序列：查询序列 （例如文本）和键-值序列 （例如图像特征）。\n 中的每个位置会计算与  中所有位置的注意力权重，并从  中提取信息来更新自己的表示。\n这种机制不限制信息流，允许两个模态之间的自由交互。\n\n\n功能：\n跨模态交互：交叉注意力特别适合多模态任务（例如图像-文本任务），因为它能让一个模态的信息影响另一个模态。\n信息融合：通过这种机制，模型可以将图像和文本的特征结合起来，生成更丰富的表示。\n\n\n在BLIP中的应用：\nImage-grounded Text Encoder：在这个模块中，文本序列作为查询（Query），图像特征作为键（Key）和值（Value）。交叉注意力让文本表示能够融合图像信息，从而更好地理解图像-文本对的语义。\nImage-grounded Text Decoder：在生成文本时，交叉注意力让模型在每一步生成中都能参考图像特征，帮助生成与图像内容相关的描述。\n\n\n\n\n\n3.2. 预训练目标 (Pre-training Objectives)为了充分训练MED模型，BLIP设计了三个预训练目标并进行联合优化，其中包括两个理解类目标和一个生成类目标。这种多任务学习是高效的：对于每个输入的图文对，计算成本高昂的图像编码器只需进行一次前向传播，而文本转换器部分则分别激活三种不同功能，进行三次前向传播来计算对应的三个损失函数。\n\n图文对比损失 (Image-Text Contrastive Loss, ITC):\n\n该损失激活单模态编码器。\n其目标是通过鼓励正样本（匹配的）图文对在特征空间中拥有相似的表示，同时拉远负样本（不匹配的）对的表示，来对齐视觉和语言的特征空间。这已被证明是提升视觉语言理解能力的有效方法。\n具体实现上，BLIP沿用了ALBEF中的方法，引入一个动量编码器（momentum encoder）来产生更稳定的特征，并利用动量编码器生成的相似度分数创建软标签（soft labels），以此作为训练目标。这种软标签机制可以解释批次内负样本中可能存在的“假阴性”（即实际上也匹配的图文对）问题。\n\n\n图文匹配损失 (Image-Text Matching Loss, ITM):\n\n该损失激活图像接地的文本编码器。\n其目标是学习能够捕捉视觉和语言之间细粒度对齐关系的多模态表征。\nITM被设计成一个二元分类任务：模型利用一个ITM头（一个简单的线性层）来预测输入的图文对是正样本（匹配）还是负样本（不匹配）。\n为了提升训练的有效性，模型采用了ALBEF中的难负样本挖掘 (hard negative mining) 策略。具体来说，在同一个批次中，那些与图像在对比学习特征空间中相似度更高（即更具迷惑性）的文本负样本，会被优先选择用于计算ITM损失。\n\n\n语言建模损失 (Language Modeling Loss, LM):\n\n该损失激活图像接地的文本解码器。\n其目标是训练模型具备在给定图像的条件下生成相应文本描述的能力。\n它优化的是一个交叉熵损失函数，以自回归的方式最大化模型生成真实文本的概率。在计算损失时，应用了0.1的标签平滑来防止模型过拟合。\n与VLP中广泛使用的掩码语言建模相比，LM损失直接赋予了模型将视觉信息转化为连贯文本字幕的生成能力。\n\n\n参数共享策略:\n\n为了在实现多任务学习的同时保持训练效率，文本编码器和文本解码器共享了除自注意力（SA）层之外的所有参数。\n这一设计的核心理由是：编码任务（理解当前输入）和解码任务（预测下一个词元）之间的本质差异主要体现在SA层的功能上（双向 vs. 因果）。而嵌入层、交叉注意力（CA）层和前馈网络（FFN）在两种任务中扮演的角色相似，因此共享这些层的参数不仅能减少模型总参数量、提升训练效率，还能从多任务学习中受益。\n\n\n\n\n\n\n\n\n\n损失函数详解\n图文对比损失（Image-Text Contrastive Loss, ITC）图文对比损失（ITC）是一种对比学习损失，旨在通过优化单模态编码器（视觉Transformer和文本Transformer），使正样本图文对（匹配的图像和文本）在特征空间中的表示相似，而负样本对（不匹配的图像和文本）表示远离。这种方法通过对齐图像和文本的特征空间，提升模型在跨模态任务中的表现。\n\n具体计算：\n假设我们有一个批次（batch）包含  个图文对  ，其中  是图像， 是对应的文本。\n视觉Transformer将图像  编码为特征向量 ，文本Transformer将文本  编码为特征向量 。\nITC的目标是最大化正样本对  的相似度（例如余弦相似度），同时最小化负样本对的相似度。\n损失函数通常基于InfoNCE（Noise-Contrastive Estimation）形式：  , 其中  是余弦相似度， 是温度参数（控制相似度的缩放）, 第一项是图像到文本的对比损失，第二项是文本到图像的对比损失。\n\n\n功能：\nITC通过对比学习对齐图像和文本的特征空间，使模型能够学习到模态无关的表示，适用于检索任务（例如图像-文本检索）。动量编码器是什么？在训练过程中，我们有一个不断更新的“在线编码器”（Online Encoder），它的参数通过反向传播实时更新。动量编码器（Momentum Encoder）是这个在线编码器的“影子克隆”，但它更新得非常缓慢。\n\n\n工作原理: \n它的参数不通过反向传播来更新。相反，它的参数是“在线编码器”参数的移动平均值。\n , 其中  是一个接近1的动量系数（比如0.995）。这意味着动量编码器在每一步只吸收一小部分在线编码器的新知识，主体上保持了过去一段时间的稳定状态。\n作用: 它提供了一个更稳定的训练目标。如果让在线编码器自己跟自己学（即正样本对的目标特征也由快速变化的在线编码器生成），训练会很不稳定，就像追逐一个不断移动的目标。动量编码器像一个“更沉稳、更有智慧的老师”，为在线编码器提供了一个缓慢变化、更一致的特征作为学习目标，从而稳定了对比学习的训练过程。软标签 (Soft Labels) 是什么？在传统的对比学习中，标签是“硬”的：对于一张图片，与它配对的文本是正样本（标签为1），所有其他文本都是负样本（标签为0）。但这里有个问题：在一个批次中，一张“猫”的图片可能被标记为与另一段描述“一只在睡觉的小猫”的文本为负样本（因为它们不是原始配对），但实际上它们语义上高度相关, 强行将它们的相似度推向0是有害的。\n\n\n工作原理: \n“软标签”就是为了解决这个问题。它不再使用[1, 0, 0, …]这样的硬标签。对于一个给定的图文对(I, T)，它的目标相似度（即标签）是通过动量编码器计算出来的图文相似度 。\n\n\n作用:\n考虑潜在正样本: 如果一个“负样本对”其实在语义上很相似，那么动量编码器算出的相似度会是一个大于0的数值（比如0.3）。模型的目标就是把在线编码器算出的相似度也拉近到0.3，而不是粗暴地推向0。这相当于告诉模型：“虽然它们不是最佳拍档，但也有点关系，别把它们推得太远。”\n提供更丰富的监督信号: 相比于非0即1的硬标签，软标签为模型优化提供了更平滑、更丰富的梯度信息。\n\n\n\n\n\n图文匹配损失 (ITM)图文匹配损失（ITM）是一个二分类任务，目标是让模型学习判断一个图文对是否匹配（正样本）或不匹配（负样本）。它通过基于图像的文本编码器（Image-grounded Text Encoder）生成多模态表示，并使用一个线性层（ITM头）预测匹配概率。\n\n\n具体计算：\n对于一个图文对 ，基于图像的文本编码器生成多模态特征 。\nITM头（线性层）将  映射到一个二分类概率：  , 其中  表示图文对是正样本的概率。\n损失函数是二分类交叉熵：  ，其中  表示正样本， 表示负样本。\n\n\n功能：\nITM损失帮助模型学习图像和文本之间的细粒度对齐，增强多模态表示的能力，适用于任务如视觉问答（VQA）和图像-文本检索。困难负样本挖掘策略 (Hard Negative Mining) 是什么？在一个批次中，我们可以轻易地为一张图片找到很多“简单负样本”。比如，图片是一只狗，负样本文本是“一辆蓝色的汽车”，模型很容易就能判断它们不匹配。用这种简单的样本训练，模型学不到精细的辨别能力。\n\n\n工作原理: \n困难负样本挖掘是一种“择优录取”的训练策略。它专门挑选那些最容易让模型混淆的负样本来训练。\n首先，对于批次中的每一张图片，利用ITC计算出的图文相似度，找出与它最相似的非配对文本。\n这个最相似的文本就是这张图片的“困难负样本”。例如，对于“一只黄色的狗在草地上奔跑”的图片，困难负样本可能是“一只棕色的狗在公园里玩耍”，而不是“一艘船在海上航行”。\n在计算ITM损失时，模型被强制要求区分“正样本”和这些“困难负样本”。\n\n\n\n\n作用: \n强迫模型学习图文之间更细粒度的对应关系。模型不能只看到“狗”，它必须学会分辨“黄色的狗”和“棕色的狗”，“草地”和“公园”，从而真正理解图像和文本的深层语义。\n\n\n\n\n\n语言建模损失 (LM)语言建模损失（LM）用于训练基于图像的文本解码器（Image-grounded Text Decoder），目标是根据图像生成连贯的文本描述。它是一个自回归任务，优化文本序列的似然概率。\n\n\n具体计算：\n假设输入图像  和对应的文本序列 。\n解码器以自回归方式生成文本，预测下一个词的概率：  , 其中  表示前  个词， 是图像特征。\n损失函数是交叉熵损失：   , 其中  是模型预测的词  的概率。\n\n\n功能：\nLM损失使模型能够根据图像生成连贯的文本描述，适用于图像描述生成（Image Captioning）等任务。\n相比掩码语言建模，LM更适合生成任务，因为它直接优化序列的生成能力。标签平滑（Label Smoothing）是什么？标签平滑是一种正则化技术，用于避免模型对预测过于自信，防止过拟合。\n\n\n具体实现：\n在标准交叉熵损失中，目标词的标签是one-hot向量（例如 。\n标签平滑将one-hot标签替换为平滑分布，例如：   , 其中  是平滑系数（BLIP中设为0.1）， 是词汇表大小。 \n例如，若词汇表大小 ，目标词的one-hot标签为 ，平滑后变为 。\n\n\n\n\n损失函数使用平滑后的标签计算交叉熵。为什么用标签平滑？\n防止过拟合：平滑标签减少了模型对单一正确答案的过度依赖，提高泛化能力。\n处理数据噪声：在图像-文本数据中，文本描述可能有噪声（例如不完全准确的标注），标签平滑可以缓解这种影响。\n\n\n\n3.3. 字幕生成与过滤 (CapFilt)CapFilt机制旨在解决网络图文数据质量不佳的问题。高质量的人工标注数据集（例如COCO）数量有限，而网络上自动收集的替代文本（alt-text）则充满噪声。\n\n\n核心流程: CapFilt引入了两个模块：一个用于生成字幕的Captioner和一个用于移除噪声的Filter。如图3所示，这两个模块都是从同一个预训练好的MED模型初始化，然后在高质量的COCO数据集上分别进行轻量化的微调。\n字幕器 (Captioner):\n它本质上是一个图像接地的文本解码器。\n它通过在COCO上进行LM目标微调，学会了为图像生成高质量的描述。 \n在CapFilt流程中，对于每一张网络图片 ，Captioner都会生成一条新的合成字幕 (synthetic caption) 。\n\n\n过滤器 (Filter): \n它是一个图像接地的文本编码器。\n它通过在COCO上进行ITC和ITM目标微调，学会了精确判断图文是否匹配。\n在流程中，Filter会对原始的网络文本  和新生成的合成字幕  进行双重检验。如果Filter的ITM头预测某个文本与图像不匹配，那么该文本就会被视为噪声并被丢弃。\n\n\n数据集自举 (Dataset Bootstrapping):\n最后，经过过滤后保留下来的网络图文对和合成图文对，会与原始的人工标注数据集（如COCO）合并，形成一个全新的、规模更大且质量更高的预训练数据集。\n这个经过“自举”优化的新数据集，将被用来从头开始训练一个新的、性能更强的BLIP模型。 \n\n\n\n4. 实验与讨论 (Experiments and Discussions)本章节首先介绍预训练的详细配置，然后通过一系列消融实验，深入分析和验证所提出方法的有效性。\n4.1. 预训练详情 (Pre-training Details)\n实现与硬件:\n模型使用 PyTorch 框架实现，并在两个拥有16个GPU的节点上进行预训练。\n\n\n模型初始化:\n图像Transformer（ViT）的权重初始化自一个在ImageNet上预训练过的ViT模型。\n文本Transformer的权重则初始化自  的权重。\n\n\n模型变体:\n实验中探索了两种不同规模的视觉骨干网络：ViT-B/16 和 ViT-L/16。\n除非特别指明，论文中报告的所有名为“BLIP”的结果均默认使用ViT-B作为骨干网络。\n\n\n训练超参数:\n优化器采用 AdamW，权重衰减（weight decay）设置为 0.05。\n预训练共进行 20 个 epoch。\n批量大小（batch size）根据模型规模设定，ViT-B为2880，ViT-L为2400。\n学习率采用预热（warm-up）策略，ViT-B的峰值学习率为 ，ViT-L为 ，之后学习率以0.85的速率进行线性衰减。\n\n\n图像分辨率:\n在预训练阶段，使用  分辨率的随机图像裁剪。\n在下游任务微调阶段，为了获得更好的性能，将图像分辨率提升至 。\n\n\n预训练数据集:\n模型使用了与ALBEF方法相同的1400万张图像数据集进行基础预训练。\n该数据集由高质量的人工标注数据集（COCO和Visual Genome）和规模更大但噪声更多的网络数据集（Conceptual Captions、Conceptual 12M和SBU captions）混合而成。\n为了验证方法在大规模噪声数据上的可扩展性，实验还额外使用了一个包含1.15亿张图像的LAION数据集，该数据集的文本噪声程度更高。\n由于LAION数据集规模巨大，在预训练时每个epoch只使用其1/5的数据。\n\n\n\n4.2. CapFilt 的效果分析为了验证字幕生成与过滤（CapFilt）机制的有效性，本节在图像-文本检索和图像字幕生成等下游任务上进行了一系列对比实验。\n\n\n核心作用:\n实验结果表明，在1400万图像数据集上，无论是单独使用字幕器（Captioner）还是单独使用过滤器（Filter），相较于直接使用原始噪声文本的基线模型，性能均有可见的提升。\n当字幕器和过滤器协同工作时，它们的效果能够相互补充，共同带来比基线模型显著的性能飞跃，这充分证明了CapFilt机制的强大作用。\n\n\n可扩展性:\nCapFilt的效果可以随着数据和模型规模的扩大而进一步增强。当应用于更大的数据集（129M图像）和更大的视觉骨干（ViT-L）时，性能提升更为明显，这验证了其在数据和模型两个维度上的良好可扩展性。\n一个值得注意的发现是，使用更强大的ViT-L模型作为字幕器和过滤器，来为ViT-B基础模型的预训练准备数据，同样可以提升最终基础模型的性能。这表明，强大的CapFilt模块可以将其知识有效地“蒸馏”到较小的模型中。\n\n\n定性示例:\n图4直观地展示了CapFilt的工作流程。对于给定的图像，字幕器能够生成全新的、描述性更强的文本（），而过滤器则能有效识别并移除原始网络文本（）和合成文本中的噪声。\n例如，一张图片的原网络文本是“在我家附近的桥上”，这与图片内容（一群鸟飞过湖面）不符，被过滤器拒绝；而模型合成的文本“a flock of birds flying over a lake at sunset”则因与图像匹配而被接受。\n另一个例子中，一张盆栽植物的图片，其网络文本是关于奥地利的一栋房子，被过滤器拒绝；而模型合成的文本“a potted plant sitting on top of a pile of rocks”则被接受。\n\n\n\n\n4.3. 多样性是合成字幕的关键\n\n生成策略对比:\n在CapFilt中，合成字幕是通过核采样 (Nucleus Sampling) 的方式生成的。这是一种随机解码方法，它从一个累积概率超过特定阈值p（实验中p=0.9）的词元集合中进行采样，从而为生成过程引入了随机性和多样性。\n实验将核采样与束搜索 (Beam Search) 进行了比较，后者是一种确定性解码方法，旨在搜索并生成整体概率最高的文本序列。\n\n\n结果与分析:\n实验结果显示，尽管核采样生成的文本被过滤器判定为噪声的比例更高（拒绝率为25%，高于束搜索的19%），但使用这些文本训练的模型在各项下游任务上均表现出明显更优的性能。\n研究者对此的推断是，多样性是根本原因。核采样能够生成更多样化、更出人意料的字幕，这些字幕包含了模型可以学习的新知识和新颖的表达方式。\n相比之下，束搜索倾向于生成“安全”的、在数据集中非常常见的字幕，这些字幕能提供给模型的额外信息较少。因此，模型从信息量更大、更多样化的文本中获益更多。\n\n\n\n4.4. 参数共享与解耦本节探讨了模型设计中的两个关键策略：预训练期间的参数共享和CapFilt微调期间的参数解耦。\n\n\n预训练中的参数共享:\nBLIP的默认策略是让文本编码器和解码器共享除自注意力（SA）层之外的所有参数。\n消融实验的结果表明，这是在性能和效率之间取得最佳平衡的策略。\n如果共享所有层（包括SA层），模型性能会因为编码（需要双向理解上下文）和解码（需要因果预测）任务之间的内在冲突而显著下降。\n如果完全不共享任何参数，虽然性能与默认策略相近，但模型参数量会大幅增加，从而降低了训练和推理的效率。\n\n\n\n\n\nCapFilt中的参数解耦:\n在CapFilt流程中，字幕器和过滤器是从同一个预训练模型初始化后，独立进行微调的，这一过程被称为解耦。\n实验对比了这种解耦策略与在微调时依然让字幕器和过滤器共享参数的策略。\n结果显示，共享参数会导致最终模型在下游任务上的性能下降。\n其主要原因被归结为确认偏差 (confirmation bias)。当参数共享时，过滤器和字幕器的耦合过于紧密，导致过滤器对字幕器产生的噪声字幕“不够严格”，不容易将其过滤掉。这一点可以从噪声拒绝率的显著降低（从25%降至8%）得到验证。因此，将两者解耦，让它们独立微调，是保证过滤效果、提升数据质量的关键。\n\n\n\n5. 与最先进技术的比较 (Comparison with State-of-the-arts)本章节将BLIP模型与现有的视觉语言预训练（VLP）方法，在一系列广泛的下游任务上进行全面比较，以验证其性能。值得注意的是，实验基准中省略了SNLI-VE任务，因为其测试数据据报道存在噪声问题。\n5.1. 图文检索 (Image-Text Retrieval)\n任务与数据集: 该任务评估模型在给定文本时检索相关图像（Text-to-Image Retrieval, IR）和在给定图像时检索相关文本（Image-to-Text Retrieval, TR）的能力。实验在COCO和Flickr30K这两个标准数据集上进行。\n微调与推理策略:\n微调阶段，模型使用图文对比损失（ITC）和图文匹配损失（ITM）进行联合优化。\n为了在推理时兼顾速度和精度，采用了两阶段的检索策略：\n候选筛选: 首先，利用计算速度较快的图文特征相似度（通过ITC学习得到）快速地从整个库中筛选出前k个最相关的候选者。\n重排序: 然后，仅对这k个候选者计算计算成本更高但更精确的图文匹配（ITM）分数，并以此进行重排序，得到最终结果。\n\n\n在COCO数据集上，k设为256；在Flickr30K上，k设为128。\n\n\n结果分析:\n微调后检索 (Finetuned Retrieval): 如表5所示，BLIP的性能全面超越了现有方法。特别是在使用相同的1400万预训练图像时，BLIP在COCO数据集上的平均Recall@1指标比之前的最佳模型ALBEF高出2.7%。\n零样本检索 (Zero-shot Retrieval): 如表6所示，将在COCO上微调好的模型直接迁移到Flickr30K上进行测试（零样本设置），BLIP同样以巨大优势超越了包括CLIP和ALIGN在内的现有方法。\n\n\n\n\n5.2. 图像字幕生成 (Image Captioning)\n任务与数据集: 该任务要求模型为给定图像生成描述性文本。实验在COCO数据集上进行微调和评估，并在NoCaps数据集上进行评估，以测试模型对新物体的泛化能力。\n微调与推理策略: 模型使用语言建模损失在COCO训练集上进行微调。在推理时，研究者发现，在待生成的字幕开头添加提示语“a picture of”（一张……的图片）可以略微提升效果。\n结果分析:\n如表7所示，使用1400万图像预训练的BLIP，其性能显著优于使用相似规模数据预训练的其他方法。\n使用12900万图像预训练的BLIP，其性能与使用2亿图像的LEMON模型相当。\n尤其重要的是，BLIP在实现高性能的同时，推理效率远高于LEMON等方法，因为它不依赖于计算成本高昂的物体检测器，并且使用的输入图像分辨率更低（ vs ）。\n\n\n\n\n5.3. 视觉问答 (Visual Question Answering, VQA)\n任务与数据集: VQA任务要求模型根据图像内容回答一个相关问题。实验在VQA2.0数据集上进行。\n任务范式: 与许多将VQA视为多答案分类任务的方法不同，BLIP创新地将其视为一个答案生成 (answer generation) 任务。这种范式使得模型能够处理开放式的问题，而不是从预设的答案列表中选择。\n模型架构与微调: 如图5(a)所示，在微调时，模型被重组：图像和问题首先被编码为多模态嵌入，然后该嵌入被送入一个答案解码器中，以自回归的方式生成答案。模型使用LM损失进行端到端的微调。\n结果分析:\n如表8所示，使用1400万图像预训练的BLIP，在VQA测试集上的得分比ALBEF高出1.64%。\n使用12900万图像预训练的BLIP，其性能甚至优于使用了13倍训练数据（1.8B）和更复杂视觉骨干网络的SimVLM。\n\n\n\n\n5.4. 自然语言视觉推理 (Natural Language Visual Reasoning, )\n任务与数据集: 该任务要求模型判断一个句子是否准确地描述了一对（两张）图像，需要模型具备跨图像的推理能力。\n模型架构修改: 为了处理双图像输入，研究者对预训练模型做了一个简单而高效的修改。如图5(b)所示，在图像接地的文本编码器的每个Transformer块中，设置了两个并行的交叉注意力（CA）层，分别处理两张输入图像。这两个CA层由同一个预训练权重初始化。它们的输出在一个合并层（Merge Layer）中被融合，然后再送入前馈网络（FFN）。这种设计比之前的方法在计算上更高效。\n结果分析: 如表8所示，BLIP的性能优于所有现有方法，除了进行了额外定制化预训练的ALBEF。一个有趣的发现是，增加更多的网络图像数据对任务的性能提升不大，这可能是由于网络数据与该任务所需的数据领域之间存在较大差异。\n\n5.5. 视觉对话 (Visual Dialog, VisDial)\n任务与数据集: 该任务将VQA扩展到多轮对话场景，模型需要根据图像、当前问题以及之前的对话历史来预测答案。实验遵循判别式设定，即从一个候选答案池中选出正确答案。\n模型架构与微调: 如图5(c)所示，图像和其标题的嵌入被拼接后，通过交叉注意力送入一个对话编码器。该编码器结合问题和对话历史，使用ITM损失进行训练，以判断候选答案对于当前对话上下文是否正确。\n结果分析: 如上面的表9所示，BLIP在VisDial v1.0验证集上取得了最先进的性能。\n\n5.6. 向视频-语言任务的零样本迁移\n核心思想: 为了验证BLIP学到的图文表征的强大泛化能力，研究者将其直接应用于视频-语言任务，而不进行任何针对视频数据的微调。\n视频处理策略: 采用了一种非常简单的方法来处理视频输入：从每个视频中均匀采样N帧（检索任务N=8，问答任务N=16），然后将这些帧的特征拼接成一个单一的序列输入给模型。这种方法完全忽略了视频中的时序信息。\n结果分析:\n尽管存在图像和视频之间的领域差异，并且缺乏任何时序建模，BLIP在文本到视频检索（表10）和视频问答（表11）两个任务上均取得了惊人的、最先进的零样本性能。\n尤其是在文本到视频检索任务上，零样本的BLIP性能甚至比那些在目标视频数据集上进行过完全微调的模型还要高出12.4% (Recall@1)，这强有力地证明了BLIP框架学习到的视觉-语言对齐能力的高度通用性。\n\n\n\n\n6. 额外的消融研究 (Additional Ablation Study)本章节通过额外的消融实验，进一步论证CapFilt机制的有效性并非源于其他无关因素。\n\nCapFilt的提升是否源于更长的训练时间？\n问题: 由于CapFilt生成的自举数据集比原始数据集包含更多的文本样本，因此在训练相同epoch数的情况下，其实际训练步数更多。性能提升是否仅仅是因为训练时间更长？\n实验设计: 为了验证这一点，研究者将原始的噪声网络文本进行复制，使其每个epoch的训练样本数与自举数据集完全相同。\n结果与结论: 如表12所示，仅仅在原始噪声数据上进行更长时间的训练并不能带来性能提升。这证明了CapFilt的有效性来自于其提升了数据质量，而非简单地增加了训练时长。\n\n\n\n\n\n使用自举数据集时是否需要训练新模型？\n问题: 在获得自举数据集后，是应该用它从头训练一个新模型，还是可以直接在生成该数据集的旧模型上继续训练？\n实验设计: 实验比较了“从头训练新模型”与“在旧模型上继续训练”两种策略。\n结果与结论: 如表13所示，从头训练一个新模型的效果优于在旧模型上继续训练。这一发现与知识蒸馏领域的普遍认知相符，即学生模型通常不应从教师模型的权重初始化，以避免陷入次优解，并能更好地从高质量数据中学习。在更高质量的数据集上从零开始，能让模型探索更优的参数空间。\n\n\n\n\n","slug":"BLIP论文精读","date":"2025-07-06T01:10:15.000Z","categories_index":"","tags_index":"多模态,BLIP","author_index":"犬夜叉"},{"id":"b18ad5e5297b60924e424af2ef6c52c5","title":"Flamingo论文精读","content":"Introduction1. 核心挑战与现有方法的局限性多模态机器学习领域面临一个核心的开放性挑战：如何构建能够仅凭少数几个标注样本就迅速适应新任务的模型。 这种快速学习能力是智能的一个关键特征，即在接收到简短指令后能学会执行新任务。 \n目前，计算机视觉领域最广泛应用的范式仍然是“预训练-微调”（pre-training and fine-tuning）。 这个过程通常包括两个阶段：\n\n预训练阶段：在一个大规模的、有监督的数据集上预先训练模型。\n微调阶段：在目标任务的特定数据集上对预训练好的模型进行参数微调。 \n\n然而，这种主流范式存在显著的缺点：\n\n数据依赖性强：成功的微调往往需要成千上万个为特定任务标注的数据点，获取这些数据成本高昂。\n\n高昂的调优成本：针对每个新任务，都需要进行仔细的超参数调整，这是一个繁琐且耗费计算资源的过程。\n\n资源密集：整个微调过程需要大量的计算资源。\n\n\n为了克服微调的限制，近年来出现了一些新的方法，但它们同样有其局限性：\n\n对比学习模型 (Contrastive Models)：像CLIP这样的多模态视觉语言模型，通过对比学习目标进行训练，实现了对新任务的“零样本”适应，无需微调。 它们的工作原理是为文本和图像学习一个共享的嵌入空间，并计算两者之间的相似度分数。 这种机制限制了它们的应用场景，主要适用于分类等选择题式的任务，即从一个预先给定的有限选项中做出选择。 关键的缺陷在于，这类模型无法生成语言，这使得它们不适用于更开放式的任务，例如视觉问答（VQA）或图像描述（Captioning），因为这些任务需要模型生成自由形式的文本答案。 \n\n视觉条件下的语言生成模型 (Visually-conditioned Language Generation Models)：虽然有一些研究探索了根据视觉输入生成文本的模型，但这些模型在数据量较少（即小样本）的情况下，尚未展现出足够好的性能。 \n\n\n图1：从Flamingo-80B获得的输入和输出精选示例。Flamingo可通过少样本提示快速适应各种图像/视频理解任务。此外，Flamingo原生支持多图像视觉对话。\n2. Flamingo模型的提出与核心能力为了解决上述挑战，本研究引入了一个名为Flamingo的视觉语言模型系列。 Flamingo的核心突破在于其卓越的小样本学习（few-shot learning）能力。 \n\n工作范式：Flamingo通过“提示（prompting）”来适应新任务。用户只需向模型提供几个包含输入/输出对的样本（例如，&lt;图片，对应描述&gt; 或 &lt;图片，问答对&gt;），模型就能理解任务要求并对新的查询图片或视频生成相应的文本输出。 这种方式极大地降低了对大量标注数据的依赖。\n\n卓越的性能：在一个包含16个不同的视觉和语言任务的广泛评测中，Flamingo展现了最先进的小样本学习性能。 更为引人注目的是，在其中的6个任务上，Flamingo仅使用极少数的样本（例如32个），其性能就超越了在数千甚至数万倍任务专属数据上进行微调的现有最先进模型。 \n\n\n\n3. 设计思想：将大语言模型的能力扩展至多模态领域Flamingo的设计灵感主要来源于近年来在大型语言模型领域取得的巨大成功，例如GPT-3等模型展现出的强大的小样本学习能力。 \n\nLMs的小样本学习机制：一个强大的大型语言模型能够仅通过其文本接口来执行多种任务。具体做法是，将任务的几个示例（examples）和新的查询输入（query input）一起打包成一个文本提示（prompt），然后模型会自回归地生成一个续写，这个续写就是对查询的预测输出。 \n\n将该机制迁移至视觉任务：本研究证明，同样的方法论可以被成功地应用于图像和视频理解任务。 像分类、描述生成、视觉问答等任务，都可以被重新定义和构建为以视觉输入为条件的文本预测问题。 \n\n与纯语言模型的关键区别：与仅处理文本的LM不同，视觉任务需要模型能够处理一个多模态的提示（multimodal prompt），这个提示中包含了与文本交错在一起的图像或视频。 Flamingo模型的核心能力之一就是处理这种任意交错的视觉和文本序列。 \n\n\n4. Flamingo的架构理念与关键组件Flamingo模型是一个能够接收文本、图像、视频交错序列作为输入，并生成自由文本作为输出的视觉条件自回归文本生成模型。 其架构设计的核心理念是有效地连接和利用两个强大的、预训练好的互补模型，同时保留它们在各自领域学到的丰富知识。\n\n利用预训练模型：Flamingo架构的基础是两个预训练且被冻结（frozen）的模型：\n\n一个视觉模型，负责“感知”视觉场景。 \n\n一个大型语言模型，负责执行基本的推理和文本生成。 \n\n\n\n创新的桥接设计：在冻结的视觉和语言模型之间，引入了全新设计的、从零开始训练的架构组件。 这种“桥接”方式至关重要，因为它可以在不破坏原有模型知识（防止灾难性遗忘）的前提下，高效地将视觉信息融入到语言模型的处理流程中。 \n\n高效处理高分辨率视觉输入：为了处理高分辨率的图像或视频，Flamingo采用了一个基于Perceiver的架构。 该模块可以将视觉编码器产生的大量、可变数量的视觉特征，压缩并重采样成一小组固定数量的“视觉令牌（visual tokens）”。 这一设计极大地提高了处理效率。\n\n\n\n5. 训练策略的关键性大型语言模型的强大性能很大程度上归功于其在海量文本数据上的训练，这赋予了它们通用的文本生成能力，从而在接收到任务提示时表现出色。 \n\n训练数据的重要性：与此类似，本研究证明，Flamingo模型的训练方式对其最终的性能至关重要。 \n\n独特的训练数据：模型在一个精心挑选的、大规模、多模态的网络语料库上进行训练。这些数据的一个关键特征是包含了任意交错的文本和图像，这与传统的成对（image-text pair）数据有本质区别。 \n\n实现小样本能力的关键：正是这种在真实网络页面数据上的训练，才赋予了Flamingo强大的上下文小样本学习（in-context few-shot learning）能力。 经过这样的训练后，模型无需任何针对特定任务的微调，就能直接通过小样本提示的方式适应新的视觉任务。 \n\n\nApproach本节详细阐述了Flamingo模型的架构设计、工作原理与训练策略。Flamingo是一个视觉语言模型，其核心功能是接收以任意方式穿插的文本与图像/视频序列作为输入，并以自回归的方式生成自由形式的文本作为输出。模型设计的指导思想是高效地桥接两个强大的、已预训练好的独立模型——一个用于视觉感知的模型和一个用于语言理解与生成的模型，从而在不破坏各自预训练知识的前提下，实现强大的多模态处理能力。\n2.1 视觉处理与Perceiver Resampler模块视觉信息的处理是整个模型的第一步，其目标是将原始的像素输入转化为紧凑且固定长度的、可供语言模型利用的表征。\n\n视觉编码器 (Vision Encoder)：\n\n模型选择：采用了一个预训练好且在整个Flamingo训练过程中保持冻结的Normalizer-Free ResNet (NFNet) 模型（具体为F6版本）。冻结视觉编码器可以保留其强大的、泛化的视觉特征提取能力，并节省大量计算资源。\n预训练方式：该视觉编码器是在大规模图文对数据集上通过对比学习目标独立预训练的。这种训练方式旨在让模型学习到一种通用的视觉表示，使其能够理解图像内容并与文本描述对齐。\n特征输出：对于图像输入，编码器输出其网络末端的一个二维空间特征图。这个特征图保留了图像的空间信息，随后被展平为一维的特征序列。对于视频输入，首先以1帧/秒的速率对视频进行采样，然后独立地对每一帧进行编码，得到一系列的帧特征。为了融入时序信息，模型会为这些帧特征添加一个可学习的时序嵌入，形成一个三维时空特征网格。最后，这个三维特征同样被展平为一维序列，准备送入下一模块。\n\n\nPerceiver Resampler (感知器重采样器)：\n\n核心功能与目的：该模块是连接视觉编码器和冻结语言模型的关键桥梁。它的核心任务是接收来自视觉编码器的大量且可变长度的视觉特征（无论是来自高分辨率图像还是长视频），并将其压缩成少数固定数量的视觉令牌，在本研究中固定为64个。\n解决的痛点：直接将高维度的视觉特征输入到大型语言模型中进行注意力计算，其计算成本会非常高昂。Perceiver Resampler通过显著减少视觉令牌的数量，极大地降低了后续视觉-文本交叉注意力（cross-attention）的计算复杂度，使得整个模型更加高效。\n工作机制：其设计借鉴了Perceiver和DETR等模型的思想。它定义了一组预设数量的、可学习的潜在输入查询（latent input queries）。这些查询向量作为“信息汇总器”，通过一个Transformer结构中的交叉注意力机制，去“观察”和“查询”由视觉编码器生成的大量视觉特征。通过这个过程，它们将视觉特征中的核心信息“蒸馏”并吸收到自身中。最终，这些经过信息蒸馏的查询向量的输出，就构成了那一小组固定数量的视觉令牌。实验证明，这种方法比使用简单的多层感知机或标准的Transformer来进行特征池化效果更优。\n\n\n\n2.2 在视觉表征上对冻结语言模型进行条件化模型的文本生成能力由一个强大的、预训练好的冻结语言模型提供。为了让这个LM能够“看到”图像内容，需要将Perceiver Resampler产生的视觉令牌有效地融入其处理流程中。\n\n\nGATED XATTN-DENSE（门控交叉注意力-稠密连接）层：\n\n非侵入式集成：为了保留LM强大的预训练知识并防止“灾难性遗忘”（即模型在学习新知识时忘记旧知识），Flamingo不直接微调LM的参数。相反，它在原始LM的各个预训练层之间，插入了若干个全新的、从零开始训练的模块，即GATED XATTN-DENSE层。\n结构与功能：\n交叉注意力 (XATTN)：这是实现视觉与语言融合的核心。在这一层中，注意力机制的查询（Query, Q）来自于前一个冻结LM层的文本表征，而键（Key, K）和值（Value, V）则来自于Perceiver Resampler输出的视觉令牌。这使得在生成每一个文本词元时，模型都能够有针对性地“关注”视觉输入中的相关区域。\n稠密连接 (DENSE)：交叉注意力层之后连接一个标准的前馈神经网络（Feed-Forward Network），进行进一步的特征转换。\n门控机制 (GATED)：这是一个对训练稳定性和最终性能至关重要的设计。新添加模块的输出并不会直接与原始的文本表示相加。相反，它会经过一个tanh门控机制。该机制通过一个可学习的标量参数  来控制新模块的输出贡献。这个  被初始化为0。\n\n\n初始化优势：由于在初始化时 ，所以 tanh(α) 也为0，导致整个新添加模块的输出在训练开始时为零。这意味着，在训练初期，整个Flamingo模型的输出与那个未经改动的、冻结的LM完全相同。这保证了训练的稳定启动，避免了随机初始化的新层对强大预训练模型的干扰。随着训练的进行，模型会逐渐学习调整  的值，从而平滑地、自适应地将视觉信息融合进来。\n\n\n模型规模：研究团队基于Chinchilla系列语言模型构建了三种不同规模的Flamingo：Flamingo-3B, Flamingo-9B, 和 Flamingo-80B。在扩大模型尺寸时，主要是增加了冻结LM的参数量以及可训练的GATED XATTN-DENSE模块的数量，而视觉编码器和Perceiver Resampler的尺寸在不同规模的模型间保持不变。\n\n\n2.3 多视觉输入支持：逐图像/视频的注意力掩码为了让模型能够处理包含多个视觉输入的提示（例如，在小样本学习场景下，提示中包含多个&lt;图像, 文本&gt;对），并正确地将文本与对应的视觉输入关联起来，Flamingo采用了一种精巧的注意力掩码策略。\n\n工作机制：在进行文本到图像的交叉注意力计算时，模型采用了因果掩码（causal masking）。具体来说，当模型正在预测某个文本词元时，它的交叉注意力模块被限制为只能关注（attend to）在交错序列中紧邻于它之前的那个图像/视频所对应的视觉令牌。模型不能直接通过交叉注意力“回顾”更早出现的其他所有图像。\n信息流的保持：尽管交叉注意力被限制在单个最近的图像上，但对先前所有图像的信息依赖性并不会丢失。这些信息是通过语言模型内部的自注意力机制来间接维持的。当模型处理完一个&lt;图像, 文本&gt;对后，视觉信息已经影响了生成的文本，这些文本作为历史信息存储在LM的状态中。在后续步骤里，LM可以通过自注意力机制访问和利用这些包含了早前视觉信息的状态。\n核心优势：这种“单次只看一张图”的交叉注意力方案，不仅计算高效，更重要的是赋予了模型极佳的泛化能力。它使得模型可以无缝地处理任意数量的视觉输入，即使在训练时接触的图像数量有限（例如，训练时最多使用5张图），在推理时也能从多达32个图文对中获益。\n\n2.4 在混合视觉与语言数据集上的训练Flamingo的小样本学习能力严重依赖于其独特的训练数据和策略。模型在一个混合了三种从网络上爬取的数据集上进行训练，未使用任何专为机器学习目的而人工标注的数据。\n\n数据集构成：\n\nM3W (MultiModal MassiveWeb)：这是一个包含约4300万个网页的交错图文数据集。通过解析网页的DOM树结构，将图像以&lt;image&gt;标签的形式插入到其在原文中相应位置的文本流中，从而构建出自然的、图文混排的训练样本。这是训练模型理解上下文和进行小样本学习的关键。\n图文对数据集：这部分由两块组成，一是公开的ALIGN数据集（18亿图文对），二是团队自己收集的、描述更长更优质的LTIP数据集（3.12亿图文对）。\n视频文本对数据集 (VTP)：一个包含2700万个短视频及其文本描述的数据集。\n\n\n语法对齐与多目标训练：\n\n为了统一训练格式，所有图文对和视频文本对数据都被处理成与M3W相似的语法，即在文本描述前后分别加上&lt;image&gt;和&lt;EOC&gt;（end of chunk）特殊标记。\n论文通过最小化给定视觉输入时每个数据集的文本期望负对数似然加权和来训练模型： , 其中是输入文本的第个语言标记，是前面的标记集合，是交错序列中先于标记的图像/视频集合，和分别表示第个数据集及其权重。调整每个数据集的权重是提升性能的关键。该论文对所有数据集的梯度进行累积，实验表明该方法优于“轮询”策略。\n在优化策略上，团队发现梯度累积的方式，即计算完所有数据集的梯度后再进行一次统一的参数更新，比轮流在单个数据集上训练的“round-robin”方法效果更佳。\n\n\n\n2.5 利用小样本上下文学习进行任务适配模型一旦训练完成，就可以通过上下文学习（in-context learning）的方式快速适应新的视觉任务，而无需任何参数更新。\n\n提示构建 (Prompting)：为了解决一个新任务，用户需要构建一个多模态提示。这个提示由若干个“支持样本”和一个“查询样本”组成。例如，可以这样构建提示：&lt;图片1&gt; &lt;答案1&gt; &lt;图片2&gt; &lt;答案2&gt; ... &lt;查询图片&gt;。模型在看到这个提示后，会续写出针对查询图片的答案。\n评估方式：\n对于开放式任务（如VQA），使用集束搜索解码策略来生成最可能的自由文本答案。\n对于封闭式任务（如多项选择），模型会分别计算每个选项作为答案的对数似然概率（log-likelihood），并选择概率最高的那个作为最终答案。\n\n\n零样本泛化：研究还探索了一种特殊的零样本设置，即在提示中只提供任务的纯文本示例（不附带图像），以测试模型是否能仅从文本描述中理解任务的格式和要求。\n\nExperiments本部分旨在通过一系列广泛的实验来评估Flamingo模型的性能，核心目标是验证其在多样化且具有挑战性的任务上快速适应的能力。\n实验设置与评估策略\n\n评测基准：为了全面评估模型，实验覆盖了16个当前流行的多模态基准数据集。这些数据集涵盖了图像和视频理解的多个方面，包括图像描述、视频描述、视觉问答、视频问答、视觉对话以及多项选择题等。\n\n开发集与留出集（Held-out Set）：为了保证评估的公正性和科学性，实验将这16个基准分为了两组：\n\n开发集（DEV Set）：包含5个基准（COCO, OKVQA, VQAv2, MSVDQA, VATEX）。这组数据集在研究过程中被用来验证模型的设计决策和调整超参数。研究者承认，由于模型在开发阶段“看到”了这些任务，其在这些基准上的最终性能评估可能存在偏向性（即可能被高估），但这种做法在领域内是普遍的。\n留出集：包含其余的11个基准。这组数据集在模型设计和超参数选择的整个过程中都未被使用。它们仅在最后阶段被用来评估模型的最终性能，从而为模型的小样本学习能力提供一个无偏的、更可信的估计。\n\n\n数据划分：为了在开发集上进行严谨的评估并避免数据泄露，每个开发集基准都被划分为四个子集：验证集的支持样本（用于开发阶段构建提示）、验证集的查询样本（用于开发阶段评估）、测试集的支持样本（用于最终报告构建提示）、测试集的查询样本（用于最终报告评估）。对于留出集，则只需要测试集的支持和查询样本。\n\n评估超参数：为了证明模型的通用性，所有评估用的超参数（如解码策略等）在全部16个基准上都保持固定。根据任务的性质（例如是生成式还是选择式），会采用四种预设的提示模板中的一种。\n\n\n3.1 小样本学习性能这是实验的核心部分，旨在衡量模型在仅有少量标注样本的情况下学习新任务的能力。\n\n与先前小样本方法的对比：实验结果表明，在所有16个评测基准上，Flamingo的性能都显著超越了以往所有已发表的零样本或小样本方法。这一成就仅需每个任务提供极少数（例如4个）的示例即可实现，充分展示了模型高效且实用的任务适应能力。\n\n\n\n与完全微调（Fine-tuned）方法的对比：更引人注目的是，Flamingo的性能不仅在小样本领域领先，甚至能与那些在成千上万、乃至数十万个任务专属标注数据上进行完全微调的当前最先进（SOTA）模型相媲美。在其中的6个任务上，Flamingo仅凭一个通用的、未经微调的模型和32个任务样本，就超越了这些经过大量数据微调的SOTA模型。\n\n泛化能力验证：模型在11个留出基准上的强劲表现，证实了其设计和训练方法的泛化能力。这表明模型的优异性能并非过拟合于开发集，而是具备广泛适用性的。\n\n规模效应分析（Scaling Analysis）：\n\n模型参数规模：性能随着模型参数量的增加而稳步提升，即Flamingo-80B优于Flamingo-9B，后者又优于Flamingo-3B。这与在大型语言模型领域观察到的“规模法则”（Scaling Law）相一致，即更大的模型通常具备更强的学习能力。\n样本数量（Number of Shots）：对于同一个模型，其性能随着在提示中提供的上下文样本（shots）数量的增加而提高。\n协同效应：研究发现，最大的模型（Flamingo-80B）能更好地利用更多的上下文样本。这表明模型规模和上下文信息量之间存在协同效应，更大的模型能更有效地从示例中学习。\n架构灵活性的体现：一个非常关键的发现是，尽管模型在训练阶段最多只接触过包含5张图像的序列，但在推理（评估）时，它却能有效利用多达32张图像或视频的上下文信息并持续提升性能。这有力地证明了Flamingo架构（特别是其逐图像的注意力掩码机制）的灵活性和卓越的泛化能力。\n\n\n\n3.2 Flamingo作为预训练模型进行微调的性能尽管小样本学习是核心焦点，本部分也探索了当有充足标注数据时，将Flamingo作为预训练模型进行传统微调的潜力。\n\n微调方法：研究团队对最大规模的Flamingo模型，在有大量标注数据的任务上进行微调。微调过程采用了一个较短的训练周期和较小的学习率。一个关键的改动是，在微调期间解冻了视觉主干网络。这使得模型的视觉部分也能针对特定任务进行调整，例如适应更高分辨率的图像输入。\n\n微调结果：通过微调，模型的性能在之前小样本学习的基础上得到了进一步提升。在那些之前通过小样本学习未能达到SOTA的9个任务中，微调后的Flamingo在其中的5个任务上（VQAv2, VATEX, VizWiz, MSRVTTQA, HatefulMemes）刷新了最先进记录（SOTA）。\n\n\n\n3.3 消融研究为了理解模型各个组件和设计选择的重要性，研究者进行了一系列消融实验。这些实验主要在较小的Flamingo-3B模型上进行，以节省计算成本。\n\n训练数据组合的重要性：\n\n移除交错图文数据集会导致模型性能出现超过17%的灾难性下降，这证明了在自然混排的图文数据上进行训练对于培养小样本学习能力至关重要。\n移除传统的图文对数据集同样会使性能下降近10%，这表明交错数据和成对数据是互补的，两者都不可或缺。\n移除视频-文本数据集会对所有视频相关任务的性能产生负面影响。\n\n\n视觉条件化架构的关键设计：\n\n门控机制：在融合视觉和文本信息时使用的tanh门控机制至关重要。移除这个机制不仅导致性能下降4.2%，还会引发训练过程的不稳定。\n交叉注意力架构：实验证明，本文提出的GATED XATTN-DENSE架构优于其他可替代的方案，如vanilla交叉注意力或一些“嫁接”（grafting）方法。\n\n\n计算与性能的权衡：\n\n交叉注意力频率：在语言模型的每一层之间都插入新的交叉注意力模块能获得最佳性能，但这会显著增加训练的参数量和时间复杂度。实验发现，每隔4层插入一个模块是一个极佳的权衡点，它能将训练速度提升66%，而整体性能仅有1.9%的微小下降。基于这个发现，更大的模型采用了这种稀疏插入策略以在硬件限制下实现最优配置。\nResampler架构：Perceiver Resampler在性能和速度上均优于使用普通MLP或Transformer作为重采样器的替代方案。\n\n\n视觉编码器的影响：使用一个强大的视觉编码器非常重要。团队自己预训练的NFNet-F6编码器显著优于公开的CLIP ViT-L/14模型，证明高质量的视觉特征是模型性能的基石。\n\n冻结语言模型的必要性：这是整个方法论中最关键的发现之一。\n\n如果从零开始训练语言模型部分，性能会暴跌12.9%。\n更重要的是，即便是微调（而非冻结）预训练好的语言模型，也会导致8%的显著性能下降。这清晰地揭示了“灾难性遗忘”现象：在适应新的多模态目标时，语言模型会忘记其预训练阶段学到的宝贵的、通用的语言知识。因此，冻结语言模型是保证性能的必要手段，是一种比将纯文本预训练数据混入多模态训练中更优的策略。\n\n\n\n\nRelated Work本节将Flamingo模型置于现有研究的广阔背景之下，阐述其与相关工作的联系与区别，主要涵盖三个领域：语言建模与小样本适应、视觉与语言的交叉研究，以及网络规模的训练数据集。\n1. 语言建模与小样本适应\n技术背景：近年来，基于Transformer架构的语言模型取得了巨大进步，“预训练-再适应”已成为标准范式。Flamingo正是构建在这一坚实基础之上，其核心语言模块采用了强大的Chinchilla 70B大型语言模型。\n\n模型适应技术的多样性：将预训练好的大型语言模型适配到新任务上有多种技术路径：\n\n适配器模块（Adapter Modules）：在预训练模型的层与层之间插入一些小型的、可训练的神经网络模块，在适配新任务时只训练这些适配器，而保持主干模型冻结。\n部分参数微调：只微调模型中一小部分的参数（例如，只微调偏置项 bias），在保持大部分参数不变的情况下实现任务适配。\n提示优化（Prompt Optimization）：保持模型完全不变，而是通过梯度下降等方法来学习最优的、能够引导模型产生正确输出的输入提示。\n上下文学习（In-context Learning）：这是Flamingo采用的思路，其灵感直接来源于GPT-3等模型。这种方法无需任何梯度更新或参数修改，仅通过在模型的输入中提供几个任务示例，就能引导模型在推理时执行新任务。\n\n\nFlamingo的选择与定位：相较于其他需要复杂梯度优化的方法，Flamingo选择了更为简洁的上下文学习路径。它避开了基于度量学习（metric learning，旨在学习一个好的样本间相似度函数）或元学习（meta-learning，即“学会如何学习”，旨在让模型能从少量数据中快速学习）等更为复杂的少样本学习框架，转而将纯文本领域的上下文学习成功地推广到了多模态领域。\n\n\n2. 当语言与视觉相遇\nBERT的深远影响：语言模型（尤其是BERT）的成功极大地启发了视觉语言领域的研究。大量V-L模型借鉴了BERT的架构，使用Transformer来融合视觉和文本特征。然而，这些模型与Flamingo的一个根本区别在于，它们大多需要在下游任务上进行微调才能获得良好性能，而Flamingo则专注于无需微调的小样本学习。\n\n对比学习模型：这是V-L领域的另一大分支（如CLIP）。这类模型通过对比学习来对齐图像和文本的表示，从而计算它们之间的相似度。虽然Flamingo的视觉编码器本身是使用对比学习预训练的，但Flamingo模型整体的功能远超于此。对比学习模型只能进行“打分”，无法生成自由形式的文本，而Flamingo的核心能力之一正是生成性。\n\n自回归视觉语言模型：Flamingo属于能够自回归生成文本的VLM家族。在它出现的同时期，也有其他一些工作探索了将多种视觉任务统一表述为文本生成问题的范式。\n\n基于冻结语言模型的研究趋势：为了防止在多模态训练中破坏大型语言模型预训练好的强大能力（即“灾难性遗忘”），近期的一系列工作开始探索冻结语言模型的方案。Flamingo正是这一思想的践行者和集大成者。\n\nFlamingo的核心创新：尽管存在上述种种相关工作，Flamingo的独特性和核心创新在于，它是首个能够处理任意交错（arbitrarily interleaved）的图像、视频和文本序列的语言模型。这与之前大多数只能处理单个图像/视频与文本对的模型相比，是一个质的飞跃，使其能够处理更复杂、更自然的真实世界多模态场景。\n\n\n3. 网络规模的视觉与语言训练数据集\n数据瓶颈：高质量、人工标注的视觉语言数据集（如COCO, VQA）规模通常在数万到数十万级别，获取成本高昂，这限制了模型的扩展性。\n网络数据的利用：为了突破这一瓶颈，许多研究转向从互联网上自动爬取海量的、自然存在的图文对数据。\nFlamingo的贡献：Flamingo不仅利用了这种大规模的图文对数据，还进一步证明了训练数据的形态至关重要。它的一个关键贡献是证明了在包含交错图文的完整网页上进行训练的巨大价值。这种将网页视为一个单一、连贯的多模态序列的训练方式，是其强大上下文学习能力的关键来源。\n与同期工作的比较：同期的CM3模型也使用了网页数据进行训练。但两者目标不同：CM3旨在生成HTML标记语言，而Flamingo将任务简化为生成纯文本；在评估上，CM3更侧重于语言任务，而Flamingo的重点是验证在各类视觉任务上的小样本学习能力。\n\n","slug":"flamingo论文精读","date":"2025-07-05T02:32:50.000Z","categories_index":"","tags_index":"多模态,小样本","author_index":"犬夜叉"},{"id":"5cac6105e8bc407ec91f6896ed602917","title":"ALBEF论文精读","content":"\n\n\n\n\n\n摘要\n大规模视觉语言表示学习已在各类视觉语言任务上展现出显著改进。现有大多数方法采用基于Transformer的多模态编码器来联合建模视觉标记（基于区域的图像特征）和文本标记。由于视觉标记与文本标记处于未对齐的空间，多模态编码器难以学习图像-文本的交互关系。在本文中，我们提出一种对比损失，用于在通过跨模态注意力融合图像和文本表示之前先进行对齐（即ALBEF），这使得视觉语言表示学习更具语义根基。与大多数现有方法不同，我们的方法无需边界框标注，也不需要高分辨率图像。为提升从噪声Web数据中学习的能力，我们提出动量蒸馏（Momentum Distillation），这是一种自训练方法，通过动量模型生成的伪目标进行学习。我们从互信息最大化的视角对ALBEF进行了理论分析，表明不同训练任务可解释为为图像-文本对生成“视图”的不同方式。ALBEF在多个下游视觉语言任务上实现了SOTA性能：在图像-文本检索任务中，ALBEF超越了在数据规模大若干数量级的数据集上预训练的方法；在VQA和NLVR2任务中，ALBEF相比SOTA方法分别实现了2.37%和3.84%的绝对性能提升，同时推理速度更快。代码和模型见https://github.com/salesforce/ALBEF。\n\n\n论文链接：[https://arxiv.org/abs/2107.07651]\nIntroduction1. 视觉-语言预训练（VLP）的现状与挑战视觉-语言预训练（VLP）旨在通过大规模图像-文本对学习多模态表示，以提升下游任务性能。现有主流方法（如LXMERT、UNITER、OSCAR）依赖预训练的物体检测器提取基于区域的图像特征，并通过多模态编码器融合图像与文本标记。然而，这类方法存在以下关键局限性：  \n\n（1）模态特征未对齐：图像区域特征与文本标记处于不同语义空间，多模态编码器难以有效建模跨模态交互关系。  \n（2）物体检测器的高成本：预训练需边界框标注，推理时依赖高分辨率图像。  \n（3）噪声数据过拟合：主流图像-文本数据集收集自网络，存在大量噪声，传统预训练目标（如掩码语言建模MLM）易过拟合，导致模型泛化能力下降。  \n\n2. ALBEF框架的核心解决方案针对上述问题，论文提出 ALign BEfore Fuse（ALBEF）框架，其核心思路为：  \n\n（1）对齐先行（ALign）：在融合图像-文本特征前，通过图像-文本对比学习（ITC）对齐单模态表示。  \n使用无检测器的图像编码器（ViT）和文本编码器（BERT）独立编码，通过对比损失学习跨模态相似性函数，将图像与文本映射到共享语义空间，降低多模态编码器的建模难度。  \n引入动量编码器维护历史特征队列，通过InfoNCE损失最大化正样本互信息，并利用对比硬负样本挖掘提升样本多样性。  \n\n\n（2）动量蒸馏（Momentum Distillation, MoD）：应对噪声数据过拟合问题。  \n通过动量模型（参数指数滑动平均）生成软伪标签，作为额外监督信号，允许模型学习与噪声标注不同但合理的输出，增强鲁棒性。  \n损失函数结合原始监督信号与伪标签的KL散度，公式为： , 其中，为蒸馏权重。  \n\n\n\n3. 理论分析：互信息最大化视角从互信息（MI）最大化角度解释ALBEF的有效性：  \n\nITC和MLM的视图生成：ITC将图像和文本视为同一数据对的两种“视图”，通过模态分离最大化视图间互信息；MLM通过掩码文本生成视图，利用图像和上下文预测原词，增强跨模态依赖。  \n动量蒸馏的视图增强：动量模型生成语义相似的新视图，迫使基础模型学习对语义保持不变性的表示，进一步提升互信息下界。  \n\n4. 实验效果与创新点\n性能突破：ALBEF在多个下游任务中超越现有SOTA方法：  \n图像-文本检索：在Flickr30K和COCO上超越CLIP、ALIGN，零样本迁移能力显著。  \nVQA和NLVR2：相比SOTA方法VILLA，绝对性能提升分别达2.37%和3.84%，且推理速度快10倍以上。  \n弱监督视觉定位：在RefCOCO+上通过Grad-CAM可视化验证，模型可准确定位物体、属性和关系。  \n\n\n创新优势：  \n无检测器设计：摆脱对物体检测器的依赖，支持低分辨率输入（预训练256×256，微调384×384），降低计算成本。  \n噪声鲁棒性：动量蒸馏有效利用大规模噪声Web数据，提升模型泛化能力。  \n统一单模态与多模态能力：结合对比学习（CLIP类方法）和跨模态推理（Transformer类方法），兼顾检索与复杂推理任务。  \n\n\n\n5. 结论与资源ALBEF通过“对齐-融合”框架、动量蒸馏和互信息理论，实现了视觉-语言表示学习的高效性与鲁棒性，为大规模噪声数据的利用提供了新范式。\nrelated work2.1 视觉-语言表示学习（Vision-Language Representation Learning）现有视觉语言表示学习方法主要分为两类：  \n\n基于多模态编码器的方法  \n代表方法：LXMERT、UNITER、OSCAR等，采用Transformer架构联合建模图像区域特征与文本标记，通过掩码语言建模（MLM）、图像-文本匹配（ITM）等任务学习跨模态交互。  \n优势：在需要复杂推理的任务（如VQA、NLVR²）中表现优异。  \n局限性：依赖预训练物体检测器提取图像区域特征，需高分辨率图像（如600×1000），计算成本高；部分方法（如ViLT）虽移除检测器但性能下降。  \n\n\n基于单模态编码器的对比学习方法  \n代表方法：CLIP、ALIGN，通过对比损失在大规模噪声数据中学习图像和文本的独立嵌入空间，擅长图像-文本检索任务。  \n优势：无需物体检测器，可处理大规模Web数据，零样本迁移能力强。  \n局限性：缺乏对图像-文本复杂交互的建模能力，难以应对需要细粒度推理的任务。  \n\n\n\nALBEF的定位：  \n\n融合两类方法的优势，通过单模态编码器对比对齐（ITC损失）和多模态编码器交互建模（MLM、ITM损失），实现检索与推理任务的平衡。  \n无需物体检测器，采用ViT作为图像编码器，降低计算成本，同时支持低分辨率输入（预训练256×256，微调384×384）。  \n\n2.2 知识蒸馏（Knowledge Distillation）\n传统知识蒸馏：从预训练的“教师模型”向“学生模型”迁移知识，通常通过匹配输出概率（如KL散度）实现，适用于模型压缩或跨任务迁移。  \n在线蒸馏：同时训练多个模型，利用模型集合作为教师，如深度互学习。  \n动量蒸馏（MoD）的创新：  \n属于在线自蒸馏，通过维护模型参数的指数滑动平均（动量模型）生成伪标签，作为额外监督信号。  \n与半监督学习（如Mean Teacher）和对比学习（如MoCo）相关，但首次将动量蒸馏应用于视觉-语言预训练，缓解噪声数据过拟合问题。  \n理论与实验表明，动量蒸馏可提升模型在噪声数据中的鲁棒性，且适用于多种下游任务（包括干净标注数据）。 \n\n\n\nALBEF Pre-training3.1 模型架构ALBEF的架构由三个核心模块组成，如图所示：\n\n图像编码器：\n结构：采用12层视觉Transformer（ViT-B/16），输入图像尺寸为256×256（预训练）或384×384（微调），通过16×16的Patch划分，最终输出序列嵌入，其中是用于全局表示的[CLS]标记，是Patch嵌入。  \n初始化：权重来自ImageNet-1k预训练的ViT模型，提升图像语义理解能力\n\n\n文本编码器：\n结构：6层Transformer，基于BERT-base的前6层初始化，输入文本通过Tokenization生成标记序列，输出嵌入，其中  为文本全局表示。\n\n\n多模态编码器：\n结构：6层Transformer，基于BERT-base的后6层初始化，接收图像编码器的输出和文本编码器的输出，通过 跨注意力机制（Cross-Attention）逐层融合图像与文本特征，实现跨模态交互。\n核心操作：在每一层，图像特征作为键（Key）和值（Value），文本特征作为查询（Query），通过跨注意力学习文本对图像的细粒度对齐（如定位图像中的物体对应文本描述）。\n\n\n\n\n\n\n\n\n\n\n为什么文本编码器使用 BERT-base 的前六层初始化，而多模态编码器使用 BERT-base 后六层来初始化？\nALBEF 的文本编码器和多模态编码器的分层初始化策略，旨在分离单模态理解与跨模态交互的能力，具体原因如下：\n\nBERT 的层功能分工：\nBERT 的前几层更擅长捕捉单模态文本的局部语义和语法结构（如词级关联、短语结构），适合作为文本编码器的初始化，专注于文本单模态表示的学习。\nBERT 的后几层更擅长建模全局语义依赖和跨标记交互（如句子级语义整合），与多模态编码器需要处理图像 - 文本跨模态交互的需求更匹配。\n\n\n单模态与多模态的功能解耦：\n文本编码器：仅处理文本输入，需强化单模态文本理解能力，因此复用 BERT 前六层的文本建模能力。\n多模态编码器：需融合图像与文本特征，其核心是跨注意力机制（Cross-Attention），而 BERT 后六层的自注意力机制（Self-Attention）已具备建模复杂交互的能力，微调后可适配跨模态场景。\n\n\n参数效率与迁移学习：\n利用 BERT 预训练的权重初始化，避免从头训练带来的不稳定，同时通过分模块初始化（前六层 vs. 后六层）实现单模态到多模态的渐进式能力扩展。\n实验表明，这种分层初始化策略在 VQA、NLVR² 等需要跨模态推理的任务中，比随机初始化或全层复用性能更优。\n\n\n\n\n\n3.2 预训练目标（Pre-training Objectives）ALBEF采用三个预训练目标，联合优化单模态对齐与多模态交互：  \n3.2.1 图像-文本对比学习（Image-Text Contrastive Learning, ITC）\n目标：在融合前对齐图像和文本的单模态表示，学习跨模态相似性函数，其中  是将全局嵌入映射到256维空间的线性层。  \n动量编码器与队列机制：  \n维护两个队列存储动量编码器的历史特征（受MoCo启发），动量编码器参数通过指数滑动平均（EMA）更新：, 其中  为动量系数，为当前模型参数，为动量模型参数。  \n队列中存储最近  个图像-文本对的动量特征  和 。  \n\n\n相似度计算：  \n图像到文本相似度：\n文本到图像相似度：\n\n\n对比损失（InfoNCE）：\n\n\n\n其中为one-hot真实标签，为可学习的温度参数，为交叉熵损失。 \n3.2.2 掩码语言建模（Masked Language Modeling, MLM）\n目标：利用图像信息预测文本中被掩码的标记，增强跨模态语义关联。  \n操作：随机掩码文本标记（15%概率），其中80%用[MASK]替换，10%用随机词替换，10%保持不变。  \n损失函数：其中为掩码文本，为模型预测的标记概率，为真实标记的one-hot向量。  \n\n3.2.3 图像-文本匹配（Image-Text Matching, ITM）\n目标：判断图像-文本对是否匹配，引入对比硬负样本挖掘提升训练难度。  \n硬负样本挖掘：在批次内根据对比相似度分布采样硬负样本：对每个图像，选择与该图像相似度最高的非匹配文本作为硬负样本；对每个文本，选择相似度最高的非匹配图像作为硬负样本。  \n损失函数： , 其中为匹配概率，为真实标签。  \n\n总损失函数：\n3.3 动量蒸馏（Momentum Distillation, MoD）\n动机：缓解Web数据噪声导致的过拟合，利用动量模型生成软伪标签作为额外监督。  \n核心思想：当前模型输出与动量模型的伪标签对齐，通过KL散度最小化实现知识蒸馏。  \n公式推导：  \nITC的动量蒸馏损失： , 其中为动量模型计算的软伪概率，为蒸馏权重。  \nMLM的动量蒸馏损失： , 其中 为动量模型预测的掩码标记概率。  \n\n\n效果:  伪标签允许模型学习与噪声标注不同但语义合理的输出，增强鲁棒性（如图所示，伪标签覆盖了真实文本未描述的视觉概念）。\n\n\n3.4 预训练数据集（Pre-training Datasets）\n数据集组成：  \nWeb数据：Conceptual Captions (295万图像)、SBU Captions (86万图像)、Conceptual12M (1006万图像，噪声较大)。  \n领域内数据：COCO (11.3万图像)、Visual Genome (10万图像)。  \n\n\n数据规模：  \n基础数据集：400万图像，510万图像-文本对。  \n扩展数据集：1410万图像（含Conceptual12M），用于验证模型对大规模噪声数据的适应性。  3.5 实现细节（Implementation Details）\n\n\n训练配置：  \n8块NVIDIA A100 GPU，批次大小512，训练30 epoch。  \n优化器：AdamW，权重衰减0.02，学习率余弦衰减（初始，热身1000步）。  \n\n\n图像预处理：随机裁剪256×256，应用RandAugment（不含颜色变换，避免与文本颜色信息冲突）。  \n微调细节：图像分辨率提升至384×384，通过插值适配ViT的位置编码。  \n\nA mutual Information Maximization Perspective4.1 互信息（MI）与视觉-语言表示学习核心思想：互信息衡量两个随机变量的依赖程度，最大化互信息可使模型学习到更具语义关联的跨模态表示。在视觉-语言任务中，图像-文本对的不同“视图”（View）可视为随机变量，通过最大化视图间的互信息，模型能捕捉更鲁棒的语义不变性。数学定义：设随机变量  和  为图像-文本对的两种视图，互信息  表示为：  最大化  可通过最小化InfoNCE损失实现，该损失是互信息的一个下界：  其中  为视图  和  的相似度得分， 包含正样本  和负样本集合。\n4.2 ITC损失的互信息解释ITC的视图定义：将图像  和文本  视为同一对的两个独立视图，通过对比学习最大化它们的互信息。公式推导：ITC损失可重写为对称的InfoNCE损失：  \n\n正样本：真实匹配的图像-文本对 。  \n负样本：队列中存储的  个不匹配样本  和 。  \n物理意义：通过对比学习，迫使匹配对的视图在特征空间中接近，非匹配对远离，从而最大化图像与文本视图的互信息。4.3 MLM损失的互信息解释MLM的视图定义：将“掩码词”与“图像+掩码文本”视为两种视图，通过预测掩码词最大化其依赖关系。公式推导：MLM损失可重构为：  \n查询向量：掩码词的真实标记  通过嵌入层映射为 。  \n键向量：多模态编码器输出的掩码上下文表示 ，包含图像和未掩码文本的信息。  \n物理意义：将掩码词预测视为从上下文视图中检索正确标记，通过最大化掩码词与上下文的互信息，增强跨模态语义关联。4.4 动量蒸馏（MoD）的互信息增强核心机制：动量模型生成的伪目标相当于引入新的“语义相似视图”，迫使基础模型学习对视图变化不变的表示。公式分析：以ITC的动量蒸馏损失为例：   , 其中伪目标  由动量模型的相似度  计算得到：  \n新视图生成：动量模型通过历史参数平均生成更平滑的特征表示， 对应不同于当前模型的视图相似度，引入语义等价但细节不同的负样本。\n互信息提升：最小化  迫使当前模型匹配动量模型的视图分布，相当于最大化当前模型与动量模型生成的新视图之间的互信息，增强表示的不变性。4.5 统一理论视角：视图生成与不变性学习\n\n\n\n\n\n任务\n视图生成方式\n互信息优化目标\n\n\n\n\nITC\n模态分离（图像vs.文本）\n最大化单模态视图间的互信息\n\n\nMLM\n词掩码（完整文本vs.掩码文本+图像）\n最大化掩码词与上下文视图的互信息\n\n\nMoD\n动量模型生成语义相似视图\n最大化当前模型与动量视图的互信息\n\n\n\n\n下游视觉-语言任务与实验结果ALBEF预训练模型被适配到五类下游任务，展示了其在检索、推理、生成和定位任务中的泛化能力。以下是各任务的详细分析：\n5.1 图像-文本检索（Image-Text Retrieval）\n任务目标：实现图像与文本的相互检索，包括图像到文本检索（TR）和文本到图像检索（IR）。  \n数据集：  \nFlickr30K：29k训练图像，1k验证/测试图像，每图像对应5句文本。  \nCOCO：113k训练图像，5k验证/测试图像，每图像对应5句文本。  \n\n\n模型调整：  \n微调时联合优化 ITC损失（单模态对齐）和 ITM损失（多模态匹配）。  \n推理时先用ITC计算特征相似度筛选Top-k候选，再用ITM进行细粒度排序，大幅提升推理速度（仅计算少量候选的ITM分数）。  \n\n\n关键结果：  \n零样本迁移：在Flickr30K上，仅用COCO训练的模型实现TR/IR平均召回率94.1%/82.8%，超越CLIP和ALIGN。  \n微调性能：14M预训练图像下，Flickr30K的TR/IR平均召回率达98.70%/94.07%，COCO上达97.2%/90.5%，显著优于UNITER、OSCAR等方法。  \n\n\n\n\n5.2 视觉蕴含（Visual Entailment, SNLI-VE）任务\n任务目标：判断图像与文本的关系（蕴含、中立、矛盾），属于三分类问题。\n数据集：SNLI-VE，基于SNLI文本和Flickr30K图像构建，含29.8k训练样本。\n模型调整：使用多模态编码器的[CLS]标记输出，通过多层感知机（MLP）分类。\n关键结果：\n14M预训练图像下，测试集准确率达80.91%，相比VILLA提升1.88%，验证细粒度视觉推理能力。\n\n\n\n\n5.3 视觉问答（Visual Question Answering, VQA）\n任务目标：给定图像和问题，生成自然语言答案。\n数据集：VQA2.0，含83k训练图像，41k验证图像，81k测试图像，问题涉及物体、属性、关系等。\n模型调整：\n附加6层Transformer解码器，通过自回归生成答案，解码器权重由多模态编码器初始化。\n限制答案生成范围为3,192个候选词，确保与现有方法可比。\n\n\n关键结果：\n14M预训练图像下，test-std准确率76.04%，相比VILLA提升2.37%，推理速度快10倍以上。\nGrad-CAM可视化显示，模型能准确聚焦问题相关区域。\n\n\n\n\n5.4 自然语言视觉推理（NLVR²）\n任务目标：判断文本是否同时描述两张图像，属于二分类问题。  \n数据集：NLVR²，含50k训练图像对，8.5k验证/测试图像对。  \n模型调整：  \n多模态编码器每层复制两个Transformer块，分别处理两张图像的嵌入，共享跨注意力参数。  \n预训练阶段新增文本分配（TA）任务：将文本匹配到两张图像之一或都不匹配，提升图像对推理能力。  \n\n\n关键结果：  \n14M预训练图像下，test-P准确率83.14%，相比VILLA提升3.84%，验证多图像推理的有效性。  5.5 弱监督视觉定位（Weakly-Supervised Visual Grounding）\n\n\n任务目标：在无边界框标注的情况下，定位图像中与文本描述对应的区域。  \n数据集：RefCOCO+，含19,992张COCO图像，141,564条指代表达（如“穿绿衬衫的女孩”）。  \n模型调整：\n仅使用图像-文本对监督，通过Grad-CAM生成注意力热图，定位文本提及的区域。  \nITC vs. ITM对比：  \nITC：基于单模态相似度，定位物体整体（如“大象”）。  \nITM：基于多模态交互，定位细粒度属性和关系（如“卷鼻子的大象”）。  \n\n\n\n\n\n\n\n关键结果：\nALBEF-ITM在RefCOCO+的TestB子集准确率46.25%，远超ARN、CCL等基线方法（32.13%/33.56%）。\n可视化显示，模型能区分“大行李箱”与“小背包”等细微差异，证明跨模态交互的重要性。\n\n\n\n\n","slug":"ALBEF论文精读","date":"2025-07-03T03:20:45.000Z","categories_index":"","tags_index":"多模态,VLP","author_index":"犬夜叉"},{"id":"8c60d49e88f3e150291142683c6b267b","title":"ViLT 论文精读","content":"摘要\n\n\n\n\n\n\n\n\n视觉语言预训练已提升了各类视觉 - 语言联合下游任务的性能。当前 VLP 方法高度依赖图像特征提取流程，其中大部分涉及区域监督（如目标检测）和卷积架构（如 ResNet）。尽管现有文献未予重视，但我们发现这一模式存在两方面问题：（1）效率 / 速度层面，仅输入特征提取所需的计算量就远超多模态交互步骤；（2）表达能力层面，其上限受限于视觉嵌入器的表达能力及其预定义的视觉词汇表。本文提出一种极简 VLP 模型 —— 视觉语言 Transformer（ViLT），其核心在于将视觉输入的处理大幅简化为与文本输入相同的无卷积模式。实验表明，ViLT 的速度可达以往 VLP 模型的数十倍，同时在下游任务中具备相当或更优的性能。我们的代码和预训练权重可从https://github.com/dandelin/vilt获取。\n论文介绍1. 视觉语言预训练（VLP）的现状与挑战\n主流方法的依赖与问题： 现有 VLP 模型高度依赖基于卷积神经网络（CNN）的图像特征提取（如 ResNet）和区域监督（如目标检测），导致两大核心问题：\n效率瓶颈：特征提取的计算量远超多模态交互步骤（如 UNITER 模型中视觉处理耗时占比超 90%）。\n表达能力受限：依赖预定义的视觉词汇表（如 Visual Genome 的 1600 个物体类别），难以泛化未知物体或场景。\n\n\n学术研究与实际应用的脱节： 学术实验中常通过预缓存区域特征减轻训练时的计算负担，但在实际应用中，实时输入仍需经历耗时的特征提取流程，限制了模型的部署效率。\n\n2. ViLT 的核心创新：极简架构与统一处理\n无卷积的视觉特征嵌入： 受 ViT（Vision Transformer）启发，ViLT 将图像分割为固定大小的补丁（如 32×32 像素），通过线性投影层直接生成视觉嵌入，完全摒弃 CNN 和目标检测模块。这一设计使视觉处理耗时仅为～0.4 ms，参数仅 2.4M，远低于 ResNet 等传统 backbone。\n统一的 Transformer 架构： 视觉和文本输入均通过 Transformer 进行多模态交互，首次实现 模态特定组件计算量（VE+TE）&lt;多模态交互计算量（MI） 的架构（如图 1 中 ViLT 的 MI 耗时～15 ms，远超 VE 的 0.4 ms）。这种 轻嵌入、重交互 的设计显著提升计算效率，同时保持下游任务性能。\n\n3. 性能与效率对比\n速度优势： ViLT 的总推理时间～15 ms，比基于区域特征的模型（如 UNITER，~900 ms）快 60 倍以上，比基于网格特征的 Pixel-BERT（~60 ms）快 4 倍。\n任务表现： 在 NLVR2（视觉推理）、F30K 检索等任务中，ViLT 的准确率与传统模型相当或更优（如 NLVR2 测试集准确率 74.57%，接近 UNITER 的 75.8%），证明无需复杂视觉 backbone 仍可实现有效跨模态建模。\n\n4. 关键贡献与意义\n架构革新： 首次证明 VLP 模型可完全脱离卷积和区域监督，为轻量化多模态模型设计提供新范式。\n方法创新： 引入全词掩码（Whole Word Masking）和图像增强（RandAugment），提升预训练效率和下游任务泛化能力。\n研究启示： 呼吁 VLP 领域从 “单模态特征增强” 转向 “多模态交互优化”，为后续模型（如更大规模的 ViLT-L/H）奠定基础。\n\n\n从论文中的图片可以看出，对于传统模型，一般的处理流程包括：\n\n图像输入后，先通过卷积神经网络提取网格特征，再经目标检测模块生成区域建议（RoI），并通过非极大值抑制（NMS）和 RoI 头处理，最终得到区域特征。\n直接使用 CNN backbone（如 ResNet）输出的网格特征，跳过目标检测步骤，通过线性嵌入输入 Transformer。在这片论文中，图像直接分割为固定大小的补丁（如 32×32 像素），通过线性投影层（Linear Embedding）将每个补丁转换为特征向量，无需 CNN 或目标检测模块。\n\n\n效率优势：ViLT 的总运行时间（~15 ms）仅为 UNITER 的 1.7%，Pixel-BERT 的 25%。\n性能保持：在 NLVR2、F30K 检索等任务中，ViLT 性能接近或超过传统模型，证明去除 CNN 和区域监督不会显著损失表达能力。\n\n研究背景1. 视觉-语言模型的分类体系\n2. 模态交互方式\n单流 vs. 双流：\n单流模型（如 VisualBERT）将图像和文本拼接后输入 Transformer，参数效率更高；\n双流模型（如 ViLBERT）分模态处理，引入额外参数，ViLT 采用单流架构。\n\n\n\n3. 视觉嵌入的瓶颈\n区域特征： 通过 Faster R-CNN 等目标检测器生成，需经历 RPN、NMS 等复杂流程，计算量大（如 UNITER 视觉处理耗时 810 ms），且依赖预定义类别（如 Visual Genome 的 1600 个物体类），泛化能力受限。\n网格特征： 直接使用 CNN 输出（如 Pixel-BERT），虽省去目标检测，但 CNN backbone（如 ResNet）仍为性能瓶颈（Pixel-BERT-R50 视觉处理耗时 45 ms）。\nViLT 的创新： 采用Patch投影，将图像分割为 32×32 像素块，通过线性投影生成嵌入，仅需 0.4 ms 和 2.4M 参数，彻底摒弃 CNN 和目标检测。\n\n4. 关键问题与突破\n传统模型的效率与表达局限： 视觉嵌入的高计算量（如区域特征提取）和预定义视觉词汇限制了模型速度和泛化能力，学术实验中预缓存特征的做法无法解决实际应用中的实时性需求。\nViLT 的设计思路： 受 ViT 启发，利用 Transformer 的自注意力机制直接处理图像补丁和文本 tokens，将计算重点从 “单模态特征提取” 转向 “跨模态深度交互”，实现轻量化与高性能的平衡。\n\nVision-and-Language Transformer一、模型整体架构与核心设计1. 统一的单流Transformer架构ViLT采用单流 Transformer 处理视觉和文本输入，即图像和文本嵌入被拼接为单一序列，通过多层Transformer层进行联合建模。这一设计避免了双流架构的额外参数开销，同时确保深度跨模态交互。\n2. 视觉嵌入：无卷积的Patch投影\n输入处理：图像被分割为固定大小的补丁（如32×32像素），每个补丁被展平为一维向量  为补丁尺寸， 为图像通道数， 为补丁数量）。\n线性投影：通过可学习的线性层  将补丁向量映射到隐藏空间 （H=768 为隐藏层维度），并添加位置嵌入 。\n关键优势：\n仅需 2.4M参数，远小于ResNet-50（25M）等卷积 backbone。\n计算耗时仅 0.4 ms，彻底消除CNN和目标检测的耗时瓶颈。\n\n\n\n3. 文本嵌入：基于BERT的轻量设计\n输入处理：文本通过BERT分词器生成 tokens，包含特殊标记（如[CLS]）和位置嵌入 。\n模态类型嵌入：为区分视觉和文本模态，分别添加模态类型向量  和，避免模态混淆。\n\n4. 多模态交互：Transformer编码器\n输入拼接：视觉嵌入  和文本嵌入  拼接为 。\nTransformer层：通过12层Transformer编码器（每层包含多头自注意力MSA和MLP）进行特征交互，公式如下：， 其中，层归一化采用ViT的“前归一化”策略（先归一化再计算注意力/MLP），与BERT的“后归一化”不同。\n池化输出：最终序列的首个标记  通过线性投影和激活函数生成池化表示 ，用于下游任务分类或检索。\n\n二、与训练目标与技术创新\n\n\n\n\n\n\n实验数据集介绍\n1、 MSCOCO（Microsoft Common Objects in Context）\n\n规模：113K 张图像，567K 条字幕（平均每张图 5 条字幕），字幕长度为 11.81±2.81。\n定位：微软开发的 多任务基准数据集，支持目标检测、场景分割、图像字幕、视觉问答（VQA）等任务。\n\n\n核心特点：\n字幕标注 聚焦场景核心内容（如 “一只狗在公园奔跑”），避免冗余细节，适合训练模型抓重点；\n图像覆盖真实生活场景（如家庭、户外），物体类别丰富（80 类常见物体），是视觉语言任务的 “试金石”。\n\n\n\n2、VG（Visual Genome）\n\n规模：108K 张图像，5.41M 条字幕（平均每张图 50 条 + 字幕），字幕长度为 5.53±1.76。\n定位：斯坦福李飞飞团队推出的 细粒度语义理解数据集，主打 场景图（Scene Graph） 标注（结构化描述物体、属性、关系，如 “人→骑→自行车”）。\n核心特点：\n\n标注极丰富：除字幕，还有 “区域描述”（局部区域的文本解释）、“视觉问答”（针对图像的问答对）；\n分布呈长尾性：常见关系（如 “on” “in”）占比极高，稀有关系极少，后续衍生出 “VG150” 等清洗版本（筛选高频类别）。\n\n3、 GCC（Google Conceptual Captions）\n\n规模：3.01M 张图像，3.01M 条字幕（1:1 配对），字幕长度为 10.66±4.93。\n定位：大规模弱监督数据集，图像和字幕从网络爬取，属于 “弱关联” 标注（无需严格语义对应）。\n核心特点：\n数据极多样：图像风格覆盖更广（如艺术画、表情包），突破 MSCOCO 的场景限制，提升模型泛化能力；\n预训练核心：ViLT 等模型用其预训练，通过海量数据学习 “松散跨模态关联”，弥补强监督数据集的规模不足。\n\n\n\n4、 SBU（SBU Captions）\n\n规模：867K 张图像，867K 条字幕（1:1 配对），字幕长度为 15.0±7.74。\n定位：早期 图像 - 字幕检索数据集，最初用于 “给定图像找匹配字幕” 或反之的检索任务。\n核心特点：\n字幕更长且自由（均值 15 词，长于其他数据集），适合训练模型理解复杂文本描述；\n标注简洁：1 张图对应 1 条字幕，训练成本低，常与其他数据集联合预训练（如 ViLT）。\n\n\n\n\n\n\n\n1. 图像文本匹配\n任务定义: 判断图像-文本对是否匹配，正样本为真实配对，负样本为随机替换图像的配对。\n损失函数: 二元交叉熵损失, 通过线性层将池化表示  映射为匹配概率\n创新: 词-补丁对齐 (Word-Patch Alignment, WPA) 受最有传输理论启发, 引入WPA 目标增强跨模态对齐:\n计算文本子集和视觉子集之间的Wasserstein距离, 使用近似最近点方法优化\n将距离损失按系数0.1加权后加入ITM损失, 提升细粒度语义对齐(如下图的可视化热力图)\n\n\n\n\n\n\n\n\n\n\n词-补丁对齐详解: 基于最优传输的跨模态语义对齐\n\n核心问题：跨模态细粒度对齐的挑战在视觉语言任务中，文本中的单词（如“花朵”）需与图像中的对应区域（如花朵所在的补丁）建立精准关联。传统方法（如点积、注意力）通过特征相似度建模交互，但缺乏对语义结构的显式对齐。例如，文本中的“giraffe”需对应图像中长颈鹿的整体区域，而非零散像素。WPA的目标：通过最优传输理论，将文本单词与图像补丁视为两个分布，计算它们之间的结构化对齐成本，强制模型学习语义级别的跨模态对应关系。\n理论基础：最优传输与Wasserstein距离最优传输（Optimal Transport, OT）：起源于运输问题，旨在找到两个概率分布之间的最优映射路径，使总运输成本最小。例如，将一堆沙子（源分布）移动到另一堆沙子（目标分布）的最小工作量。Wasserstein距离（推土机距离）：衡量两个分布之间的最小运输成本，公式为： ， 其中， 和  分别为文本和视觉特征分布，为两者的联合分布， 为单个样本的运输成本（如特征向量距离）。关键价值：Wasserstein距离不仅衡量整体分布差异，还能捕捉局部结构对应（如单词与补丁的一一映射），适合细粒度语义对齐。\nWPA的技术实现：基于IPOT的近似优化论文采用 近似最近点方法（IPOT） 高效计算Wasserstein距离，避免传统OT的高复杂度。具体步骤如下：步骤1：特征子集提取文本子集 ：从Transformer最后一层输出中提取文本 tokens 的特征（不包含特殊标记如[CLS]）。视觉子集 ：提取图像补丁的特征（不包含补丁分类标记[VCLS]）。形状：假设文本有  个单词，视觉有  个补丁，则特征矩阵分别为 和 为隐藏层维度）。步骤2：运输成本矩阵构建计算每个单词与补丁之间的成对距离，形成成本矩阵 ：  （归一化欧氏距离），分母  为缩放因子，确保数值稳定性。步骤3：IPOT迭代优化通过IPOT算法求解最优传输计划 ，其中  表示单词  与补丁  的对齐强度（概率值）。初始化：均匀分布 。迭代更新：, 其中， 为温度参数，控制对齐的平滑度；迭代次数 （与训练阶段一致）。约束：确保每行（单词）和每列（补丁）的概率和为1，即 ， 。步骤4：Wasserstein距离计算近似Wasserstein距离为：  , 即运输计划与成本矩阵的内积，反映整体对齐成本。步骤5：损失加权与反向传播将Wasserstein距离按系数  加权后加入图像文本匹配（ITM）损失：  ， 该损失引导模型调整Transformer参数，使语义相关的单词与补丁之间的运输成本降低（即对齐强度增加）。\n可视化与效果：语义对齐的直观呈现论文通过上面的热图可视化WPA的对齐结果。例如，文本“flowers”对应图像中花朵区域的补丁，“wall”对应墙壁区域的补丁，透明度越高表示对齐强度）越大。关键发现：  \n\n\n传统模型（如UNITER）依赖目标检测框标注，只能对齐预定义类别；  \nViLT通过WPA实现无显式标注的语义对齐，可泛化至任意物体。消融实验：移除WPA后，NLVR2准确率下降约0.8%，证明其对跨模态推理的重要性。\n\n\n为什么WPA有效？结构化对齐：区别于简单特征融合（如CLIP的点积），WPA显式建模单词与补丁的一对多关系，捕捉复杂语义关联（如“giraffe”对应多个身体部位补丁）。抗局部最优：IPOT的迭代优化避免陷入浅层交互的局部最优解，迫使模型学习全局一致的跨模态映射。轻量化开销：WPA仅在预训练阶段引入约0.4 ms计算量，远低于目标检测的数百毫秒耗时，适合端到端优化。\n\n\n\n2. 掩码语言建模（Masked Language Modeling, MLM）\n任务定义：随机掩码文本 tokens（15%概率），预测原始标记。\n论文创新：全词掩码（whole world masking）\n掩码整个单词的所有子词单元（如“giraffe”掩码为[gi, [MASK], ##fe]），迫使模型依赖图像信息而非局部文本上下文。\n实验表明，全词掩码显著提升NLVR2等推理任务性能（+0.96% test-P准确率）。\n\n\n\n\n3. 图像增强（RandAugment）\n应用场景：仅在微调阶段使用，避免预训练时缓存特征的限制。\n策略选择：采用RandAugment中的非颜色反转和非裁剪策略（保留颜色信息和小物体），超参数设为 。\n效果：结合全词掩码和更长训练步数（200K步），ViLT在VQAv2测试集准确率提升至71.26%。\n\n\n\n\n\n\n\n\n表格解释\n\n视觉嵌入方式：\nRegion：基于目标检测的区域特征。\nGrid：基于 CNN 网格特征。\nLinear：ViLT 的线性补丁投影。\n\n\n任务：\nVQAv2：视觉问答，准确率越高越好。\nNLVR2：视觉推理（二元分类），dev/test-P 准确率越高越好。\n\n\n关键标注：\n†：预训练额外使用 GQA、VQAv2、VG-QA 数据集。\n‡：额外使用 Open Images 数据集。\na：微调时应用 RandAugment 数据增强。\n+：预训练步数延长至 200K 步（默认 100K 步）。\n\n\n\n\n\n三、关键技术对比与性能优势1. 与传统VLP模型的对比\n\n\n\n维度\n传统模型（如UNITER）\nViLT\n\n\n\n\n视觉嵌入\nCNN+目标检测（如ResNet+Faster R-CNN）\n线性补丁投影（无卷积/检测）\n\n\n计算量\n视觉处理占90%以上耗时（~810 ms）\n视觉处理仅0.4 ms，总耗时~15 ms\n\n\n参数规模\n150M+（含CNN权重）\n87.4M（仅Transformer+线性层）\n\n\n下游任务性能\nVQAv2 test-dev 72.7%\nVQAv2 test-dev 71.26%（+RandAugment）\n\n\n推理效率\n需预缓存特征（训练时高效，推理时慢）\n端到端高效（无需预计算）\n\n\n\n\n2. 效率与性能的平衡\n速度优势：ViLT比UNITER快60倍以上，比Pixel-BERT（网格特征）快4倍。\n泛化能力：无需预定义视觉词汇（如Visual Genome的1600类），通过自注意力学习任意图像补丁与文本的关联，适用于未知物体场景。\n\n","slug":"ViLT论文精读","date":"2025-07-02T09:37:29.000Z","categories_index":"","tags_index":"多模态,VLP","author_index":"犬夜叉"},{"id":"64c2453c0ac7d584fda9903cba47649e","title":"CLIP 论文精读","content":"CLIP论文精读CLIP通过对比学习从大量的图像-文本中学习视觉概念，实现了强大的零样本图像分类能力\n\n\n\n\n\n\n\n\n\n论文地址：Learning Transferable Visual Models From Natural Language Supervision\n\n\n\n\n\n\n论文创新点\n\n它不使用传统计算机视觉任务中常见的、带有固定类别（如“猫”、“狗”）的标签，而是直接从互联网上收集的（图像，文本）对中学习。这种文本描述提供了比单一标签丰富得多的监督信号，涵盖了几乎无限的视觉概念。\n\n\n为了让模型理解图像与文本的关联，CLIP采用了一种名为“对比学习”的自监督方法。其核心思想是：\n\n构建双编码器架构：CLIP包含一个图像编码器（Image Encoder，如ResNet或Vision Transformer）和一个文本编码器（Text Encoder，如Transformer）。\n学习多模态嵌入空间：在训练过程中，模型会接收一批图像和文本。图像编码器将图像转换为特征向量，文本编码器将文本转换为特征向量。CLIP的目标是在这个共享的多模态嵌入空间中，拉近真实的“图像-文本”对的特征向量（正样本），同时推远不匹配的“图像-文本”对的特征向量（负样本）。\n高效的代理任务：通过判断哪个文本与哪个图像配对，这个看似简单的“代理任务”却极其高效地迫使模型学习图像内容和文本语义之间的深刻联系。\n\n\nCLIP最令人瞩目的成果是其强大的零样本学习能力。传统的模型在面对一个新的分类任务时，通常需要进行微调，即在新任务的标注数据上进行再训练，而经过预训练的CLIP无需任何微调即可直接应用于新的视觉分类任务。其实现方式为：\n\n动态构建分类器：对于一个给定的分类任务（例如，区分“猫”和“狗”的图片），CLIP会将类别名称（”cat”, “dog”）转换成标准的提示语，如 “a photo of a cat” 和 “a photo of a dog”。\n相似度匹配预测：将这些提示语通过文本编码器生成文本特征向量。当输入一张待分类的图像时，图像编码器会生成其图像特征向量。最后，模型会计算该图像特征向量与所有类别提示语的文本特征向量之间的余弦相似度，相似度最高者即为预测的类别。\n\n\n\n\n\n1. 核心方法：对比学习CLIP的目标不是像传统模型那样预测一个固定的类别，而是学习一个多模态的嵌入空间，在这个空间里，匹配的图像和文本对的特征向量距离很近，而不匹配的则很远。具体实现如下：\n\n构建批次：在一个训练批次中，包含 N 个匹配的对。\n双塔编码：\n图像编码器 (Image Encoder)：将 N 个图像编码成 N 个图像特征向量 \n文本编码器 (Text Encoder)：将 N 个文本编码成 N 个文本特征向量 \n\n\n计算相似度：计算所有可能的 对的余弦相似度，形成一个 N×N 的相似度矩阵\n定义损失：在这个矩阵中，对角线上的 N 个元素是正样本（匹配的图文对），其余的  个元素都是负样本。CLIP的优化目标是一个对称的交叉熵损失函数，即同时在行和列的方向上进行优化：对于每个图像，模型需要从 N 个文本中找出正确的那个；反之，对于每个文本，也需要从N个图像中找出正确的那个。\n\n\n2. 核心能力：零样本迁移这是CLIP方法论的直接应用，也是其价值的主要体现\n\n动态构建分类器：对于任何一个分类任务（比如对ImageNet的1000类进行分类），CLIP不需要重新训练。而是通过“提示工程”为每个类别创建一个或多个描述性文本。例如，对于类别 “dog”，可以生成文本 “A photo of a dog.”。\n\n\n\n推理与匹配：将待分类的图像输入Image Encoder得到其特征 。然后，将所有类别的提示文本输入Text Encoder得到一组类别特征 ,… 。最后，计算  与每个类别特征  的余弦相似度，相似度最高者即为预测类别。\n提示工程的重要性：精心设计的提示语至关重要。它能解决词义模糊问题（如”boxer”是狗还是拳击手）并提升性能。论文中通过集成80个不同的提示模板，在ImageNet上的准确率提升了3.5%。这一技巧的有效性，使得CLIP的性能得到了显著增强。\n\n\n3. 关键决策：追求最高的训练效率在论文中，作者强调，由于计算资源是有限的，选择一个计算效率最高的预训练方法至关重要。他们对比了三种方法：\n\n方法一：Transformer语言模型 (预测文本)：类似VirTex，用图像作为上下文，生成描述文本。这种方法表现力强，但任务难度大，学习效率最低。\n方法二：词袋模型：不要求生成完整句子，只要求预测文本中的单词。效率比方法一高3倍，但仍不够理想。\n方法三：对比学习：只要求判断图文是否匹配，任务最简单。其训练效率比词袋模型还要高4倍。\n\n\n这个实验结论是CLIP成功的关键之一：在超大规模数据下，一个更简单、更高计算效率的训练目标，能让模型在有限时间内学到更好的表征。\n4. 模型与数据规模\n数据集：OpenAI构建了一个名为WebImageText的私有数据集，包含从互联网上收集的4亿个图文对。\n图像编码器：论文中测试了两种架构：\nResNet-D：对标准ResNet进行了一些修改，如用注意力池化层替换全局平均池化层。共测试了5个不同规模的ResNet。\nVision Transformer (ViT)：共测试了3个不同规模的ViT。实验发现ViT的计算效率比ResNet更高。最终性能最好的模型是 ViT-L/14，并在336x336的分辨率下进行了额外的微调。\n\n\n文本编码器：一个标准的63M参数、12层、512宽、8个注意力头的Transformer模型。\n\n5. 实验结果与分析CLIP的实验部分非常详尽，覆盖了超过30个不同的数据集，主要结论如下：\n\n与全监督模型匹敌：在ImageNet上，Zero-shot CLIP的准确率可以达到76.2%，与一个在ImageNet上经过完整监督训练的ResNet-50效果相当。\n超强的鲁棒性：CLIP最令人印象深刻的是其在自然分布漂移上的表现。在ImageNet-V2, Rendition, Sketch, Adversarial等更具挑战性的数据集上，其性能远超监督模型。最极端的例子是在ImageNet-A上，ResNet101的准确率从76.2%骤降至2.7%，而CLIP仍能达到77.1%，展现了惊人的泛化能力。\n\n\n\n数据重叠检查：为了验证性能不是来自于训练集和测试集的重叠（数据泄露），作者进行了详尽的检查，发现重合率中位数仅为2.2%，且移除这些重叠样本后，模型性能没有显著变化，证明了其强大的泛化能力是真实有效的。\n优秀的特征表示：即使不用于零样本分类，CLIP学习到的特征本身也极为优秀。在标准的线性探查（linear probe）评测中，CLIP的特征在性能和计算效率方面均优于当时的其他自监督方法。\n\n\n上面图片中，左图估算了线性分类器达到零样本 CLIP 性能所需的每类标注样本数，范围从不足 1 到 184，中位数 5.4，均值 20.8。这表明zero - shot transfer 数据效率差异大，部分任务需大量标注，部分几乎无需。图 8 显示zero - shot 与线性探针性能正相关，但zero - shot 普遍低 10%-25%，仅 5 个数据集接近。这说明 CLIP zero - shot 能力与表征质量相关，但仍有提升空间，多数任务距最优有差距。\n\n上图对比了 CLIP 模型与 EfficientNet、MoCo、ViT 等先进计算机视觉模型的线性探针性能。左图为 Kornblith 等研究的 12 个数据集平均分数，右图为 27 个更多样分布数据集的平均分数。结果显示，CLIP 模型，尤其是 ViT 架构的 CLIP-L/14@336px，在两类数据集上均表现出色，其最佳模型平均得分超过现有模型，且 Vision Transformer 比 ResNet 更高效。虚线表示微调或高分辨率评估的模型，体现了 CLIP 在表征学习上的优势与高效性。\n\n","slug":"CLIP论文精读","date":"2025-07-02T08:41:41.000Z","categories_index":"","tags_index":"多模态,对比学习","author_index":"犬夜叉"},{"id":"290cb712ccbf87b18cb10d019f08a741","title":"ViT 论文精读","content":"\n\n\n\n\n\n摘要\n构建仅使用少量带注释示例即可快速适应新任务的模型，是多模态机器学习研究的一个开放性挑战。我们引入了Flamingo，这是一系列具备此能力的视觉语言模型（VLM）。我们提出了关键的架构创新（i）连接强大的仅视觉和仅语言预训练模型；（ii）处理任意交错的视觉和文本数据序列；（iii）无缝接收图像或视频作为输入。得益于其灵活性，Flamingo模型可在包含任意交错文本和图像的大规模多模态网络语料库上进行训练，这是赋予它们上下文少样本学习能力的关键。我们对模型进行了全面评估，探索并衡量了它们快速适应各种图像和视频任务的能力。这些任务包括开放式任务，如视觉问答（模型被提示一个问题并必须回答）、字幕任务（评估描述场景或事件的能力），以及封闭式任务，如多项选择视觉问答。对于此范围内的任何任务，单个Flamingo模型只需用特定任务的示例提示模型，即可通过少样本学习达到新的最先进水平。在众多基准测试中，Flamingo的表现优于在数千倍更多特定任务数据上微调的模型。\n\n\nVision Transformer的出现标志着 Transformer 架构成功应用于计算机视觉领域，挑战了卷积神经网络在该领域的主导地位。ViT 通过将图像分割成小块 (patches)，并将这些图像块视为序列输入到标准的 Transformer 编码器中，从而实现了对图像的有效处理。这一进展不仅在图像分类等任务上取得了最先进的成果，更重要的是，它为视觉和语言（以及其他模态）提供了一种通用的架构语言——Transformer。这种架构上的统一极大地促进了后续多模态模型（如CLIP、LLaVA等，它们通常采用ViT或其变体作为视觉编码器）的设计和发展，使得不同模态的基于token的表示可以在相似的计算框架内进行交互和融合。 \n\n从数据图中可以看出，在较小的数据集上，Vision Transformer比计算成本相当的ResNets更容易过拟合。例如，ViT-B/32比ResNet50稍快；它在9M子集上的表现要差得多，但在90M+子集上表现更好。ResNet152x2和ViT-L/16也是如此。这个结果强化了这样一种直觉：卷积的归纳偏置对于较小的数据集是有用的，但对于更大的数据集，直接从数据中学习相关模式是足够的，甚至是有益的。\nI. 摘要尽管 Transformer 架构已成为自然语言处理（NLP）任务事实上的标准，但其在计算机视觉领域的应用仍然有限。在视觉领域，注意力机制要么与卷积网络（CNN）结合使用，要么用于替代卷积网络中的某些组件，但整体结构仍然保留。我们证明了这种对 CNN 的依赖并非必要，一个直接应用于图像块序列的纯 Transformer 模型可以在图像分类任务上表现得非常好。 当在大量数据上进行预训练，并迁移到多个中等或小型图像识别基准测试（如 ImageNet, CIFAR-100, VTAB 等）时，Vision Transformer (ViT) 相比于最先进的卷积网络取得了优异的结果，同时训练所需的计算资源也大幅减少。\nII. 创新点范式革新：将图像视为序列处理论文首次证明了一个纯粹的、标准的 Transformer 模型可以直接用于图像分类，而无需依赖卷积神经网络。传统视觉任务长期由 CNN 主导，这篇论文打破了这一惯例。它通过将图像分割成固定大小的图块，并将这些图块的线性嵌入序列作为 Transformer 的输入，成功地将 NLP 领域的成功范式迁移到了视觉领域。\n数据量胜于归纳偏置论文发现，当在超大规模数据集（如 JFT-300M，包含3亿张图片）上进行预训练时，Vision Transformer (ViT) 的性能超越了当前最先进的卷积网络。这揭示了一个重要现象：CNN 中固有的（如局部性、平移不变性）的归纳偏置在数据量较小时非常有效，但当数据量足够大时，模型可以从数据中直接学习到这些空间关系，强大的模型容量和更少的先验限制反而成为优势。\n卓越的计算效率和可扩展性与性能相当的 SOTA 卷积网络相比，ViT 在达到同等甚至更高精度时，所需的预训练计算资源要少得多。例如，ViT-L/16 在 JFT-300M 数据集上预训练后，其性能优于在同一数据集上训练的 BiT-L (一个大型 ResNet 模型)，而训练成本却显著降低。这证明了 Transformer 架构在可扩展性上的巨大潜力。\n简洁而有效的模型设计论文的设计理念是尽可能少地修改原始的 Transformer 架构，使其可以直接利用 NLP 领域成熟的高效实现和可扩展架构。这种简洁性不仅体现在模型结构上，也体现在对图像的处理上，除了初始的图块划分和用于适应不同分辨率的位置编码插值外，几乎没有引入图像特有的归纳偏置。\nIII. 网络原理详解ViT模型概览ViT模型的核心思想是将图像转换为一个序列，然后用标准的Transformer Encoder来处理这个序列。\n图像分块处理 (Image Patching)Transformer接受的输入数据格式是一维的词嵌入序列，为了处理二维图像数据，论文将图像  重塑为一个扁平化的二维图块序列 ，其中：\n\n(H, W) 是原始图像的分辨率\nC 是通道数\n(P, P) 是每个图像图块的分辨率\n 是最终得到的图块数量， 也作为 Transformer 的有效输入序列长度。\n\n\nTransformer 在其所有层中使用恒定的潜在向量大小 D，因此论文将图块扁平化，并通过一个可训练的线性投影映射到 D 维。这个投影的输出称为图块嵌入 (Patch Embeddings)。具体的分块实现可以使用卷积来实现，例如对于 224x224x3 的图像，可使用卷积核大小为 16x16、步长为 16，卷积核数量为 768，将原图像输出为 14x14x768，再将前两个维度展平，得到了最终的 196x768 的张量。\n\n可学习的分类嵌入 (Class Token)原论文在嵌入图块序列的前面添加一个类似于BERT的可学习的嵌入 [CLS] Token，用于对图像进行分类。在进行物体分类任务时，如果不添加Class token，直接把 196 x 768 维的张量输入到编码器（Encode）中，编码器输出的同样是 196 x 768 维的张量，也就是196个 1 x 768 维的向量。但此时面临一个难题：难以确定该选取哪个向量作为最终的输出向量来进行物体分类。为了解决上述问题，在将数据输入编码器之前，添加一个 1 x 768 维的向量，也就是Class token。这个向量会被放置在 196 x 768 维向量的前面。这样一来，编码器输出的向量维度就变成了 197 x 768。之后，只需通过切片操作获取第一个 1 x 768 维的向量，再把它送入分类头进行分类即可。\n融合位置编码 (Positional Encoding)若不添加类似Transformer中的位置编码，那么ViT对于不同顺序的图块会得到相同的结果，这是违反直觉的。Transformer中使用的是正弦位置编码，ViT原始论文中使用的是一维可学习的位置嵌入，因为论文通过实验没有观察到使用二维感知位置嵌入会带来显著的性能提升。在得到经过操作后的 197 x 768 维的张量（由 1 x 768 维的Class token和 196 x 768 维的张量x拼接而成）后，会加上一个维度同样为 197 x 768 的位置编码向量 position Embedding。由于二者维度相同，所以可以直接进行逐元素相加操作，这样就将位置信息融入到了嵌入向量中。最终的输入数据为：\nTransformer EncoderViT的 Encoder 使用的就是 Transformer Encoder 的结构，经过L个encoder结构后，输入维度没有发生变换，仍为 197*768 维。Transformer 的 Encoder 接收输入序列后，先通过词嵌入和位置编码融合语义与位置信息，随后经过多层处理。每层先通过多头自注意力机制计算词与词之间的关联权重，动态聚合上下文信息，再经残差连接和层归一化稳定训练；接着通过前馈神经网络非线性变换特征，同样伴随残差与归一化。最终输出富含上下文信息的序列表示，核心在于自注意力的全局交互和层堆叠的渐进特征抽象。\nMLP Head (分类头)经过encoder结构后，输出的维度为 197*768，此时我们会通过切片的方式提取出Class token的信息，其维度为 1*768。接着会拿这个 1*768 维的Class token经过MLP Head层。ViT中的MLP Head结构非常简洁，它的设计目标是作为一个简单的线性分类器，将从 [CLS] Token中提取到的高度浓缩的图像特征映射到最终的分类结果上。在最常见的实现中，MLP Head仅仅由一个线性层构成。输入维度等于Transformer模型内部的隐藏维度，输出维度等于任务所需的类别总数。在某些论文或实现中（特别是在预训练阶段），这个MLP Head可能会稍微复杂一点，比如包含一个 tanh 激活函数和一个线性层，即 Linear(tanh(Input))。但在将预训练好的模型应用于下游任务时，通常会丢弃预训练的MLP Head，换上一个全新的、符合新任务类别数量的单线性层。ViT整体结构如下图所示：\n\nIV. ViT代码复现1. Patch Embeddingimport torch\nimport torch.nn as nn\nfrom functools import partial\nfrom collections import OrderedDict\n\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    将图像分割成块 (patch) 并进行线性嵌入\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n        # img_size 输入图片大小 | patch_size 图片分块大小 | in_c 输入通道数 | embed_dim 嵌入后的维度\n        super().__init__()\n        img_size = (img_size, img_size)\n        patch_size = (patch_size, patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n        # 使用二维卷积实现分块和嵌入 (B,3,224,224) -&gt; (B,768,14,14)\n        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"输入图像大小{H}*{W}与模型期望大小{self.img_size[0]}*{self.img_size[1]}不匹配\"\n\n        # (B,768,14,14) --flatten--&gt; (B,768,196) --transpose--&gt; (B,196,768)\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x\n\n2. multi-headclass Attention(nn.Module):\n    def __init__(self,\n                 dim,  # 输入的token维度,768\n                 num_heads = 8, # 注意力头数,为8\n                 qkv_bias=False, # 生成QKV的时候是否添加偏置\n                 qk_scale=None, # 用于缩放QK的系数,如果None,则使用1/sqrt(head_dim)\n                 atte_drop_ration=0., # 注意力分数的dropout比率\n                 proj_drop_ration=0.): # 最终投影层的dropout比率\n        super().__init__()\n        self.num_heads = num_heads # 注意力头数\n        head_dim = dim // num_heads  # 每个注意力头数的维度\n        self.scale = qk_scale or head_dim ** -0.5  #qk的缩放因子\n        self.qkv = nn.Linear(dim,dim*3,bisa=qkv_bias) # 通过全连接层生成QKV,为了并行计算,提高计算效率,参数更少\n        \"\"\"\"\n        这是实现多头注意力的一个巧妙且高效的方式。\n        它用一个全连接层，一次性地将输入 x (维度为 dim) 映射到一个维度为 dim * 3 的张量。\n        这个 dim * 3 的张量可以被看作是 Q, K, V 三个部分横向拼接在一起的结果，\n        后续我们只需要对这个大张量进行切分即可。\n        这样做比定义三个独立的线性层(一个给Q,一个给K,一个给V)在计算上更高效。\n        \"\"\"\n        self.atte_drop = nn.Dropout(atte_drop_ration)\n        self.proj_drop = nn.Dropout(proj_drop_ration)\n        self.proj = nn.Linear(dim,dim) # 将每个head得到的输出进行concat拼接,然后通过线性变换映射为原本的嵌入dim\n\n    def forward(self,x):\n        B,N,C = x.shape  # 批大小, 图块数+1(这个1为class_token), 通道数\n        # reshape: B,N,3*C -&gt; B,N,3,num_head,C//num_head\n        # permute: B,N,3,num_head,C//num_head  -&gt;  3,B,num_heads,N,C//self.num_heads\n        #这样一来，Q, K, V就被分开了，并且每个头的计算所需的数据（N 和 head_dim）都排列在一起，非常适合进行批处理矩阵运算\n        qkv = self.qkv(x).reshape(B,N,3,self.num_head,C//self.num_head).permute(2,0,3,1,4)\n        # 用切片拿到q,k,v. 形状都是: [B, num_heads, N, head_dim]\n        q,k,v = qkv[0],qkv[1],qkv[2]\n        # transpose: (B,num_heads,N,C//self.num_heads)  -&gt;  (B,num_heads,C//self.num_heads,N)\n        # 这是一个批处理矩阵乘法。对于 B 个样本中的每一个和 num_heads 个头中的每一个，\n        # 我们都计算一个 [N, head_dim] 的 q 矩阵和一个 [head_dim, N] 的 k 转置矩阵的乘积\n        # 结果是一个 [N, N] 的矩阵，这个矩阵的第 (i, j) 个元素表示序列中第 i 个 token 对第 j 个 token 的注意力分数\n        attn = (q @ k.transpose(-2,-1))*self.scale  #形状为[B,num_heads,N,N] \n        attn = attn.softmax(dim=-1) # 对每个头的注意力分数矩阵的 每一行 进行归一化，使其和为1\n        attn = self.atte_drop(attn) # 应用dropout\n        # 注意力权重对V进行加权求和\n        # attn @ V: B,num_heads,N,C//self.num_heads\n        # transpose(1,2): B,N,num_heads,C//self.num_heads\n        # reshape(B,N,C): 将最后两个维度信息拼接,合并多个头输出,回到总的嵌入维度\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        # 将拼接好的多头输出通过最后一个线性层 self.proj。这一步允许模型学习如何最好地融合来自不同头的信息\n        x = self.proj(x)\n        x = self.proj_drop(x) # 应用最后的 dropout，防止过拟合\n        return x\n\n3. Blockclass MLP(nn.Module):\n    def __init__(self,in_features,hidden_features=None,out_features=None,act_layer=nn.GELU,drop=0.):\n        # in_features输入的维度  hidden_features隐藏层的维度,通常为in_features的4倍  out_features输出的维度,通常与in_features相同\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features,hidden_features)   # 第一个全连接层\n        self.act = act_layer()  # 激活层,默认GELU函数\n        self.fc2 = nn.Linear(hidden_features,out_features)  # 第二个全连接层\n        self.drop = nn.Dropout(drop)   # dropout层\n\n    def forward(self,x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef drop_path(x, drop_prob:float = 0., training: bool = False):\n    \"\"\"\n    实现DropPath的核心功能。\n    以 drop_prob 的概率将输入的整个张量 x 置零。\n    这是 Stochastic Depth 网络中的主要正则化方法。\n    \"\"\"\n    # 如果丢弃概率为0，或者当前不是训练模式，则直接返回原始输入x\n    # 在评估或推理时，我们不希望随机丢弃任何路径\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1-drop_prob   # 计算需要保留的路径的概率 (keep_prob)\n    # 创建一个生成随即掩码的形状元组，shape会是 (batch_size, 1, 1, ...)，1的数量取决于x的维度\n    shape = (x.shape[0],) + (1,)*(x.ndim - 1)\n    # 生成一个随机张量。torch.rand生成[0, 1)之间的均匀分布随机数。\n    # 加上keep_prob后，random_tensor的范围变为 [keep_prob, 1 + keep_prob)\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.devide)\n    # floor_(): 对随机张量进行向下取整\n    # 结果是：原先在 [keep_prob, 1) 区间的数值变为0，原先在 [1, 1 + keep_prob) 区间的数值变为1\n    # 一个数落在 [1, 1+keep_prob) 的概率恰好是 keep_prob\n    # 这样，random_tensor就变成了一个二值掩码（0或1）\n    random_tensor.floor_()\n    # 将输入x除以keep_prob，然后乘以二值掩码\n    # 乘以random_tensor：将一部分样本的整个张量置零（实现DropPath）。\n    # 除以keep_prob：这是一种被称为\"Inverted Dropout\"的技术。通过在训练时放大保留下来的输出，\n    # 可以保证在推理时（此时keep_prob为1，不做任何操作）网络的期望输出与训练时保持一致，无需在推理阶段进行额外的缩放\n    output = x.div(keep_prob)*random_tensor\n    return output\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super(DropPath,self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self,x):\n        return drop_path(x,self.drop_prob,self.training) \n\n\nclass Block(nn.Module):\n    def __init__(self,\n                 dim, # 每个token的维度\n                 num_heads,  #多头自注意力的头数量\n                 mlp_ratio=4,  #计算hidden_features大小 为输入的四倍\n                 qkv_bias=False,  # qkv偏置\n                 qk_scale = None,  # 注意力缩放因子\n                 drop_ratio=0.,  # 多头自注意力机制的最后dropout比例\n                 attn_drop_ratio=0.,  # 生成qkv之后的dropout比例\n                 drop_path_ratio=0., # drop_path比例\n                 act_layer=nn.GELU,  # 激活函数\n                 norm_layer=nn.LayerNorm  # 正则化层\n                 ):\n        super(Block,self).__init__()\n        self.norm1 = norm_layer(dim) # transformer encoder 中的第一个norm层\n        self.attn = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias,qk_scale=qk_scale,\n                              attn_drop_ratio=attn_drop_ratio,proj_drop_ration=drop_ratio)\n        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio else nn.Identity()\n        self.norm2 = norm_layer(dim)  # 定义第二个layer_norm层\n        mlp_hidden_dim = int(dim*mlp_ratio)\n        # 定义mlp层\n        self.mlp = MLP(in_features=dim,hidden_features=mlp_hidden_dim,act_layer=act_layer,drop=drop_ratio)\n\n    def forward(self,x):\n        x = x + self.drop_path(self.attn(self.norm1(x))) # 前向传播部分，输入的x先经过layernorm再经过多头注意力\n        x = x + self.drop_path(self.mlp(self.norm2(x)))  # 将得到的x一次通过layernorm、mlp、drop_path\n\n4. Vision Transformerdef _init_vit_weights(m):\n    # 1. 如果是全连接层\n    if isinstance(m,nn.Linear):\n        # 使用截断正态分布进行初始化，这是原始ViT论文推荐的方法\n        # 截断正态分布可以防止权重值离均值太远，让初始状态更稳定\n        # std=0.02 是一个经验值。带下划线的方法表示这是个in-place（原地）操作\n        nn.init.trunc_normal_(m.weight,std=0.01)\n        # 如果该线性层有偏置(bias)项\n        if m.bias is not None:\n            # 将偏置初始化为0\n            nn.init.zeros_(m.bias)\n    # 2. 如果是2D卷积层\n    elif isinstance(m,nn.Conv2d):\n        # 使用Kaiming正态分布初始化。这是一种非常适合带有ReLU激活函数的卷积层的初始化方法\n        # 它可以有效防止梯度消失或爆炸\n        # mode=\"fan_out\" 表示根据输出通道数来调整方差\n        nn.init.kaiming_normal_(m.weight,mode=\"fan_out\")\n        # 如果该卷积层有偏置项\n        if m.bias is not None:\n            # 将偏置项初始化为0\n            nn.init.zeros_(m.bias)\n    # 3. 如果是层归一化层\n    elif isinstance(m,nn.LayerNorm):\n        # 将偏置初始化为0\n        nn.init.zeros_(m.bias)\n        # 将权重初始化为1\n        nn.init.ones_(m.weight)\n        # 这样做的目的是，当训练刚开始时，LayerNorm层几乎等同于一个恒等映射，不会改变输入的分布，让训练更稳定\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size=224,patch_size=16,in_c=3,num_classes=1000,\n                 embed_dim=768,depth=12,num_head=12,mlp_ratio=4.0,qkv_bias=True,\n                 qk_scale=None,representation_size=None,distilled=False,drop_ratio=0.,\n                 attn_drop_ratio=0.,drop_path_ratio=0.,embed_layer=PatchEmbed,norm_layer=None,\n                 act_layer=None):\n        \"\"\"\n        ViT模型构造函数。\n        Args:\n            img_size (int): 输入图像的尺寸\n            patch_size (int): 每个图像块(patch)的尺寸\n            in_c (int): 输入图像的通道数\n            num_classes (int): 最终分类的类别数\n            embed_dim (int): Token的嵌入维度 (D)\n            depth (int): Transformer Encoder的总层数\n            num_head (int): 多头注意力机制中的头数\n            mlp_ratio (float): Transformer Encoder中MLP层的维度扩展比例\n            qkv_bias (bool): 是否在Q,K,V生成时使用偏置\n            qk_scale (float, optional): QK缩放因子，默认为 1/sqrt(head_dim)\n            representation_size (int, optional): 在最终分类头之前，可选的中间全连接层维度\n            distilled (bool): 是否使用蒸馏模式 (DeiT模型)\n            drop_ratio (float): 全局Dropout比率\n            attn_drop_ratio (float): 注意力权重Dropout比率\n            drop_path_ratio (float): 随机深度的DropPath比率\n            embed_layer (nn.Module): 用于生成Patch Embedding的层\n            norm_layer (nn.Module, optional): 使用的归一化层\n            act_layer (nn.Module, optional): 使用的激活函数层\n        \"\"\"\n        # 调用父类nn.Module的初始化方法\n        super(VisionTransformer,self).__init__()\n        self.num_classes = num_classes # 保存分类数\n        self.num_features = self.embed_dim = embed_dim # 保存嵌入维度\n        # 判断是否使用蒸馏。如果使用，会多一个distillation token，总共2个特殊token；否则只有cls_token，1个\n        self.num_tokens = 2 if distilled else 1\n        # 如果未指定归一化层，则默认使用LayerNorm，并设置eps以增加数值稳定性。\n        # partial用于创建一个预设了部分参数的新函数\n        norm_layer = norm_layer or partial(nn.LayerNorm,eps=1e-6)\n        \"\"\"\n        partial 来自 Python 内置的 functools 模块，它的作用是将一个函数的某些参数“冻结”住，从而创建一个新的、更简单的函数\n        此处的意义是将所有nn.LayerNorm的eps固定为1e-6\n        \"\"\"\n        act_layer = act_layer or nn.GELU() # 如果未指定激活函数，则默认使用GELU\n        # 1. Patch Embedding层：将输入的图片(B, C, H, W)转换为一系列token(B, N, D)\n        self.patch_embed = embed_layer(img_size=img_size,patch_size=patch_size,in_c=in_c,embed_dim=embed_dim)\n        # 获取patch的数量\n        num_patches = self.patch_embed.num_patches  \n        # 2. 定义可学习的 [CLS] token。这个token最终的输出将代表整个图像的特征用于分类\n        # 初始形状为(1, 1, embed_dim)，1个token，维度为embed_dim\n        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n        # 3. 如果是蒸馏模式，定义可学习的 [DIST] token\n        self.dist_token = nn.Parameter(torch.zeros(1,1,embed_dim)) if distilled else None\n        # 4. 定义可学习的位置编码(Positional Embedding)。因为Transformer本身不感知顺序，需要它来提供位置信息\n        # 长度为 patch数量 + 特殊token数量\n        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches+self.num_tokens,embed_dim))\n        # 5. 在位置编码加入后，应用一个Dropout层\n        self.pos_drop = nn.Dropout(p = drop_ratio)\n        # 6. 构建随机深度(Stochastic Depth)的衰减率序列\n        # torch.linspace生成一个从0到drop_path_ratio的等差序列，长度为depth\n        # 这样，越深的Block，其drop_path_ratio越大，被\"丢弃\"的概率也越高\n        dpr = [x.item() for x in torch.linspace(0,drop_path_ratio,depth)]\n        # 7. 构建Transformer Encoder主体，由连续的Block堆叠而成\n        # 使用nn.Sequential将多个Block串联起来\n        self.block = nn.Sequential(*[\n            Block(dim=embed_dim,num_heads=num_head,mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,qk_scale=qk_scale,\n                  drop_ratio = drop_ratio,attn_drop_ratio=attn_drop_ratio,drop_path_ratio=dpr[i],\n                  norm_layer=norm_layer,act_layer=act_layer)\n            for i in range(depth)\n        ])\n        # 8. 经过所有Transformer Block后的最后一个LayerNorm层。\n        self.norm = norm_layer(embed_dim)\n        # 9. 定义分类头之前的一个可选的\"pre-logits\"层 (常用于从JFT等大数据集预训练后迁移学习)\n        if representation_size and not distilled:\n            self.has_logits = True\n            # 更新最终输出的特征维度\n            self.num_features = representation_size\n            # pre_logits是一个包含全连接层和Tanh激活的序列\n            self.pre_logits = nn.Sequential(OrderedDict([\n                (\"fc\",nn.Linear(embed_dim,representation_size)),\n                (\"act\",nn.Tanh())\n            ]))\n        else:\n            self.has_logits=False\n            # 如果不使用，则用一个恒等映射层代替\n            self.pre_logits=nn.Identity()\n        # 10. 定义最终的分类头 (Head)\n        # 将提取的特征映射到最终的分类数\n        self.head = nn.Linear(self.num_features,num_classes) if num_classes&gt;0 else nn.Identity()\n        # 11. 如果是蒸馏模式，为distillation token也定义一个分类头\n        self.head_dist = None\n        if distilled:\n            self.head_dist = nn.Linear(self.embed_dim,self.num_classes) if num_classes&gt;0 else nn.Identity()\n\n        # 12. 权重初始化\n        # 对位置编码、dist_token、cls_token进行截断正态分布初始化\n        nn.init.trunc_normal_(self.pos_embed,std=0.02)\n        if self.dist_token is not None:\n            nn.init.trunc_normal_(self.dist_token,std=0.02)\n        nn.init.trunc_normal_(self.cls_token,std=0.02)\n        # 使用self.apply()方法，将_init_vit_weights函数递归地应用到模型的所有子模块上\n        self.apply(_init_vit_weights)\n\n    def forward_features(self,x):\n        \"\"\"提取特征的前向传播过程，不包括最后的分类头。\"\"\"\n        # x 初始形状: [B, C, H, W]\n        # 1. Patch Embedding: [B, C, H, W] -&gt; [B, num_patches, embed_dim]\n        x = self.patch_embed(x)\n        # 2. 将cls_token在batch维度上进行扩展，以匹配输入x的batch_size\n        # expand()是一个高效的操作，它不会实际复制数据\n        cls_token = self.cls_token.expand(x.shape[0],-1,-1) # [1, 1, D] -&gt; [B, 1, D]\n        # 3. 将特殊token与patch token拼接在一起\n        if self.dist_token is None:\n            # 若没有蒸馏token，拼接: [B, 1, D] 和 [B, N, D] -&gt; [B, N+1, D]\n            x = torch.cat((cls_token,x),dim=1)\n        else:\n            # 蒸馏模式下，拼接cls_token和dist_token\n            x = torch.cat((cls_token,self.dist_token.expand(x.shape[0],-1,-1),x),dim=1)\n        # 4. 加上位置编码，然后应用dropout\n        # pos_embed的[B]维度会自动广播以匹配x\n        x = self.pos_drop(x+self.pos_embed)\n        # 5. 通过Transformer Encoder主干网络\n        x = self.block(x)\n        # 6. 通过最后的LayerNorm\n        x = self.norm(x)\n        # 7. 提取用于分类的token的输出\n        if self.dist_token is None:\n            # 只返回cls_token的输出 (在序列的第0个位置)，并通过pre_logits层\n            return self.pre_logits(x[:,0])\n        else:\n            # 返回cls_token和dist_token的输出\n            return x[:,0],x[:,1]\n\n    def forward(self,x):\n        \"\"\"完整的从输入到输出的前向传播过程。\"\"\"\n        # 1. 首先通过forward_features提取特征\n        x = self.forward_features(x)\n        # 2. 通过最后的分类头得到logits\n        if self.head_dist is not None:\n            # 蒸馏模式下，两个token分别通过各自的分类头\n            # x此时是一个元组 (cls_output, dist_output)\n            x_cls,x_dist = self.head(x[0]),self.head_dist(x[1])\n            if self.training and not torch.jit.is_scripting():\n                # 在训练并且不是在 TorchScript 编译模式下，返回两个头的输出，以便分别计算损失\n                return x_cls,x_dist\n            else:\n                # 在评估时，返回两个头输出的平均值作为最终预测\n                return (x_cls + x_dist) / 2\n        else:\n            # 标准模式下，直接将特征通过分类头\n            x = self.head(x)\n        return x\n\n\n\n\n\n\n\n\n\n\n蒸馏模式与 dist_token代码里的“蒸馏模式”来源于一篇非常重要的论文 DeiT (Data-efficient Image Transformers)。(1)什么是知识蒸馏 (Knowledge Distillation)？这是一种模型压缩和迁移学习的技术，核心思想是让一个强大而复杂的“教师模型”来指导一个轻量级的“学生模型”进行学习。\n\n教师模型: 通常是一个已经在大规模数据集上训练好的、性能非常强的模型（比如一个超大的 ResNet 或者另一个 ViT）。\n学生模型: 我们当前正在训练的模型（比如这个 VisionTransformer）。指导的方式不仅仅是让学生模型学习正确的标签（比如图片是“猫”），还会让它学习教师模型输出的“软标签”。软标签是指教师模型对所有类别的预测概率分布，例如它可能认为图片是“猫”的概率是85%，是“狗”的概率是10%，是“老虎”的… 这个概率分布包含了教师模型“思考过程”的丰富信息。\n\n(2)为什么 ViT 需要蒸馏？原始的 ViT 需要在海量的数据集（如谷歌内部的 JFT-300M，包含3亿张图片）上预训练才能获得优异的性能。如果只在 ImageNet-1k（约120万张图片）这种“中等”规模的数据集上从头训练，效果往往不如经典的 CNN 模型。DeiT 论文发现，通过知识蒸馏，可以让一个 ViT 在仅使用 ImageNet-1k 的情况下，达到甚至超过在 JFT-300M 上预训练的效果，极大地提高了 ViT 的数据效率。\n(3)dist_token 的作用DeiT 论文提出了一种新颖的蒸馏方式，就是通过添加一个专门用于蒸馏的 distillation token（即 dist_token）。\n\ncls_token 的任务: 和原来一样，它的最终输出用来和真实的标签 (ground-truth label) 计算损失，我们称之为“硬标签损失”。\ndist_token 的任务: 它是一个和 cls_token 地位相同的可学习向量，也被拼接到序列中，通过 Transformer 网络。但它的最终输出是专门用来和教师模型的预测（软标签或硬标签） 计算损失的，我们称之为“蒸馏损失”。\n\n通过这种方式，模型在训练时会同时优化两个目标：\n\n让 cls_token 的输出尽可能接近真实答案。\n让 dist_token 的输出尽可能模仿教师模型的答案。这种双重监督机制被证明非常有效，dist_token 就像一个专门负责从教师那里“偷师学艺”的通道，帮助学生模型学得更好。\n\npre-logits 层的作用pre-logits 层，可以理解为一个在最终分类头（self.head）之前的一个特征处理/映射层。它的主要作用是为了更好地进行迁移学习。想象一个场景：\n\n一个机构（比如谷歌）在一个超级庞大的私有数据集（比如 JFT-300M，有18000个类别）上预训练了一个 ViT 模型。\n这个预训练模型的最终分类头是 nn.Linear(embed_dim, 18000)。\n现在你想把这个模型用到你自己的任务上，比如一个只有10个类别的猫狗分类任务。\n\n显然，那个输出18000个维度的分类头对你来说是没用的。但是，它之前的网络层学到的特征提取能力是非常宝贵的。在这种情况下，pre-logits 层就派上用场了：\n\n它通常是一个 nn.Linear(embed_dim, representation_size) 加上一个激活函数（如 Tanh）。\n在预训练时，模型会先将 cls_token 的输出通过这个 pre-logits 层，得到一个固定维度的“特征表示” (representation)，然后再将这个表示送入最终的分类头。\n当你拿到这个预训练模型进行迁移学习时，你可以丢弃掉原有的最终分类头，但保留 pre-logits 层。然后，你只需要在 pre-logits 层的输出后面接上你自己任务的分类头，例如 nn.Linear(representation_size, 10)。\n\n5. 创建应用层def vit_base_patch16_224(num_classes:int =100, pretrained=False):\n    model = VisionTransformer(img_size=224,\n                              patch_size=16,\n                              embed_dim=768,\n                              depth=12,\n                              num_head=12,\n                              representation_size=None,\n                              num_classes=num_classes)\n    return model\n\n参考笔记：ViT讲解\n","slug":"ViT论文精读","date":"2025-07-02T03:32:00.000Z","categories_index":"","tags_index":"Vision-Transformer,深度学习","author_index":"犬夜叉"}]