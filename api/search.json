[{"id":"8c60d49e88f3e150291142683c6b267b","title":"ViLT 论文精读","content":"摘要\n\n\n\n\n\n\n\n\n视觉语言预训练已提升了各类视觉 - 语言联合下游任务的性能。当前 VLP 方法高度依赖图像特征提取流程，其中大部分涉及区域监督（如目标检测）和卷积架构（如 ResNet）。尽管现有文献未予重视，但我们发现这一模式存在两方面问题：（1）效率 / 速度层面，仅输入特征提取所需的计算量就远超多模态交互步骤；（2）表达能力层面，其上限受限于视觉嵌入器的表达能力及其预定义的视觉词汇表。本文提出一种极简 VLP 模型 —— 视觉语言 Transformer（ViLT），其核心在于将视觉输入的处理大幅简化为与文本输入相同的无卷积模式。实验表明，ViLT 的速度可达以往 VLP 模型的数十倍，同时在下游任务中具备相当或更优的性能。我们的代码和预训练权重可从https://github.com/dandelin/vilt获取。\n论文介绍1. 视觉语言预训练（VLP）的现状与挑战\n主流方法的依赖与问题： 现有 VLP 模型高度依赖基于卷积神经网络（CNN）的图像特征提取（如 ResNet）和区域监督（如目标检测），导致两大核心问题：\n效率瓶颈：特征提取的计算量远超多模态交互步骤（如 UNITER 模型中视觉处理耗时占比超 90%）。\n表达能力受限：依赖预定义的视觉词汇表（如 Visual Genome 的 1600 个物体类别），难以泛化未知物体或场景。\n\n\n学术研究与实际应用的脱节： 学术实验中常通过预缓存区域特征减轻训练时的计算负担，但在实际应用中，实时输入仍需经历耗时的特征提取流程，限制了模型的部署效率。\n\n2. ViLT 的核心创新：极简架构与统一处理\n无卷积的视觉特征嵌入： 受 ViT（Vision Transformer）启发，ViLT 将图像分割为固定大小的补丁（如 32×32 像素），通过线性投影层直接生成视觉嵌入，完全摒弃 CNN 和目标检测模块。这一设计使视觉处理耗时仅为～0.4 ms，参数仅 2.4M，远低于 ResNet 等传统 backbone。\n统一的 Transformer 架构： 视觉和文本输入均通过 Transformer 进行多模态交互，首次实现 模态特定组件计算量（VE+TE）&lt;多模态交互计算量（MI） 的架构（如图 1 中 ViLT 的 MI 耗时～15 ms，远超 VE 的 0.4 ms）。这种 轻嵌入、重交互 的设计显著提升计算效率，同时保持下游任务性能。\n\n3. 性能与效率对比\n速度优势： ViLT 的总推理时间～15 ms，比基于区域特征的模型（如 UNITER，900 ms）快 60 倍以上，比基于网格特征的 Pixel-BERT（60 ms）快 4 倍。\n任务表现： 在 NLVR2（视觉推理）、F30K 检索等任务中，ViLT 的准确率与传统模型相当或更优（如 NLVR2 测试集准确率 74.57%，接近 UNITER 的 75.8%），证明无需复杂视觉 backbone 仍可实现有效跨模态建模。\n\n4. 关键贡献与意义\n架构革新： 首次证明 VLP 模型可完全脱离卷积和区域监督，为轻量化多模态模型设计提供新范式。\n方法创新： 引入全词掩码（Whole Word Masking）和图像增强（RandAugment），提升预训练效率和下游任务泛化能力。\n研究启示： 呼吁 VLP 领域从 “单模态特征增强” 转向 “多模态交互优化”，为后续模型（如更大规模的 ViLT-L/H）奠定基础。从论文中的图片可以看出，对于传统模型，一般的处理流程包括：\n\n\n图像输入后，先通过卷积神经网络提取网格特征，再经目标检测模块生成区域建议（RoI），并通过非极大值抑制（NMS）和 RoI 头处理，最终得到区域特征。\n直接使用 CNN backbone（如 ResNet）输出的网格特征，跳过目标检测步骤，通过线性嵌入输入 Transformer。在这片论文中，图像直接分割为固定大小的补丁（如 32×32 像素），通过线性投影层（Linear Embedding）将每个补丁转换为特征向量，无需 CNN 或目标检测模块。\n\n\n效率优势：ViLT 的总运行时间（~15 ms）仅为 UNITER 的 1.7%，Pixel-BERT 的 25%。\n性能保持：在 NLVR2、F30K 检索等任务中，ViLT 性能接近或超过传统模型，证明去除 CNN 和区域监督不会显著损失表达能力。\n\n研究背景1. 视觉-语言模型的分类体系\n2. 模态交互方式\n单流 vs. 双流：\n单流模型（如 VisualBERT）将图像和文本拼接后输入 Transformer，参数效率更高；\n双流模型（如 ViLBERT）分模态处理，引入额外参数，ViLT 采用单流架构。\n\n\n\n3. 视觉嵌入的瓶颈\n区域特征： 通过 Faster R-CNN 等目标检测器生成，需经历 RPN、NMS 等复杂流程，计算量大（如 UNITER 视觉处理耗时 810 ms），且依赖预定义类别（如 Visual Genome 的 1600 个物体类），泛化能力受限。\n网格特征： 直接使用 CNN 输出（如 Pixel-BERT），虽省去目标检测，但 CNN backbone（如 ResNet）仍为性能瓶颈（Pixel-BERT-R50 视觉处理耗时 45 ms）。\nViLT 的创新： 采用Patch投影，将图像分割为 32×32 像素块，通过线性投影生成嵌入，仅需 0.4 ms 和 2.4M 参数，彻底摒弃 CNN 和目标检测。\n\n4. 关键问题与突破\n传统模型的效率与表达局限： 视觉嵌入的高计算量（如区域特征提取）和预定义视觉词汇限制了模型速度和泛化能力，学术实验中预缓存特征的做法无法解决实际应用中的实时性需求。\nViLT 的设计思路： 受 ViT 启发，利用 Transformer 的自注意力机制直接处理图像补丁和文本 tokens，将计算重点从 “单模态特征提取” 转向 “跨模态深度交互”，实现轻量化与高性能的平衡。\n\nVision-and-Language Transformer一、模型整体架构与核心设计1. 统一的单流Transformer架构ViLT采用单流 Transformer 处理视觉和文本输入，即图像和文本嵌入被拼接为单一序列，通过多层Transformer层进行联合建模。这一设计避免了双流架构的额外参数开销，同时确保深度跨模态交互。\n2. 视觉嵌入：无卷积的Patch投影\n输入处理：图像被分割为固定大小的补丁（如32×32像素），每个补丁被展平为一维向量  为补丁尺寸， 为图像通道数， 为补丁数量）。\n线性投影：通过可学习的线性层  将补丁向量映射到隐藏空间 （ 为隐藏层维度），并添加位置嵌入 。\n关键优势：\n仅需 2.4M参数，远小于ResNet-50（25M）等卷积 backbone。\n计算耗时仅 0.4 ms，彻底消除CNN和目标检测的耗时瓶颈。\n\n\n\n3. 文本嵌入：基于BERT的轻量设计\n输入处理：文本通过BERT分词器生成 tokens，包含特殊标记（如[CLS]）和位置嵌入 。\n模态类型嵌入：为区分视觉和文本模态，分别添加模态类型向量  和，避免模态混淆。\n\n4. 多模态交互：Transformer编码器\n输入拼接：视觉嵌入  和文本嵌入  拼接为 。\nTransformer层：通过12层Transformer编码器（每层包含多头自注意力MSA和MLP）进行特征交互，公式如下：其中，层归一化采用ViT的“前归一化”策略（先归一化再计算注意力/MLP），与BERT的“后归一化”不同。\n池化输出：最终序列的首个标记  通过线性投影和激活函数生成池化表示 ，用于下游任务分类或检索。\n\n二、与训练目标与技术创新实验数据集介绍：1、 MSCOCO（Microsoft Common Objects in Context）\n\n规模：113K 张图像，567K 条字幕（平均每张图 5 条字幕），字幕长度为 11.81±2.81。- 定位：微软开发的 多任务基准数据集，支持目标检测、场景分割、图像字幕、视觉问答（VQA）等任务。\n核心特点：- 字幕标注 聚焦场景核心内容（如 “一只狗在公园奔跑”），避免冗余细节，适合训练模型抓重点；- 图像覆盖真实生活场景（如家庭、户外），物体类别丰富（80 类常见物体），是视觉语言任务的 “试金石”。\n\n2、VG（Visual Genome）\n\n规模：108K 张图像，5.41M 条字幕（平均每张图 50 条 + 字幕），字幕长度为 5.53±1.76。\n定位：斯坦福李飞飞团队推出的 细粒度语义理解数据集，主打 场景图（Scene Graph） 标注（结构化描述物体、属性、关系，如 “人→骑→自行车”）。\n核心特点：\n标注极丰富：除字幕，还有 “区域描述”（局部区域的文本解释）、“视觉问答”（针对图像的问答对）；\n分布呈长尾性：常见关系（如 “on” “in”）占比极高，稀有关系极少，后续衍生出 “VG150” 等清洗版本（筛选高频类别）。\n\n\n\n 3、 GCC（Google Conceptual Captions）\n\n规模：3.01M 张图像，3.01M 条字幕（1:1 配对），字幕长度为 10.66±4.93。\n定位：大规模弱监督数据集，图像和字幕从网络爬取，属于 “弱关联” 标注（无需严格语义对应）。\n核心特点：\n数据极多样：图像风格覆盖更广（如艺术画、表情包），突破 MSCOCO 的场景限制，提升模型泛化能力；\n预训练核心：ViLT 等模型用其预训练，通过海量数据学习 “松散跨模态关联”，弥补强监督数据集的规模不足。\n\n\n\n 4、 SBU（SBU Captions）\n\n规模：867K 张图像，867K 条字幕（1:1 配对），字幕长度为 15.0±7.74。\n定位：早期 图像 - 字幕检索数据集，最初用于 “给定图像找匹配字幕” 或反之的检索任务。\n核心特点：\n字幕更长且自由（均值 15 词，长于其他数据集），适合训练模型理解复杂文本描述；\n标注简洁：1 张图对应 1 条字幕，训练成本低，常与其他数据集联合预训练（如 ViLT）。\n\n\n\n","slug":"ViLT-论文精读","date":"2025-07-02T09:37:29.000Z","categories_index":"","tags_index":"多模态,VLP","author_index":"犬夜叉"},{"id":"64c2453c0ac7d584fda9903cba47649e","title":"CLIP 论文精读","content":"CLIP论文精读CLIP通过对比学习从大量的图像-文本中学习视觉概念，实现了强大的零样本图像分类能力\n\n\n\n\n\n\n\n\n\n论文地址：Learning Transferable Visual Models From Natural Language Supervision\n\n\n\n\n\n\n\n\n\n论文创新点：\n\n它不使用传统计算机视觉任务中常见的、带有固定类别（如“猫”、“狗”）的标签，而是直接从互联网上收集的（图像，文本）对中学习。这种文本描述提供了比单一标签丰富得多的监督信号，涵盖了几乎无限的视觉概念。\n为了让模型理解图像与文本的关联，CLIP采用了一种名为“对比学习”的自监督方法。其核心思想是：\n构建双编码器架构：CLIP包含一个图像编码器（Image Encoder，如ResNet或Vision Transformer）和一个文本编码器（Text Encoder，如Transformer）。\n学习多模态嵌入空间：在训练过程中，模型会接收一批图像和文本。图像编码器将图像转换为特征向量，文本编码器将文本转换为特征向量。CLIP的目标是在这个共享的多模态嵌入空间中，拉近真实的“图像-文本”对的特征向量（正样本），同时推远不匹配的“图像-文本”对的特征向量（负样本）。\n高效的代理任务：通过判断哪个文本与哪个图像配对，这个看似简单的“代理任务”却极其高效地迫使模型学习图像内容和文本语义之间的深刻联系。\n\nCLIP最令人瞩目的成果是其强大的零样本学习能力。传统的模型在面对一个新的分类任务时，通常需要进行微调，即在新任务的标注数据上进行再训练，而经过预训练的CLIP无需任何微调即可直接应用于新的视觉分类任务。其实现方式为：\n\n\n动态构建分类器：对于一个给定的分类任务（例如，区分“猫”和“狗”的图片），CLIP会将类别名称（”cat”, “dog”）转换成标准的提示语，如 “a photo of a cat” 和 “a photo of a dog”。\n相似度匹配预测：将这些提示语通过文本编码器生成文本特征向量。当输入一张待分类的图像时，图像编码器会生成其图像特征向量。最后，模型会计算该图像特征向量与所有类别提示语的文本特征向量之间的余弦相似度，相似度最高者即为预测的类别。\n\n\n\n\n1. 核心方法：对比学习CLIP的目标不是像传统模型那样预测一个固定的类别，而是学习一个多模态的嵌入空间，在这个空间里，匹配的图像和文本对的特征向量距离很近，而不匹配的则很远。具体实现如下：\n\n构建批次：在一个训练批次中，包含 N 个匹配的对。\n双塔编码：\n图像编码器 (Image Encoder)：将 N 个图像编码成 N 个图像特征向量 \n文本编码器 (Text Encoder)：将 N 个文本编码成 N 个文本特征向量 \n\n\n计算相似度：计算所有可能的 对的余弦相似度，形成一个 N×N 的相似度矩阵\n定义损失：在这个矩阵中，对角线上的 N 个元素是正样本（匹配的图文对），其余的  个元素都是负样本。CLIP的优化目标是一个对称的交叉熵损失函数，即同时在行和列的方向上进行优化：对于每个图像，模型需要从 N 个文本中找出正确的那个；反之，对于每个文本，也需要从N个图像中找出正确的那个。\n\n2. 核心能力：零样本迁移这是CLIP方法论的直接应用，也是其价值的主要体现\n\n动态构建分类器：对于任何一个分类任务（比如对ImageNet的1000类进行分类），CLIP不需要重新训练。而是通过“提示工程”为每个类别创建一个或多个描述性文本。例如，对于类别 “dog”，可以生成文本 “A photo of a dog.”。\n推理与匹配：将待分类的图像输入Image Encoder得到其特征 。然后，将所有类别的提示文本输入Text Encoder得到一组类别特征 ,… 。最后，计算  与每个类别特征  的余弦相似度，相似度最高者即为预测类别。\n提示工程的重要性：精心设计的提示语至关重要。它能解决词义模糊问题（如”boxer”是狗还是拳击手）并提升性能。论文中通过集成80个不同的提示模板，在ImageNet上的准确率提升了3.5%。这一技巧的有效性，使得CLIP的性能得到了显著增强。\n\n3. 关键决策：追求最高的训练效率在论文中，作者强调，由于计算资源是有限的，选择一个计算效率最高的预训练方法至关重要。他们对比了三种方法：\n\n方法一：Transformer语言模型 (预测文本)：类似VirTex，用图像作为上下文，生成描述文本。这种方法表现力强，但任务难度大，学习效率最低。\n方法二：词袋模型：不要求生成完整句子，只要求预测文本中的单词。效率比方法一高3倍，但仍不够理想。\n方法三：对比学习：只要求判断图文是否匹配，任务最简单。其训练效率比词袋模型还要高4倍。这个实验结论是CLIP成功的关键之一：在超大规模数据下，一个更简单、更高计算效率的训练目标，能让模型在有限时间内学到更好的表征。\n\n4. 模型与数据规模\n数据集：OpenAI构建了一个名为WebImageText的私有数据集，包含从互联网上收集的4亿个图文对。\n图像编码器：论文中测试了两种架构：\nResNet-D：对标准ResNet进行了一些修改，如用注意力池化层替换全局平均池化层。共测试了5个不同规模的ResNet。\nVision Transformer (ViT)：共测试了3个不同规模的ViT。实验发现ViT的计算效率比ResNet更高。最终性能最好的模型是 ViT-L/14，并在336x336的分辨率下进行了额外的微调。\n\n\n文本编码器：一个标准的63M参数、12层、512宽、8个注意力头的Transformer模型。\n\n5. 实验结果与分析CLIP的实验部分非常详尽，覆盖了超过30个不同的数据集，主要结论如下：\n\n与全监督模型匹敌：在ImageNet上，Zero-shot CLIP的准确率可以达到76.2%，与一个在ImageNet上经过完整监督训练的ResNet-50效果相当。\n超强的鲁棒性：CLIP最令人印象深刻的是其在自然分布漂移上的表现。在ImageNet-V2, Rendition, Sketch, Adversarial等更具挑战性的数据集上，其性能远超监督模型。最极端的例子是在ImageNet-A上，ResNet101的准确率从76.2%骤降至2.7%，而CLIP仍能达到77.1%，展现了惊人的泛化能力。\n数据重叠检查：为了验证性能不是来自于训练集和测试集的重叠（数据泄露），作者进行了详尽的检查，发现重合率中位数仅为2.2%，且移除这些重叠样本后，模型性能没有显著变化，证明了其强大的泛化能力是真实有效的。\n优秀的特征表示：即使不用于零样本分类，CLIP学习到的特征本身也极为优秀。在标准的线性探查（linear probe）评测中，CLIP的特征在性能和计算效率方面均优于当时的其他自监督方法。上面图片中，左图估算了线性分类器达到零样本 CLIP 性能所需的每类标注样本数，范围从不足 1 到 184，中位数 5.4，均值 20.8。这表明zero - shot transfer 数据效率差异大，部分任务需大量标注，部分几乎无需。图 8 显示zero - shot 与线性探针性能正相关，但zero - shot 普遍低 10%-25%，仅 5 个数据集接近。这说明 CLIP zero - shot 能力与表征质量相关，但仍有提升空间，多数任务距最优有差距。上图对比了 CLIP 模型与 EfficientNet、MoCo、ViT 等先进计算机视觉模型的线性探针性能。左图为 Kornblith 等研究的 12 个数据集平均分数，右图为 27 个更多样分布数据集的平均分数。结果显示，CLIP 模型，尤其是 ViT 架构的 CLIP-L/14@336px，在两类数据集上均表现出色，其最佳模型平均得分超过现有模型，且 Vision Transformer 比 ResNet 更高效。虚线表示微调或高分辨率评估的模型，体现了 CLIP 在表征学习上的优势与高效性。\n\n","slug":"CLIP-论文精读","date":"2025-07-02T08:41:41.000Z","categories_index":"","tags_index":"多模态,对比学习","author_index":"犬夜叉"},{"id":"290cb712ccbf87b18cb10d019f08a741","title":"ViT 论文精读","content":"Vision Transformer的出现标志着 Transformer 架构成功应用于计算机视觉领域，挑战了卷积神经网络在该领域的主导地位。ViT 通过将图像分割成小块 (patches)，并将这些图像块视为序列输入到标准的 Transformer 编码器中，从而实现了对图像的有效处理。这一进展不仅在图像分类等任务上取得了最先进的成果，更重要的是，它为视觉和语言（以及其他模态）提供了一种通用的架构语言——Transformer。这种架构上的统一极大地促进了后续多模态模型（如CLIP、LLaVA等，它们通常采用ViT或其变体作为视觉编码器）的设计和发展，使得不同模态的基于token的表示可以在相似的计算框架内进行交互和融合。 \n\n\n从数据图中可以看出，在较小的数据集上，Vision Transformer比计算成本相当的ResNets更容易过拟合。例如，ViT-B/32比ResNet50稍快；它在9M子集上的表现要差得多，但在90M+子集上表现更好。ResNet152x2和ViT-L/16也是如此。这个结果强化了这样一种直觉：卷积的归纳偏置对于较小的数据集是有用的，但对于更大的数据集，直接从数据中学习相关模式是足够的，甚至是有益的。\nI. 摘要尽管 Transformer 架构已成为自然语言处理（NLP）任务事实上的标准，但其在计算机视觉领域的应用仍然有限。在视觉领域，注意力机制要么与卷积网络（CNN）结合使用，要么用于替代卷积网络中的某些组件，但整体结构仍然保留。我们证明了这种对 CNN 的依赖并非必要，一个直接应用于图像块序列的纯 Transformer 模型可以在图像分类任务上表现得非常好。 当在大量数据上进行预训练，并迁移到多个中等或小型图像识别基准测试（如 ImageNet, CIFAR-100, VTAB 等）时，Vision Transformer (ViT) 相比于最先进的卷积网络取得了优异的结果，同时训练所需的计算资源也大幅减少。\nII. 创新点范式革新：将图像视为序列处理论文首次证明了一个纯粹的、标准的 Transformer 模型可以直接用于图像分类，而无需依赖卷积神经网络。传统视觉任务长期由 CNN 主导，这篇论文打破了这一惯例。它通过将图像分割成固定大小的图块，并将这些图块的线性嵌入序列作为 Transformer 的输入，成功地将 NLP 领域的成功范式迁移到了视觉领域。\n数据量胜于归纳偏置论文发现，当在超大规模数据集（如 JFT-300M，包含3亿张图片）上进行预训练时，Vision Transformer (ViT) 的性能超越了当前最先进的卷积网络。这揭示了一个重要现象：CNN 中固有的（如局部性、平移不变性）的归纳偏置在数据量较小时非常有效，但当数据量足够大时，模型可以从数据中直接学习到这些空间关系，强大的模型容量和更少的先验限制反而成为优势。\n卓越的计算效率和可扩展性与性能相当的 SOTA 卷积网络相比，ViT 在达到同等甚至更高精度时，所需的预训练计算资源要少得多。例如，ViT-L/16 在 JFT-300M 数据集上预训练后，其性能优于在同一数据集上训练的 BiT-L (一个大型 ResNet 模型)，而训练成本却显著降低。这证明了 Transformer 架构在可扩展性上的巨大潜力。\n简洁而有效的模型设计论文的设计理念是尽可能少地修改原始的 Transformer 架构，使其可以直接利用 NLP 领域成熟的高效实现和可扩展架构。这种简洁性不仅体现在模型结构上，也体现在对图像的处理上，除了初始的图块划分和用于适应不同分辨率的位置编码插值外，几乎没有引入图像特有的归纳偏置。\nIII. 网络原理详解ViT模型概览ViT模型的核心思想是将图像转换为一个序列，然后用标准的Transformer Encoder来处理这个序列。\n图像分块处理 (Image Patching)Transformer接受的输入数据格式是一维的词嵌入序列，为了处理二维图像数据，论文将图像  重塑为一个扁平化的二维图块序列 ，其中：\n\n(H, W) 是原始图像的分辨率\nC 是通道数\n(P, P) 是每个图像图块的分辨率\n 是最终得到的图块数量， 也作为 Transformer 的有效输入序列长度。\n\n\nTransformer 在其所有层中使用恒定的潜在向量大小 D，因此论文将图块扁平化，并通过一个可训练的线性投影映射到 D 维。这个投影的输出称为图块嵌入 (Patch Embeddings)。具体的分块实现可以使用卷积来实现，例如对于 224x224x3 的图像，可使用卷积核大小为 16x16、步长为 16，卷积核数量为 768，将原图像输出为 14x14x768，再将前两个维度展平，得到了最终的 196x768 的张量。\n\n可学习的分类嵌入 (Class Token)原论文在嵌入图块序列的前面添加一个类似于BERT的可学习的嵌入 [CLS] Token，用于对图像进行分类。在进行物体分类任务时，如果不添加Class token，直接把 196 x 768 维的张量输入到编码器（Encode）中，编码器输出的同样是 196 x 768 维的张量，也就是196个 1 x 768 维的向量。但此时面临一个难题：难以确定该选取哪个向量作为最终的输出向量来进行物体分类。为了解决上述问题，在将数据输入编码器之前，添加一个 1 x 768 维的向量，也就是Class token。这个向量会被放置在 196 x 768 维向量的前面。这样一来，编码器输出的向量维度就变成了 197 x 768。之后，只需通过切片操作获取第一个 1 x 768 维的向量，再把它送入分类头进行分类即可。\n融合位置编码 (Positional Encoding)若不添加类似Transformer中的位置编码，那么ViT对于不同顺序的图块会得到相同的结果，这是违反直觉的。Transformer中使用的是正弦位置编码，ViT原始论文中使用的是一维可学习的位置嵌入，因为论文通过实验没有观察到使用二维感知位置嵌入会带来显著的性能提升。在得到经过操作后的 197 x 768 维的张量（由 1 x 768 维的Class token和 196 x 768 维的张量x拼接而成）后，会加上一个维度同样为 197 x 768 的位置编码向量 position Embedding。由于二者维度相同，所以可以直接进行逐元素相加操作，这样就将位置信息融入到了嵌入向量中。最终的输入数据为：\nTransformer EncoderViT的 Encoder 使用的就是 Transformer Encoder 的结构，经过L个encoder结构后，输入维度没有发生变换，仍为 197*768 维。Transformer 的 Encoder 接收输入序列后，先通过词嵌入和位置编码融合语义与位置信息，随后经过多层处理。每层先通过多头自注意力机制计算词与词之间的关联权重，动态聚合上下文信息，再经残差连接和层归一化稳定训练；接着通过前馈神经网络非线性变换特征，同样伴随残差与归一化。最终输出富含上下文信息的序列表示，核心在于自注意力的全局交互和层堆叠的渐进特征抽象。\nMLP Head (分类头)经过encoder结构后，输出的维度为 197*768，此时我们会通过切片的方式提取出Class token的信息，其维度为 1*768。接着会拿这个 1*768 维的Class token经过MLP Head层。ViT中的MLP Head结构非常简洁，它的设计目标是作为一个简单的线性分类器，将从 [CLS] Token中提取到的高度浓缩的图像特征映射到最终的分类结果上。在最常见的实现中，MLP Head仅仅由一个线性层构成。输入维度等于Transformer模型内部的隐藏维度，输出维度等于任务所需的类别总数。在某些论文或实现中（特别是在预训练阶段），这个MLP Head可能会稍微复杂一点，比如包含一个 tanh 激活函数和一个线性层，即 Linear(tanh(Input))。但在将预训练好的模型应用于下游任务时，通常会丢弃预训练的MLP Head，换上一个全新的、符合新任务类别数量的单线性层。ViT整体结构如下图所示：\nIV. ViT代码复现1. Patch Embedding1234567891011121314151617181920212223242526272829303132import torchimport torch.nn as nnfrom functools import partialfrom collections import OrderedDictclass PatchEmbed(nn.Module):    \"\"\"    将图像分割成块 (patch) 并进行线性嵌入    \"\"\"    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):        # img_size 输入图片大小 | patch_size 图片分块大小 | in_c 输入通道数 | embed_dim 嵌入后的维度        super().__init__()        img_size = (img_size, img_size)        patch_size = (patch_size, patch_size)        self.img_size = img_size        self.patch_size = patch_size        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])        self.num_patches = self.grid_size[0] * self.grid_size[1]        # 使用二维卷积实现分块和嵌入 (B,3,224,224) -&gt; (B,768,14,14)        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()    def forward(self, x):        B, C, H, W = x.shape        assert H == self.img_size[0] and W == self.img_size[1], \\            f\"输入图像大小{H}*{W}与模型期望大小{self.img_size[0]}*{self.img_size[1]}不匹配\"                # (B,768,14,14) --flatten--&gt; (B,768,196) --transpose--&gt; (B,196,768)        x = self.proj(x).flatten(2).transpose(1, 2)        x = self.norm(x)        return x\n\n2. multi-head123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Attention(nn.Module):    def __init__(self,                 dim,  # 输入的token维度,768                 num_heads = 8, # 注意力头数,为8                 qkv_bias=False, # 生成QKV的时候是否添加偏置                 qk_scale=None, # 用于缩放QK的系数,如果None,则使用1/sqrt(head_dim)                 atte_drop_ration=0., # 注意力分数的dropout比率                 proj_drop_ration=0.): # 最终投影层的dropout比率        super().__init__()        self.num_heads = num_heads # 注意力头数        head_dim = dim // num_heads  # 每个注意力头数的维度        self.scale = qk_scale or head_dim ** -0.5  #qk的缩放因子        self.qkv = nn.Linear(dim,dim*3,bisa=qkv_bias) # 通过全连接层生成QKV,为了并行计算,提高计算效率,参数更少        \"\"\"\"        这是实现多头注意力的一个巧妙且高效的方式。        它用一个全连接层，一次性地将输入 x (维度为 dim) 映射到一个维度为 dim * 3 的张量。        这个 dim * 3 的张量可以被看作是 Q, K, V 三个部分横向拼接在一起的结果，        后续我们只需要对这个大张量进行切分即可。        这样做比定义三个独立的线性层(一个给Q,一个给K,一个给V)在计算上更高效。        \"\"\"        self.atte_drop = nn.Dropout(atte_drop_ration)        self.proj_drop = nn.Dropout(proj_drop_ration)        self.proj = nn.Linear(dim,dim) # 将每个head得到的输出进行concat拼接,然后通过线性变换映射为原本的嵌入dim        def forward(self,x):        B,N,C = x.shape  # 批大小, 图块数+1(这个1为class_token), 通道数        # reshape: B,N,3*C -&gt; B,N,3,num_head,C//num_head        # permute: B,N,3,num_head,C//num_head  -&gt;  3,B,num_heads,N,C//self.num_heads        #这样一来，Q, K, V就被分开了，并且每个头的计算所需的数据（N 和 head_dim）都排列在一起，非常适合进行批处理矩阵运算        qkv = self.qkv(x).reshape(B,N,3,self.num_head,C//self.num_head).permute(2,0,3,1,4)        # 用切片拿到q,k,v. 形状都是: [B, num_heads, N, head_dim]        q,k,v = qkv[0],qkv[1],qkv[2]        # transpose: (B,num_heads,N,C//self.num_heads)  -&gt;  (B,num_heads,C//self.num_heads,N)        # 这是一个批处理矩阵乘法。对于 B 个样本中的每一个和 num_heads 个头中的每一个，        # 我们都计算一个 [N, head_dim] 的 q 矩阵和一个 [head_dim, N] 的 k 转置矩阵的乘积        # 结果是一个 [N, N] 的矩阵，这个矩阵的第 (i, j) 个元素表示序列中第 i 个 token 对第 j 个 token 的注意力分数        attn = (q @ k.transpose(-2,-1))*self.scale  #形状为[B,num_heads,N,N]         attn = attn.softmax(dim=-1) # 对每个头的注意力分数矩阵的 每一行 进行归一化，使其和为1        attn = self.atte_drop(attn) # 应用dropout        # 注意力权重对V进行加权求和        # attn @ V: B,num_heads,N,C//self.num_heads        # transpose(1,2): B,N,num_heads,C//self.num_heads        # reshape(B,N,C): 将最后两个维度信息拼接,合并多个头输出,回到总的嵌入维度        x = (attn @ v).transpose(1,2).reshape(B,N,C)        # 将拼接好的多头输出通过最后一个线性层 self.proj。这一步允许模型学习如何最好地融合来自不同头的信息        x = self.proj(x)        x = self.proj_drop(x) # 应用最后的 dropout，防止过拟合        return x\n\n3. Block1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class MLP(nn.Module):    def __init__(self,in_features,hidden_features=None,out_features=None,act_layer=nn.GELU,drop=0.):        # in_features输入的维度  hidden_features隐藏层的维度,通常为in_features的4倍  out_features输出的维度,通常与in_features相同        super().__init__()        out_features = out_features or in_features        hidden_features = hidden_features or in_features        self.fc1 = nn.Linear(in_features,hidden_features)   # 第一个全连接层        self.act = act_layer()  # 激活层,默认GELU函数        self.fc2 = nn.Linear(hidden_features,out_features)  # 第二个全连接层        self.drop = nn.Dropout(drop)   # dropout层    def forward(self,x):        x = self.fc1(x)        x = self.act(x)        x = self.drop(x)        x = self.fc2(x)        x = self.drop(x)        return x                def drop_path(x, drop_prob:float = 0., training: bool = False):    \"\"\"    实现DropPath的核心功能。    以 drop_prob 的概率将输入的整个张量 x 置零。    这是 Stochastic Depth 网络中的主要正则化方法。    \"\"\"    # 如果丢弃概率为0，或者当前不是训练模式，则直接返回原始输入x    # 在评估或推理时，我们不希望随机丢弃任何路径    if drop_prob == 0. or not training:        return x    keep_prob = 1-drop_prob   # 计算需要保留的路径的概率 (keep_prob)    # 创建一个生成随即掩码的形状元组，shape会是 (batch_size, 1, 1, ...)，1的数量取决于x的维度    shape = (x.shape[0],) + (1,)*(x.ndim - 1)    # 生成一个随机张量。torch.rand生成[0, 1)之间的均匀分布随机数。    # 加上keep_prob后，random_tensor的范围变为 [keep_prob, 1 + keep_prob)    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.devide)    # floor_(): 对随机张量进行向下取整    # 结果是：原先在 [keep_prob, 1) 区间的数值变为0，原先在 [1, 1 + keep_prob) 区间的数值变为1    # 一个数落在 [1, 1+keep_prob) 的概率恰好是 keep_prob    # 这样，random_tensor就变成了一个二值掩码（0或1）    random_tensor.floor_()    # 将输入x除以keep_prob，然后乘以二值掩码    # 乘以random_tensor：将一部分样本的整个张量置零（实现DropPath）。    # 除以keep_prob：这是一种被称为\"Inverted Dropout\"的技术。通过在训练时放大保留下来的输出，    # 可以保证在推理时（此时keep_prob为1，不做任何操作）网络的期望输出与训练时保持一致，无需在推理阶段进行额外的缩放    output = x.div(keep_prob)*random_tensor    return outputclass DropPath(nn.Module):    def __init__(self, drop_prob=None):        super(DropPath,self).__init__()        self.drop_prob = drop_prob    def forward(self,x):        return drop_path(x,self.drop_prob,self.training)                 class Block(nn.Module):    def __init__(self,                 dim, # 每个token的维度                 num_heads,  #多头自注意力的头数量                 mlp_ratio=4,  #计算hidden_features大小 为输入的四倍                 qkv_bias=False,  # qkv偏置                 qk_scale = None,  # 注意力缩放因子                 drop_ratio=0.,  # 多头自注意力机制的最后dropout比例                 attn_drop_ratio=0.,  # 生成qkv之后的dropout比例                 drop_path_ratio=0., # drop_path比例                 act_layer=nn.GELU,  # 激活函数                 norm_layer=nn.LayerNorm  # 正则化层                 ):        super(Block,self).__init__()        self.norm1 = norm_layer(dim) # transformer encoder 中的第一个norm层        self.attn = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias,qk_scale=qk_scale,                              attn_drop_ratio=attn_drop_ratio,proj_drop_ration=drop_ratio)        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio else nn.Identity()        self.norm2 = norm_layer(dim)  # 定义第二个layer_norm层        mlp_hidden_dim = int(dim*mlp_ratio)        # 定义mlp层        self.mlp = MLP(in_features=dim,hidden_features=mlp_hidden_dim,act_layer=act_layer,drop=drop_ratio)    def forward(self,x):        x = x + self.drop_path(self.attn(self.norm1(x))) # 前向传播部分，输入的x先经过layernorm再经过多头注意力        x = x + self.drop_path(self.mlp(self.norm2(x)))  # 将得到的x一次通过layernorm、mlp、drop_path        \n\n4. Vision Transformer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180def _init_vit_weights(m):    # 1. 如果是全连接层    if isinstance(m,nn.Linear):        # 使用截断正态分布进行初始化，这是原始ViT论文推荐的方法        # 截断正态分布可以防止权重值离均值太远，让初始状态更稳定        # std=0.02 是一个经验值。带下划线的方法表示这是个in-place（原地）操作        nn.init.trunc_normal_(m.weight,std=0.01)        # 如果该线性层有偏置(bias)项        if m.bias is not None:            # 将偏置初始化为0            nn.init.zeros_(m.bias)    # 2. 如果是2D卷积层    elif isinstance(m,nn.Conv2d):        # 使用Kaiming正态分布初始化。这是一种非常适合带有ReLU激活函数的卷积层的初始化方法        # 它可以有效防止梯度消失或爆炸        # mode=\"fan_out\" 表示根据输出通道数来调整方差        nn.init.kaiming_normal_(m.weight,mode=\"fan_out\")        # 如果该卷积层有偏置项        if m.bias is not None:            # 将偏置项初始化为0            nn.init.zeros_(m.bias)    # 3. 如果是层归一化层    elif isinstance(m,nn.LayerNorm):        # 将偏置初始化为0        nn.init.zeros_(m.bias)        # 将权重初始化为1        nn.init.ones_(m.weight)        # 这样做的目的是，当训练刚开始时，LayerNorm层几乎等同于一个恒等映射，不会改变输入的分布，让训练更稳定class VisionTransformer(nn.Module):    def __init__(self, img_size=224,patch_size=16,in_c=3,num_classes=1000,                 embed_dim=768,depth=12,num_head=12,mlp_ratio=4.0,qkv_bias=True,                 qk_scale=None,representation_size=None,distilled=False,drop_ratio=0.,                 attn_drop_ratio=0.,drop_path_ratio=0.,embed_layer=PatchEmbed,norm_layer=None,                 act_layer=None):        \"\"\"        ViT模型构造函数。        Args:            img_size (int): 输入图像的尺寸            patch_size (int): 每个图像块(patch)的尺寸            in_c (int): 输入图像的通道数            num_classes (int): 最终分类的类别数            embed_dim (int): Token的嵌入维度 (D)            depth (int): Transformer Encoder的总层数            num_head (int): 多头注意力机制中的头数            mlp_ratio (float): Transformer Encoder中MLP层的维度扩展比例            qkv_bias (bool): 是否在Q,K,V生成时使用偏置            qk_scale (float, optional): QK缩放因子，默认为 1/sqrt(head_dim)            representation_size (int, optional): 在最终分类头之前，可选的中间全连接层维度            distilled (bool): 是否使用蒸馏模式 (DeiT模型)            drop_ratio (float): 全局Dropout比率            attn_drop_ratio (float): 注意力权重Dropout比率            drop_path_ratio (float): 随机深度的DropPath比率            embed_layer (nn.Module): 用于生成Patch Embedding的层            norm_layer (nn.Module, optional): 使用的归一化层            act_layer (nn.Module, optional): 使用的激活函数层        \"\"\"        # 调用父类nn.Module的初始化方法        super(VisionTransformer,self).__init__()        self.num_classes = num_classes # 保存分类数        self.num_features = self.embed_dim = embed_dim # 保存嵌入维度        # 判断是否使用蒸馏。如果使用，会多一个distillation token，总共2个特殊token；否则只有cls_token，1个        self.num_tokens = 2 if distilled else 1        # 如果未指定归一化层，则默认使用LayerNorm，并设置eps以增加数值稳定性。        # partial用于创建一个预设了部分参数的新函数        norm_layer = norm_layer or partial(nn.LayerNorm,eps=1e-6)        \"\"\"        partial 来自 Python 内置的 functools 模块，它的作用是将一个函数的某些参数“冻结”住，从而创建一个新的、更简单的函数        此处的意义是将所有nn.LayerNorm的eps固定为1e-6        \"\"\"        act_layer = act_layer or nn.GELU() # 如果未指定激活函数，则默认使用GELU        # 1. Patch Embedding层：将输入的图片(B, C, H, W)转换为一系列token(B, N, D)        self.patch_embed = embed_layer(img_size=img_size,patch_size=patch_size,in_c=in_c,embed_dim=embed_dim)        # 获取patch的数量        num_patches = self.patch_embed.num_patches          # 2. 定义可学习的 [CLS] token。这个token最终的输出将代表整个图像的特征用于分类        # 初始形状为(1, 1, embed_dim)，1个token，维度为embed_dim        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))        # 3. 如果是蒸馏模式，定义可学习的 [DIST] token        self.dist_token = nn.Parameter(torch.zeros(1,1,embed_dim)) if distilled else None        # 4. 定义可学习的位置编码(Positional Embedding)。因为Transformer本身不感知顺序，需要它来提供位置信息        # 长度为 patch数量 + 特殊token数量        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches+self.num_tokens,embed_dim))        # 5. 在位置编码加入后，应用一个Dropout层        self.pos_drop = nn.Dropout(p = drop_ratio)        # 6. 构建随机深度(Stochastic Depth)的衰减率序列        # torch.linspace生成一个从0到drop_path_ratio的等差序列，长度为depth        # 这样，越深的Block，其drop_path_ratio越大，被\"丢弃\"的概率也越高        dpr = [x.item() for x in torch.linspace(0,drop_path_ratio,depth)]        # 7. 构建Transformer Encoder主体，由连续的Block堆叠而成        # 使用nn.Sequential将多个Block串联起来        self.block = nn.Sequential(*[            Block(dim=embed_dim,num_heads=num_head,mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,qk_scale=qk_scale,                  drop_ratio = drop_ratio,attn_drop_ratio=attn_drop_ratio,drop_path_ratio=dpr[i],                  norm_layer=norm_layer,act_layer=act_layer)            for i in range(depth)        ])        # 8. 经过所有Transformer Block后的最后一个LayerNorm层。        self.norm = norm_layer(embed_dim)        # 9. 定义分类头之前的一个可选的\"pre-logits\"层 (常用于从JFT等大数据集预训练后迁移学习)        if representation_size and not distilled:            self.has_logits = True            # 更新最终输出的特征维度            self.num_features = representation_size            # pre_logits是一个包含全连接层和Tanh激活的序列            self.pre_logits = nn.Sequential(OrderedDict([                (\"fc\",nn.Linear(embed_dim,representation_size)),                (\"act\",nn.Tanh())            ]))        else:            self.has_logits=False            # 如果不使用，则用一个恒等映射层代替            self.pre_logits=nn.Identity()        # 10. 定义最终的分类头 (Head)        # 将提取的特征映射到最终的分类数        self.head = nn.Linear(self.num_features,num_classes) if num_classes&gt;0 else nn.Identity()        # 11. 如果是蒸馏模式，为distillation token也定义一个分类头        self.head_dist = None        if distilled:            self.head_dist = nn.Linear(self.embed_dim,self.num_classes) if num_classes&gt;0 else nn.Identity()                # 12. 权重初始化        # 对位置编码、dist_token、cls_token进行截断正态分布初始化        nn.init.trunc_normal_(self.pos_embed,std=0.02)        if self.dist_token is not None:            nn.init.trunc_normal_(self.dist_token,std=0.02)        nn.init.trunc_normal_(self.cls_token,std=0.02)        # 使用self.apply()方法，将_init_vit_weights函数递归地应用到模型的所有子模块上        self.apply(_init_vit_weights)    def forward_features(self,x):        \"\"\"提取特征的前向传播过程，不包括最后的分类头。\"\"\"        # x 初始形状: [B, C, H, W]        # 1. Patch Embedding: [B, C, H, W] -&gt; [B, num_patches, embed_dim]        x = self.patch_embed(x)        # 2. 将cls_token在batch维度上进行扩展，以匹配输入x的batch_size        # expand()是一个高效的操作，它不会实际复制数据        cls_token = self.cls_token.expand(x.shape[0],-1,-1) # [1, 1, D] -&gt; [B, 1, D]        # 3. 将特殊token与patch token拼接在一起        if self.dist_token is None:            # 若没有蒸馏token，拼接: [B, 1, D] 和 [B, N, D] -&gt; [B, N+1, D]            x = torch.cat((cls_token,x),dim=1)        else:            # 蒸馏模式下，拼接cls_token和dist_token            x = torch.cat((cls_token,self.dist_token.expand(x.shape[0],-1,-1),x),dim=1)        # 4. 加上位置编码，然后应用dropout        # pos_embed的[B]维度会自动广播以匹配x        x = self.pos_drop(x+self.pos_embed)        # 5. 通过Transformer Encoder主干网络        x = self.block(x)        # 6. 通过最后的LayerNorm        x = self.norm(x)        # 7. 提取用于分类的token的输出        if self.dist_token is None:            # 只返回cls_token的输出 (在序列的第0个位置)，并通过pre_logits层            return self.pre_logits(x[:,0])        else:            # 返回cls_token和dist_token的输出            return x[:,0],x[:,1]            def forward(self,x):        \"\"\"完整的从输入到输出的前向传播过程。\"\"\"        # 1. 首先通过forward_features提取特征        x = self.forward_features(x)        # 2. 通过最后的分类头得到logits        if self.head_dist is not None:            # 蒸馏模式下，两个token分别通过各自的分类头            # x此时是一个元组 (cls_output, dist_output)            x_cls,x_dist = self.head(x[0]),self.head_dist(x[1])            if self.training and not torch.jit.is_scripting():                # 在训练并且不是在 TorchScript 编译模式下，返回两个头的输出，以便分别计算损失                return x_cls,x_dist            else:                # 在评估时，返回两个头输出的平均值作为最终预测                return (x_cls + x_dist) / 2        else:            # 标准模式下，直接将特征通过分类头            x = self.head(x)        return x        \n\n\n\n\n\n\n\n\n\n\n蒸馏模式与 dist_token代码里的“蒸馏模式”来源于一篇非常重要的论文 DeiT (Data-efficient Image Transformers)。(1)什么是知识蒸馏 (Knowledge Distillation)？这是一种模型压缩和迁移学习的技术，核心思想是让一个强大而复杂的“教师模型”来指导一个轻量级的“学生模型”进行学习。\n\n教师模型: 通常是一个已经在大规模数据集上训练好的、性能非常强的模型（比如一个超大的 ResNet 或者另一个 ViT）。\n学生模型: 我们当前正在训练的模型（比如这个 VisionTransformer）。指导的方式不仅仅是让学生模型学习正确的标签（比如图片是“猫”），还会让它学习教师模型输出的“软标签”。软标签是指教师模型对所有类别的预测概率分布，例如它可能认为图片是“猫”的概率是85%，是“狗”的概率是10%，是“老虎”的… 这个概率分布包含了教师模型“思考过程”的丰富信息。\n\n(2)为什么 ViT 需要蒸馏？原始的 ViT 需要在海量的数据集（如谷歌内部的 JFT-300M，包含3亿张图片）上预训练才能获得优异的性能。如果只在 ImageNet-1k（约120万张图片）这种“中等”规模的数据集上从头训练，效果往往不如经典的 CNN 模型。DeiT 论文发现，通过知识蒸馏，可以让一个 ViT 在仅使用 ImageNet-1k 的情况下，达到甚至超过在 JFT-300M 上预训练的效果，极大地提高了 ViT 的数据效率。\n(3)dist_token 的作用DeiT 论文提出了一种新颖的蒸馏方式，就是通过添加一个专门用于蒸馏的 distillation token（即 dist_token）。\n\ncls_token 的任务: 和原来一样，它的最终输出用来和真实的标签 (ground-truth label) 计算损失，我们称之为“硬标签损失”。\ndist_token 的任务: 它是一个和 cls_token 地位相同的可学习向量，也被拼接到序列中，通过 Transformer 网络。但它的最终输出是专门用来和教师模型的预测（软标签或硬标签） 计算损失的，我们称之为“蒸馏损失”。\n\n通过这种方式，模型在训练时会同时优化两个目标：\n\n让 cls_token 的输出尽可能接近真实答案。\n让 dist_token 的输出尽可能模仿教师模型的答案。这种双重监督机制被证明非常有效，dist_token 就像一个专门负责从教师那里“偷师学艺”的通道，帮助学生模型学得更好。\n\n\n\n\n\n\n\n\n\n\npre-logits 层的作用pre-logits 层，可以理解为一个在最终分类头（self.head）之前的一个特征处理/映射层。它的主要作用是为了更好地进行迁移学习。想象一个场景：\n\n一个机构（比如谷歌）在一个超级庞大的私有数据集（比如 JFT-300M，有18000个类别）上预训练了一个 ViT 模型。\n这个预训练模型的最终分类头是 nn.Linear(embed_dim, 18000)。\n现在你想把这个模型用到你自己的任务上，比如一个只有10个类别的猫狗分类任务。\n\n显然，那个输出18000个维度的分类头对你来说是没用的。但是，它之前的网络层学到的特征提取能力是非常宝贵的。在这种情况下，pre-logits 层就派上用场了：\n\n它通常是一个 nn.Linear(embed_dim, representation_size) 加上一个激活函数（如 Tanh）。\n在预训练时，模型会先将 cls_token 的输出通过这个 pre-logits 层，得到一个固定维度的“特征表示” (representation)，然后再将这个表示送入最终的分类头。\n当你拿到这个预训练模型进行迁移学习时，你可以丢弃掉原有的最终分类头，但保留 pre-logits 层。然后，你只需要在 pre-logits 层的输出后面接上你自己任务的分类头，例如 nn.Linear(representation_size, 10)。\n\n5. 创建应用层123456789def vit_base_patch16_224(num_classes:int =100, pretrained=False):    model = VisionTransformer(img_size=224,                              patch_size=16,                              embed_dim=768,                              depth=12,                              num_head=12,                              representation_size=None,                              num_classes=num_classes)    return model\n\n参考笔记：ViT讲解\n","slug":"ViT 论文精读","date":"2025-07-02T03:32:00.000Z","categories_index":"","tags_index":"Vision-Transformer,深度学习","author_index":"犬夜叉"}]