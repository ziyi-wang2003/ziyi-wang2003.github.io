[{"id":"290cb712ccbf87b18cb10d019f08a741","title":"ViT 论文精读","content":"参考笔记：ViT讲解\nVision Transformer的出现标志着 Transformer 架构成功应用于计算机视觉领域，挑战了卷积神经网络在该领域的主导地位。ViT 通过将图像分割成小块 (patches)，并将这些图像块视为序列输入到标准的 Transformer 编码器中，从而实现了对图像的有效处理。这一进展不仅在图像分类等任务上取得了最先进的成果，更重要的是，它为视觉和语言（以及其他模态）提供了一种通用的架构语言——Transformer。这种架构上的统一极大地促进了后续多模态模型（如CLIP、LLaVA等，它们通常采用ViT或其变体作为视觉编码器）的设计和发展，使得不同模态的基于token的表示可以在相似的计算框架内进行交互和融合。 \n\n\n从数据图中可以看出，在较小的数据集上，Vision Transformer比计算成本相当的ResNets更容易过拟合。例如，ViT-B/32比ResNet50稍快；它在9M子集上的表现要差得多，但在90M+子集上表现更好。ResNet152x2和ViT-L/16也是如此。这个结果强化了这样一种直觉：卷积的归纳偏置对于较小的数据集是有用的，但对于更大的数据集，直接从数据中学习相关模式是足够的，甚至是有益的。\nI. 摘要尽管 Transformer 架构已成为自然语言处理（NLP）任务事实上的标准，但其在计算机视觉领域的应用仍然有限。在视觉领域，注意力机制要么与卷积网络（CNN）结合使用，要么用于替代卷积网络中的某些组件，但整体结构仍然保留。我们证明了这种对 CNN 的依赖并非必要，一个直接应用于图像块序列的纯 Transformer 模型可以在图像分类任务上表现得非常好。 当在大量数据上进行预训练，并迁移到多个中等或小型图像识别基准测试（如 ImageNet, CIFAR-100, VTAB 等）时，Vision Transformer (ViT) 相比于最先进的卷积网络取得了优异的结果，同时训练所需的计算资源也大幅减少。\nII. 创新点范式革新：将图像视为序列处理论文首次证明了一个纯粹的、标准的 Transformer 模型可以直接用于图像分类，而无需依赖卷积神经网络。传统视觉任务长期由 CNN 主导，这篇论文打破了这一惯例。它通过将图像分割成固定大小的图块，并将这些图块的线性嵌入序列作为 Transformer 的输入，成功地将 NLP 领域的成功范式迁移到了视觉领域。\n数据量胜于归纳偏置论文发现，当在超大规模数据集（如 JFT-300M，包含3亿张图片）上进行预训练时，Vision Transformer (ViT) 的性能超越了当前最先进的卷积网络。这揭示了一个重要现象：CNN 中固有的（如局部性、平移不变性）的归纳偏置在数据量较小时非常有效，但当数据量足够大时，模型可以从数据中直接学习到这些空间关系，强大的模型容量和更少的先验限制反而成为优势。\n卓越的计算效率和可扩展性与性能相当的 SOTA 卷积网络相比，ViT 在达到同等甚至更高精度时，所需的预训练计算资源要少得多。例如，ViT-L/16 在 JFT-300M 数据集上预训练后，其性能优于在同一数据集上训练的 BiT-L (一个大型 ResNet 模型)，而训练成本却显著降低。这证明了 Transformer 架构在可扩展性上的巨大潜力。\n简洁而有效的模型设计论文的设计理念是尽可能少地修改原始的 Transformer 架构，使其可以直接利用 NLP 领域成熟的高效实现和可扩展架构。这种简洁性不仅体现在模型结构上，也体现在对图像的处理上，除了初始的图块划分和用于适应不同分辨率的位置编码插值外，几乎没有引入图像特有的归纳偏置。\nIII. 网络原理详解ViT模型概览ViT模型的核心思想是将图像转换为一个序列，然后用标准的Transformer Encoder来处理这个序列。\n图像分块处理 (Image Patching)Transformer接受的输入数据格式是一维的词嵌入序列，为了处理二维图像数据，论文将图像  重塑为一个扁平化的二维图块序列 ，其中：\n\n(H, W) 是原始图像的分辨率\nC 是通道数\n(P, P) 是每个图像图块的分辨率\n 是最终得到的图块数量， 也作为 Transformer 的有效输入序列长度。\n\n\nTransformer 在其所有层中使用恒定的潜在向量大小 D，因此论文将图块扁平化，并通过一个可训练的线性投影映射到 D 维。这个投影的输出称为图块嵌入 (Patch Embeddings)。具体的分块实现可以使用卷积来实现，例如对于 224x224x3 的图像，可使用卷积核大小为 16x16、步长为 16，卷积核数量为 768，将原图像输出为 14x14x768，再将前两个维度展平，得到了最终的 196x768 的张量。\n\n可学习的分类嵌入 (Class Token)原论文在嵌入图块序列的前面添加一个类似于BERT的可学习的嵌入 [CLS] Token，用于对图像进行分类。在进行物体分类任务时，如果不添加Class token，直接把 196 x 768 维的张量输入到编码器（Encode）中，编码器输出的同样是 196 x 768 维的张量，也就是196个 1 x 768 维的向量。但此时面临一个难题：难以确定该选取哪个向量作为最终的输出向量来进行物体分类。为了解决上述问题，在将数据输入编码器之前，添加一个 1 x 768 维的向量，也就是Class token。这个向量会被放置在 196 x 768 维向量的前面。这样一来，编码器输出的向量维度就变成了 197 x 768。之后，只需通过切片操作获取第一个 1 x 768 维的向量，再把它送入分类头进行分类即可。\n融合位置编码 (Positional Encoding)若不添加类似Transformer中的位置编码，那么ViT对于不同顺序的图块会得到相同的结果，这是违反直觉的。Transformer中使用的是正弦位置编码，ViT原始论文中使用的是一维可学习的位置嵌入，因为论文通过实验没有观察到使用二维感知位置嵌入会带来显著的性能提升。在得到经过操作后的 197 x 768 维的张量（由 1 x 768 维的Class token和 196 x 768 维的张量x拼接而成）后，会加上一个维度同样为 197 x 768 的位置编码向量 position Embedding。由于二者维度相同，所以可以直接进行逐元素相加操作，这样就将位置信息融入到了嵌入向量中。最终的输入数据为：\nTransformer EncoderViT的 Encoder 使用的就是 Transformer Encoder 的结构，经过L个encoder结构后，输入维度没有发生变换，仍为 197*768 维。Transformer 的 Encoder 接收输入序列后，先通过词嵌入和位置编码融合语义与位置信息，随后经过多层处理。每层先通过多头自注意力机制计算词与词之间的关联权重，动态聚合上下文信息，再经残差连接和层归一化稳定训练；接着通过前馈神经网络非线性变换特征，同样伴随残差与归一化。最终输出富含上下文信息的序列表示，核心在于自注意力的全局交互和层堆叠的渐进特征抽象。\nMLP Head (分类头)经过encoder结构后，输出的维度为 197*768，此时我们会通过切片的方式提取出Class token的信息，其维度为 1*768。接着会拿这个 1*768 维的Class token经过MLP Head层。ViT中的MLP Head结构非常简洁，它的设计目标是作为一个简单的线性分类器，将从 [CLS] Token中提取到的高度浓缩的图像特征映射到最终的分类结果上。在最常见的实现中，MLP Head仅仅由一个线性层构成。输入维度等于Transformer模型内部的隐藏维度，输出维度等于任务所需的类别总数。在某些论文或实现中（特别是在预训练阶段），这个MLP Head可能会稍微复杂一点，比如包含一个 tanh 激活函数和一个线性层，即 Linear(tanh(Input))。但在将预训练好的模型应用于下游任务时，通常会丢弃预训练的MLP Head，换上一个全新的、符合新任务类别数量的单线性层。ViT整体结构如下图所示：\nIV. ViT代码复现1. Patch Embedding1234567891011121314151617181920212223242526272829303132import torchimport torch.nn as nnfrom functools import partialfrom collections import OrderedDictclass PatchEmbed(nn.Module):    \"\"\"    将图像分割成块 (patch) 并进行线性嵌入    \"\"\"    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):        # img_size 输入图片大小 | patch_size 图片分块大小 | in_c 输入通道数 | embed_dim 嵌入后的维度        super().__init__()        img_size = (img_size, img_size)        patch_size = (patch_size, patch_size)        self.img_size = img_size        self.patch_size = patch_size        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])        self.num_patches = self.grid_size[0] * self.grid_size[1]        # 使用二维卷积实现分块和嵌入 (B,3,224,224) -&gt; (B,768,14,14)        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()    def forward(self, x):        B, C, H, W = x.shape        assert H == self.img_size[0] and W == self.img_size[1], \\            f\"输入图像大小{H}*{W}与模型期望大小{self.img_size[0]}*{self.img_size[1]}不匹配\"                # (B,768,14,14) --flatten--&gt; (B,768,196) --transpose--&gt; (B,196,768)        x = self.proj(x).flatten(2).transpose(1, 2)        x = self.norm(x)        return x\n\n2. multi-head123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Attention(nn.Module):    def __init__(self,                 dim,  # 输入的token维度,768                 num_heads = 8, # 注意力头数,为8                 qkv_bias=False, # 生成QKV的时候是否添加偏置                 qk_scale=None, # 用于缩放QK的系数,如果None,则使用1/sqrt(head_dim)                 atte_drop_ration=0., # 注意力分数的dropout比率                 proj_drop_ration=0.): # 最终投影层的dropout比率        super().__init__()        self.num_heads = num_heads # 注意力头数        head_dim = dim // num_heads  # 每个注意力头数的维度        self.scale = qk_scale or head_dim ** -0.5  #qk的缩放因子        self.qkv = nn.Linear(dim,dim*3,bisa=qkv_bias) # 通过全连接层生成QKV,为了并行计算,提高计算效率,参数更少        \"\"\"\"        这是实现多头注意力的一个巧妙且高效的方式。        它用一个全连接层，一次性地将输入 x (维度为 dim) 映射到一个维度为 dim * 3 的张量。        这个 dim * 3 的张量可以被看作是 Q, K, V 三个部分横向拼接在一起的结果，        后续我们只需要对这个大张量进行切分即可。        这样做比定义三个独立的线性层(一个给Q,一个给K,一个给V)在计算上更高效。        \"\"\"        self.atte_drop = nn.Dropout(atte_drop_ration)        self.proj_drop = nn.Dropout(proj_drop_ration)        self.proj = nn.Linear(dim,dim) # 将每个head得到的输出进行concat拼接,然后通过线性变换映射为原本的嵌入dim        def forward(self,x):        B,N,C = x.shape  # 批大小, 图块数+1(这个1为class_token), 通道数        # reshape: B,N,3*C -&gt; B,N,3,num_head,C//num_head        # permute: B,N,3,num_head,C//num_head  -&gt;  3,B,num_heads,N,C//self.num_heads        #这样一来，Q, K, V就被分开了，并且每个头的计算所需的数据（N 和 head_dim）都排列在一起，非常适合进行批处理矩阵运算        qkv = self.qkv(x).reshape(B,N,3,self.num_head,C//self.num_head).permute(2,0,3,1,4)        # 用切片拿到q,k,v. 形状都是: [B, num_heads, N, head_dim]        q,k,v = qkv[0],qkv[1],qkv[2]        # transpose: (B,num_heads,N,C//self.num_heads)  -&gt;  (B,num_heads,C//self.num_heads,N)        # 这是一个批处理矩阵乘法。对于 B 个样本中的每一个和 num_heads 个头中的每一个，        # 我们都计算一个 [N, head_dim] 的 q 矩阵和一个 [head_dim, N] 的 k 转置矩阵的乘积        # 结果是一个 [N, N] 的矩阵，这个矩阵的第 (i, j) 个元素表示序列中第 i 个 token 对第 j 个 token 的注意力分数        attn = (q @ k.transpose(-2,-1))*self.scale  #形状为[B,num_heads,N,N]         attn = attn.softmax(dim=-1) # 对每个头的注意力分数矩阵的 每一行 进行归一化，使其和为1        attn = self.atte_drop(attn) # 应用dropout        # 注意力权重对V进行加权求和        # attn @ V: B,num_heads,N,C//self.num_heads        # transpose(1,2): B,N,num_heads,C//self.num_heads        # reshape(B,N,C): 将最后两个维度信息拼接,合并多个头输出,回到总的嵌入维度        x = (attn @ v).transpose(1,2).reshape(B,N,C)        # 将拼接好的多头输出通过最后一个线性层 self.proj。这一步允许模型学习如何最好地融合来自不同头的信息        x = self.proj(x)        x = self.proj_drop(x) # 应用最后的 dropout，防止过拟合        return x\n\n3. Block1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class MLP(nn.Module):    def __init__(self,in_features,hidden_features=None,out_features=None,act_layer=nn.GELU,drop=0.):        # in_features输入的维度  hidden_features隐藏层的维度,通常为in_features的4倍  out_features输出的维度,通常与in_features相同        super().__init__()        out_features = out_features or in_features        hidden_features = hidden_features or in_features        self.fc1 = nn.Linear(in_features,hidden_features)   # 第一个全连接层        self.act = act_layer()  # 激活层,默认GELU函数        self.fc2 = nn.Linear(hidden_features,out_features)  # 第二个全连接层        self.drop = nn.Dropout(drop)   # dropout层    def forward(self,x):        x = self.fc1(x)        x = self.act(x)        x = self.drop(x)        x = self.fc2(x)        x = self.drop(x)        return x                def drop_path(x, drop_prob:float = 0., training: bool = False):    \"\"\"    实现DropPath的核心功能。    以 drop_prob 的概率将输入的整个张量 x 置零。    这是 Stochastic Depth 网络中的主要正则化方法。    \"\"\"    # 如果丢弃概率为0，或者当前不是训练模式，则直接返回原始输入x    # 在评估或推理时，我们不希望随机丢弃任何路径    if drop_prob == 0. or not training:        return x    keep_prob = 1-drop_prob   # 计算需要保留的路径的概率 (keep_prob)    # 创建一个生成随即掩码的形状元组，shape会是 (batch_size, 1, 1, ...)，1的数量取决于x的维度    shape = (x.shape[0],) + (1,)*(x.ndim - 1)    # 生成一个随机张量。torch.rand生成[0, 1)之间的均匀分布随机数。    # 加上keep_prob后，random_tensor的范围变为 [keep_prob, 1 + keep_prob)    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.devide)    # floor_(): 对随机张量进行向下取整    # 结果是：原先在 [keep_prob, 1) 区间的数值变为0，原先在 [1, 1 + keep_prob) 区间的数值变为1    # 一个数落在 [1, 1+keep_prob) 的概率恰好是 keep_prob    # 这样，random_tensor就变成了一个二值掩码（0或1）    random_tensor.floor_()    # 将输入x除以keep_prob，然后乘以二值掩码    # 乘以random_tensor：将一部分样本的整个张量置零（实现DropPath）。    # 除以keep_prob：这是一种被称为\"Inverted Dropout\"的技术。通过在训练时放大保留下来的输出，    # 可以保证在推理时（此时keep_prob为1，不做任何操作）网络的期望输出与训练时保持一致，无需在推理阶段进行额外的缩放    output = x.div(keep_prob)*random_tensor    return outputclass DropPath(nn.Module):    def __init__(self, drop_prob=None):        super(DropPath,self).__init__()        self.drop_prob = drop_prob    def forward(self,x):        return drop_path(x,self.drop_prob,self.training)                 class Block(nn.Module):    def __init__(self,                 dim, # 每个token的维度                 num_heads,  #多头自注意力的头数量                 mlp_ratio=4,  #计算hidden_features大小 为输入的四倍                 qkv_bias=False,  # qkv偏置                 qk_scale = None,  # 注意力缩放因子                 drop_ratio=0.,  # 多头自注意力机制的最后dropout比例                 attn_drop_ratio=0.,  # 生成qkv之后的dropout比例                 drop_path_ratio=0., # drop_path比例                 act_layer=nn.GELU,  # 激活函数                 norm_layer=nn.LayerNorm  # 正则化层                 ):        super(Block,self).__init__()        self.norm1 = norm_layer(dim) # transformer encoder 中的第一个norm层        self.attn = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias,qk_scale=qk_scale,                              attn_drop_ratio=attn_drop_ratio,proj_drop_ration=drop_ratio)        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio else nn.Identity()        self.norm2 = norm_layer(dim)  # 定义第二个layer_norm层        mlp_hidden_dim = int(dim*mlp_ratio)        # 定义mlp层        self.mlp = MLP(in_features=dim,hidden_features=mlp_hidden_dim,act_layer=act_layer,drop=drop_ratio)    def forward(self,x):        x = x + self.drop_path(self.attn(self.norm1(x))) # 前向传播部分，输入的x先经过layernorm再经过多头注意力        x = x + self.drop_path(self.mlp(self.norm2(x)))  # 将得到的x一次通过layernorm、mlp、drop_path        \n\n4. Vision Transformer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180def _init_vit_weights(m):    # 1. 如果是全连接层    if isinstance(m,nn.Linear):        # 使用截断正态分布进行初始化，这是原始ViT论文推荐的方法        # 截断正态分布可以防止权重值离均值太远，让初始状态更稳定        # std=0.02 是一个经验值。带下划线的方法表示这是个in-place（原地）操作        nn.init.trunc_normal_(m.weight,std=0.01)        # 如果该线性层有偏置(bias)项        if m.bias is not None:            # 将偏置初始化为0            nn.init.zeros_(m.bias)    # 2. 如果是2D卷积层    elif isinstance(m,nn.Conv2d):        # 使用Kaiming正态分布初始化。这是一种非常适合带有ReLU激活函数的卷积层的初始化方法        # 它可以有效防止梯度消失或爆炸        # mode=\"fan_out\" 表示根据输出通道数来调整方差        nn.init.kaiming_normal_(m.weight,mode=\"fan_out\")        # 如果该卷积层有偏置项        if m.bias is not None:            # 将偏置项初始化为0            nn.init.zeros_(m.bias)    # 3. 如果是层归一化层    elif isinstance(m,nn.LayerNorm):        # 将偏置初始化为0        nn.init.zeros_(m.bias)        # 将权重初始化为1        nn.init.ones_(m.weight)        # 这样做的目的是，当训练刚开始时，LayerNorm层几乎等同于一个恒等映射，不会改变输入的分布，让训练更稳定class VisionTransformer(nn.Module):    def __init__(self, img_size=224,patch_size=16,in_c=3,num_classes=1000,                 embed_dim=768,depth=12,num_head=12,mlp_ratio=4.0,qkv_bias=True,                 qk_scale=None,representation_size=None,distilled=False,drop_ratio=0.,                 attn_drop_ratio=0.,drop_path_ratio=0.,embed_layer=PatchEmbed,norm_layer=None,                 act_layer=None):        \"\"\"        ViT模型构造函数。        Args:            img_size (int): 输入图像的尺寸            patch_size (int): 每个图像块(patch)的尺寸            in_c (int): 输入图像的通道数            num_classes (int): 最终分类的类别数            embed_dim (int): Token的嵌入维度 (D)            depth (int): Transformer Encoder的总层数            num_head (int): 多头注意力机制中的头数            mlp_ratio (float): Transformer Encoder中MLP层的维度扩展比例            qkv_bias (bool): 是否在Q,K,V生成时使用偏置            qk_scale (float, optional): QK缩放因子，默认为 1/sqrt(head_dim)            representation_size (int, optional): 在最终分类头之前，可选的中间全连接层维度            distilled (bool): 是否使用蒸馏模式 (DeiT模型)            drop_ratio (float): 全局Dropout比率            attn_drop_ratio (float): 注意力权重Dropout比率            drop_path_ratio (float): 随机深度的DropPath比率            embed_layer (nn.Module): 用于生成Patch Embedding的层            norm_layer (nn.Module, optional): 使用的归一化层            act_layer (nn.Module, optional): 使用的激活函数层        \"\"\"        # 调用父类nn.Module的初始化方法        super(VisionTransformer,self).__init__()        self.num_classes = num_classes # 保存分类数        self.num_features = self.embed_dim = embed_dim # 保存嵌入维度        # 判断是否使用蒸馏。如果使用，会多一个distillation token，总共2个特殊token；否则只有cls_token，1个        self.num_tokens = 2 if distilled else 1        # 如果未指定归一化层，则默认使用LayerNorm，并设置eps以增加数值稳定性。        # partial用于创建一个预设了部分参数的新函数        norm_layer = norm_layer or partial(nn.LayerNorm,eps=1e-6)        \"\"\"        partial 来自 Python 内置的 functools 模块，它的作用是将一个函数的某些参数“冻结”住，从而创建一个新的、更简单的函数        此处的意义是将所有nn.LayerNorm的eps固定为1e-6        \"\"\"        act_layer = act_layer or nn.GELU() # 如果未指定激活函数，则默认使用GELU        # 1. Patch Embedding层：将输入的图片(B, C, H, W)转换为一系列token(B, N, D)        self.patch_embed = embed_layer(img_size=img_size,patch_size=patch_size,in_c=in_c,embed_dim=embed_dim)        # 获取patch的数量        num_patches = self.patch_embed.num_patches          # 2. 定义可学习的 [CLS] token。这个token最终的输出将代表整个图像的特征用于分类        # 初始形状为(1, 1, embed_dim)，1个token，维度为embed_dim        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))        # 3. 如果是蒸馏模式，定义可学习的 [DIST] token        self.dist_token = nn.Parameter(torch.zeros(1,1,embed_dim)) if distilled else None        # 4. 定义可学习的位置编码(Positional Embedding)。因为Transformer本身不感知顺序，需要它来提供位置信息        # 长度为 patch数量 + 特殊token数量        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches+self.num_tokens,embed_dim))        # 5. 在位置编码加入后，应用一个Dropout层        self.pos_drop = nn.Dropout(p = drop_ratio)        # 6. 构建随机深度(Stochastic Depth)的衰减率序列        # torch.linspace生成一个从0到drop_path_ratio的等差序列，长度为depth        # 这样，越深的Block，其drop_path_ratio越大，被\"丢弃\"的概率也越高        dpr = [x.item() for x in torch.linspace(0,drop_path_ratio,depth)]        # 7. 构建Transformer Encoder主体，由连续的Block堆叠而成        # 使用nn.Sequential将多个Block串联起来        self.block = nn.Sequential(*[            Block(dim=embed_dim,num_heads=num_head,mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,qk_scale=qk_scale,                  drop_ratio = drop_ratio,attn_drop_ratio=attn_drop_ratio,drop_path_ratio=dpr[i],                  norm_layer=norm_layer,act_layer=act_layer)            for i in range(depth)        ])        # 8. 经过所有Transformer Block后的最后一个LayerNorm层。        self.norm = norm_layer(embed_dim)        # 9. 定义分类头之前的一个可选的\"pre-logits\"层 (常用于从JFT等大数据集预训练后迁移学习)        if representation_size and not distilled:            self.has_logits = True            # 更新最终输出的特征维度            self.num_features = representation_size            # pre_logits是一个包含全连接层和Tanh激活的序列            self.pre_logits = nn.Sequential(OrderedDict([                (\"fc\",nn.Linear(embed_dim,representation_size)),                (\"act\",nn.Tanh())            ]))        else:            self.has_logits=False            # 如果不使用，则用一个恒等映射层代替            self.pre_logits=nn.Identity()        # 10. 定义最终的分类头 (Head)        # 将提取的特征映射到最终的分类数        self.head = nn.Linear(self.num_features,num_classes) if num_classes&gt;0 else nn.Identity()        # 11. 如果是蒸馏模式，为distillation token也定义一个分类头        self.head_dist = None        if distilled:            self.head_dist = nn.Linear(self.embed_dim,self.num_classes) if num_classes&gt;0 else nn.Identity()                # 12. 权重初始化        # 对位置编码、dist_token、cls_token进行截断正态分布初始化        nn.init.trunc_normal_(self.pos_embed,std=0.02)        if self.dist_token is not None:            nn.init.trunc_normal_(self.dist_token,std=0.02)        nn.init.trunc_normal_(self.cls_token,std=0.02)        # 使用self.apply()方法，将_init_vit_weights函数递归地应用到模型的所有子模块上        self.apply(_init_vit_weights)    def forward_features(self,x):        \"\"\"提取特征的前向传播过程，不包括最后的分类头。\"\"\"        # x 初始形状: [B, C, H, W]        # 1. Patch Embedding: [B, C, H, W] -&gt; [B, num_patches, embed_dim]        x = self.patch_embed(x)        # 2. 将cls_token在batch维度上进行扩展，以匹配输入x的batch_size        # expand()是一个高效的操作，它不会实际复制数据        cls_token = self.cls_token.expand(x.shape[0],-1,-1) # [1, 1, D] -&gt; [B, 1, D]        # 3. 将特殊token与patch token拼接在一起        if self.dist_token is None:            # 若没有蒸馏token，拼接: [B, 1, D] 和 [B, N, D] -&gt; [B, N+1, D]            x = torch.cat((cls_token,x),dim=1)        else:            # 蒸馏模式下，拼接cls_token和dist_token            x = torch.cat((cls_token,self.dist_token.expand(x.shape[0],-1,-1),x),dim=1)        # 4. 加上位置编码，然后应用dropout        # pos_embed的[B]维度会自动广播以匹配x        x = self.pos_drop(x+self.pos_embed)        # 5. 通过Transformer Encoder主干网络        x = self.block(x)        # 6. 通过最后的LayerNorm        x = self.norm(x)        # 7. 提取用于分类的token的输出        if self.dist_token is None:            # 只返回cls_token的输出 (在序列的第0个位置)，并通过pre_logits层            return self.pre_logits(x[:,0])        else:            # 返回cls_token和dist_token的输出            return x[:,0],x[:,1]            def forward(self,x):        \"\"\"完整的从输入到输出的前向传播过程。\"\"\"        # 1. 首先通过forward_features提取特征        x = self.forward_features(x)        # 2. 通过最后的分类头得到logits        if self.head_dist is not None:            # 蒸馏模式下，两个token分别通过各自的分类头            # x此时是一个元组 (cls_output, dist_output)            x_cls,x_dist = self.head(x[0]),self.head_dist(x[1])            if self.training and not torch.jit.is_scripting():                # 在训练并且不是在 TorchScript 编译模式下，返回两个头的输出，以便分别计算损失                return x_cls,x_dist            else:                # 在评估时，返回两个头输出的平均值作为最终预测                return (x_cls + x_dist) / 2        else:            # 标准模式下，直接将特征通过分类头            x = self.head(x)        return x        \n\n\n\n\n\n\n\n\n\n\n蒸馏模式与 dist_token代码里的“蒸馏模式”来源于一篇非常重要的论文 DeiT (Data-efficient Image Transformers)。(1)什么是知识蒸馏 (Knowledge Distillation)？这是一种模型压缩和迁移学习的技术，核心思想是让一个强大而复杂的“教师模型”来指导一个轻量级的“学生模型”进行学习。\n\n教师模型: 通常是一个已经在大规模数据集上训练好的、性能非常强的模型（比如一个超大的 ResNet 或者另一个 ViT）。\n学生模型: 我们当前正在训练的模型（比如这个 VisionTransformer）。指导的方式不仅仅是让学生模型学习正确的标签（比如图片是“猫”），还会让它学习教师模型输出的“软标签”。软标签是指教师模型对所有类别的预测概率分布，例如它可能认为图片是“猫”的概率是85%，是“狗”的概率是10%，是“老虎”的… 这个概率分布包含了教师模型“思考过程”的丰富信息。\n\n(2)为什么 ViT 需要蒸馏？原始的 ViT 需要在海量的数据集（如谷歌内部的 JFT-300M，包含3亿张图片）上预训练才能获得优异的性能。如果只在 ImageNet-1k（约120万张图片）这种“中等”规模的数据集上从头训练，效果往往不如经典的 CNN 模型。DeiT 论文发现，通过知识蒸馏，可以让一个 ViT 在仅使用 ImageNet-1k 的情况下，达到甚至超过在 JFT-300M 上预训练的效果，极大地提高了 ViT 的数据效率。\n(3)dist_token 的作用DeiT 论文提出了一种新颖的蒸馏方式，就是通过添加一个专门用于蒸馏的 distillation token（即 dist_token）。\n\ncls_token 的任务: 和原来一样，它的最终输出用来和真实的标签 (ground-truth label) 计算损失，我们称之为“硬标签损失”。\ndist_token 的任务: 它是一个和 cls_token 地位相同的可学习向量，也被拼接到序列中，通过 Transformer 网络。但它的最终输出是专门用来和教师模型的预测（软标签或硬标签） 计算损失的，我们称之为“蒸馏损失”。\n\n通过这种方式，模型在训练时会同时优化两个目标：\n\n让 cls_token 的输出尽可能接近真实答案。\n让 dist_token 的输出尽可能模仿教师模型的答案。这种双重监督机制被证明非常有效，dist_token 就像一个专门负责从教师那里“偷师学艺”的通道，帮助学生模型学得更好。\n\n\n\n\n\n\n\n\n\n\npre-logits 层的作用pre-logits 层，可以理解为一个在最终分类头（self.head）之前的一个特征处理/映射层。它的主要作用是为了更好地进行迁移学习。想象一个场景：\n\n一个机构（比如谷歌）在一个超级庞大的私有数据集（比如 JFT-300M，有18000个类别）上预训练了一个 ViT 模型。\n这个预训练模型的最终分类头是 nn.Linear(embed_dim, 18000)。\n现在你想把这个模型用到你自己的任务上，比如一个只有10个类别的猫狗分类任务。\n\n显然，那个输出18000个维度的分类头对你来说是没用的。但是，它之前的网络层学到的特征提取能力是非常宝贵的。在这种情况下，pre-logits 层就派上用场了：\n\n它通常是一个 nn.Linear(embed_dim, representation_size) 加上一个激活函数（如 Tanh）。\n在预训练时，模型会先将 cls_token 的输出通过这个 pre-logits 层，得到一个固定维度的“特征表示” (representation)，然后再将这个表示送入最终的分类头。\n当你拿到这个预训练模型进行迁移学习时，你可以丢弃掉原有的最终分类头，但保留 pre-logits 层。然后，你只需要在 pre-logits 层的输出后面接上你自己任务的分类头，例如 nn.Linear(representation_size, 10)。\n\n5. 创建应用层123456789def vit_base_patch16_224(num_classes:int =100, pretrained=False):    model = VisionTransformer(img_size=224,                              patch_size=16,                              embed_dim=768,                              depth=12,                              num_head=12,                              representation_size=None,                              num_classes=num_classes)    return model","slug":"ViT精读","date":"2025-07-02T03:32:00.000Z","categories_index":"","tags_index":"Vision-Transformer,深度学习","author_index":"犬夜叉"}]