[{"id":"524ccad428f7700ee84d802ab1c86ad8","title":"PyTorch学习-3-数据处理&项目实战","content":"PyTorch学习-3-数据处理-项目实战模块 6：高效数据加载 (Dataset &amp; DataLoader)6.1 为什么需要它们？想象一下，你有一个包含 50,000 张高清图像的数据集，总大小可能达到几十甚至上百 GB。你该如何训练你的模型？\n\n一次性全部加载到内存？ 这显然是不可行的。绝大多数情况下，你的内存（RAM 或 GPU VRAM）都无法容纳整个数据集。\n手动编写循环，一张一张读取？ 这会非常慢，因为磁盘 I/O 是一个瓶颈。更重要的是，你还需要自己处理：\n预处理 (Preprocessing): 每张图片都需要被转换成张量，可能还需要裁剪、归一化等操作。\n批处理 (Batching): 现代 GPU 擅长并行计算，一次处理一批数据（例如 64 张图片）远比一张一张处理要高效得多。你需要手动将数据组合成批次。\n打乱 (Shuffling): 为了让模型训练更稳定、泛化能力更强，每个 epoch 开始前都应该打乱数据顺序，防止模型学到数据的排列顺序。\n并行加载 (Parallel Loading): 当 GPU 正在处理当前批次的数据时，CPU 应该已经在后台准备下一个批次了，以最大限度地减少 GPU 的等待时间。\n\n\n\nPyTorch 提供了 Dataset 和 DataLoader 这对组合，优雅地解决了所有这些问题。\n\nDataset：负责定义数据集的来源和单一样本的获取方式。它只关心“总共有多少数据？”和“如何获取第 i 个数据？”。\nDataLoader：负责从 Dataset 中取出数据，并以我们期望的方式（如批处理、打乱、并行加载）提供给模型。\n\n6.2 torch.utils.data.Dataset - 数据集的“蓝图”Dataset 是一个抽象类。要创建自己的数据集，你需要继承它，并重写两个核心的“魔法”方法：\n\n__len__(self): 应该返回数据集的总大小。DataLoader 需要这个信息来知道总共有多少样本。\n__getitem__(self, idx): 应该根据给定的索引 idx，返回一个数据样本。这是 Dataset 的核心，定义了如何从源（如文件、数据库）加载、预处理并返回单个数据点（通常是一个数据和标签的元组）。\n\n代码示例：创建一个自定义的简单数据集\n假设我们有一个 CSV 文件，内容是学生的“学习小时数”、“睡眠小时数”和“是否通过考试”。我们将为它创建一个 Dataset。\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\n# 假设我们有这样的数据\n# 我们用 numpy 数组模拟，实际中可能是从文件中读取\ndata = np.array([\n    [8, 7, 1],  # [study_hours, sleep_hours, passed]\n    [5, 6, 0],\n    [2, 8, 0],\n    [9, 8, 1],\n    [4, 5, 0],\n    [7, 7, 1]\n])\n\nclass StudentDataset(Dataset):\n    def __init__(self, data):\n        # 在 __init__ 中，我们通常进行一次性的设置，比如加载数据\n        # 这里我们将数据源存储为类的属性\n        self.data = data\n        # 将特征 (X) 和标签 (y) 分开\n        self.X = torch.from_numpy(self.data[:, :-1]).float()\n        self.y = torch.from_numpy(self.data[:, -1]).float()\n        self.n_samples = self.data.shape[0]\n\n    def __getitem__(self, index):\n        # 这个方法根据索引返回一个样本\n        # DataLoader 会在后台调用这个方法来获取数据\n        return self.X[index], self.y[index]\n\n    def __len__(self):\n        # 这个方法返回数据集的总长度\n        return self.n_samples\n\n# 实例化我们的数据集\nstudent_dataset = StudentDataset(data)\n\n# 测试一下\nfirst_sample_features, first_sample_label = student_dataset[0]\nprint(f\"第一个样本的特征: {first_sample_features}\")\nprint(f\"第一个样本的标签: {first_sample_label}\")\nprint(f\"数据集总长度: {len(student_dataset)}\")\n\n第一个样本的特征: tensor([8., 7.])\n第一个样本的标签: 1.0\n数据集总长度: 6\n6.3 torch.utils.data.DataLoader - 智能的“数据搬运工”DataLoader 将 Dataset 包装成一个方便的 Python 可迭代对象。你可以在一个 for 循环中轻松地遍历它，每次迭代都会得到一个批次的数据。\n核心参数:\n\ndataset: 你实例化的 Dataset 对象。\nbatch_size (int, default=1): 每个批次包含的样本数量。\nshuffle (bool, default=False): 是否在每个 epoch 开始时打乱数据。在训练时通常设置为 True，在评估时设置为 False。\nnum_workers (int, default=0): 使用多少个子进程来预加载数据。0 表示在主进程中加载数据。设置为大于 0 的整数（如 4, 8）可以极大地加速数据加载，因为它利用了多核 CPU 在后台准备数据，让 GPU 不用等待。这是提升训练效率的关键参数。\n\n代码示例：使用 DataLoader\n# 将我们创建的 Dataset 包装进 DataLoader\n# 设置批次大小为 2，并且打乱数据\ndata_loader = DataLoader(dataset=student_dataset, batch_size=2, shuffle=True, num_workers=0)\n\n# 模拟一个 epoch 的训练过程\nprint(\"\\n--- 遍历 DataLoader ---\")\nfor epoch in range(1): # 假设只训练一个 epoch\n    print(f\"Epoch {epoch+1}:\")\n    for i, (features, labels) in enumerate(data_loader):\n        # 在这里执行你的训练五步法\n        print(f\"  Batch {i+1}:\")\n        print(f\"    特征的形状: {features.shape}\") # torch.Size([2, 2]) -> [batch_size, num_features]\n        print(f\"    标签的形状: {labels.shape}\")   # torch.Size([2]) -> [batch_size]\n        # 注意标签需要调整形状以匹配模型输出\n        # view方法用于改变张量的形状，这里-1表示该维度的大小由其他维度的大小和张量元素总数自动推断得出，1表示新形状的第二个维度大小为 1 。\n        #这里是将获得的 [a,b] 转换为 [[a],[b]]\n        labels = labels.view(-1, 1)\n        print(f\"    调整后标签的形状: {labels.shape}\") # torch.Size([2, 1])\n        # ... optimizer.zero_grad(), model(features), etc.\n\n--- 遍历 DataLoader ---\nEpoch 1:\n  Batch 1:\n    特征的形状: torch.Size([2, 2])\n    标签的形状: torch.Size([2])\n    调整后标签的形状: torch.Size([2, 1])\n  Batch 2:\n    特征的形状: torch.Size([2, 2])\n    标签的形状: torch.Size([2])\n    调整后标签的形状: torch.Size([2, 1])\n  Batch 3:\n    特征的形状: torch.Size([2, 2])\n    标签的形状: torch.Size([2])\n    调整后标签的形状: torch.Size([2, 1])\n6.4 torchvision - 计算机视觉的“快餐店”对于常见的计算机视觉任务，你甚至不需要自己写 Dataset。torchvision 包提供了：\n\ntorchvision.datasets: 包含许多经典数据集的 Dataset 实现，如 MNIST, CIFAR10, ImageNet。你只需一行代码就可以下载和加载它们。\ntorchvision.transforms: 包含一系列常用的图像预处理操作，如调整大小、裁剪、旋转、转换为张量、归一化等。\n\ntransforms.Compose 是一个非常重要的工具，它可以将多个变换操作串联起来。\n代码示例：加载带变换的 MNIST 数据集\nimport torchvision\nimport torchvision.transforms as transforms\n\n# 定义一系列变换\n# 1. ToTensor(): 将 PIL.Image 或 numpy.ndarray 转换为 torch.FloatTensor，\n#    并且将像素值从 [0, 255] 缩放到 [0.0, 1.0]。\n# 2. Normalize(mean, std): 对张量进行归一化。output = (input - mean) / std。\n#    这里的 mean 和 std 是 MNIST 数据集在所有像素上的均值和标准差。\n#    归一化有助于模型更快地收敛。\ntransform_pipeline = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,)) # MNIST是单通道，所以mean和std都只有一个值\n])\n\n# 加载训练集\ntrain_dataset = torchvision.datasets.MNIST(root='./data', \n                                           train=True, \n                                           transform=transform_pipeline, \n                                           download=True)\n\n# 加载测试集\ntest_dataset = torchvision.datasets.MNIST(root='./data', \n                                          train=False, \n                                          transform=transform_pipeline)\n\n# 创建 DataLoader\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n\nprint(f\"\\nMNIST 训练集大小: {len(train_dataset)}\")\nprint(f\"MNIST 测试集大小: {len(test_dataset)}\")\n\n\n模块 7：经典模型实战现在，让我们把所有知识融会贯通，完成两个端到端的项目。\n7.1 计算机视觉 (CV): 在 MNIST 上实现 LeNet-5LeNet-5 是由 Yann LeCun 在 1998 年提出的，是早期最成功的卷积神经网络之一，为现代 CNN 奠定了基础。我们将用 PyTorch 复现它，来解决手写数字识别问题。\nA. 完整流程规划\n\n准备数据: 使用 torchvision 加载 MNIST 数据集，并创建 DataLoader。\n定义模型: 创建一个 LeNet5 类，继承 nn.Module，并按照 LeNet-5 的结构搭建网络。\n实例化: 创建模型、损失函数 (CrossEntropyLoss) 和优化器 (Adam) 的实例。\n训练循环: 编写 for 循环，遍历 epochs 和 train_loader，执行“训练五步法”。\n评估循环: 编写评估函数，在 test_loader 上计算模型的准确率和损失。\n可视化: 绘制训练过程中的损失变化曲线，并展示一些模型的预测结果。\n\nB. 代码实现\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# --- 1. 超参数与设备 ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 64\nLEARNING_RATE = 0.001\nEPOCHS = 10\n\n# --- 2. 准备数据 ---\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # 将图像数据转换为张量，并将像素值归一化到0到1内\n    transforms.Normalize((0.1307,), (0.3081,))  # 对已经转换为张量的图像数据进行归一化处理，分别是均值和标准差\n])\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n\n# --- 3. 定义 LeNet-5 模型 ---\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        # LeNet-5 结构\n        # 输入: 1x28x28\n        # C1: 6个5x5卷积核, 输出 6x28x28 (padding=2)\n        # S2: 2x2最大池化, 输出 6x14x14\n        # C3: 16个5x5卷积核, 输出 16x10x10\n        # S4: 2x2最大池化, 输出 16x5x5\n        # C5(FC1): 全连接层, 16*5*5 -> 120\n        # FC2: 全连接层, 120 -> 84\n        # 输出: 84 -> 10\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) # 保持尺寸不变\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5) # 展平\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# --- 4. 实例化 ---\nmodel = LeNet5().to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# --- 5. 训练与评估循环 ---\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\nfor epoch in range(EPOCHS):\n    # 训练\n    model.train()\n    running_loss = 0.0\n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n\n        # 训练五步法\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    train_losses.append(running_loss / len(train_loader))\n\n    # 评估\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    test_accuracies.append(accuracy)\n    test_losses.append(test_loss / len(test_loader))\n\n    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {accuracy:.2f}%')\n\nprint('Finished Training')\n\n# --- 6. 结果可视化 ---\n\n# 6.1 绘制损失和准确率曲线\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.title('Loss over epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(test_accuracies, label='Test Accuracy', color='orange')\nplt.title('Accuracy over epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\nplt.show()\n\n# 6.2 可视化部分预测结果\ndef imshow(img):\n    img = img / 2 + 0.5 # 反归一化\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')\n\n# 获取一个批次的测试数据\ndataiter = iter(test_loader)\nimages, labels = next(dataiter)\nimages, labels = images.to(DEVICE), labels.to(DEVICE)\n\n# 进行预测\noutputs = model(images)\n_, predicted = torch.max(outputs, 1)\n\n# 显示图像和预测结果\nprint('真实标签: ', ' '.join(f'{labels[j].item()}' for j in range(8)))\nprint('预测结果: ', ' '.join(f'{predicted[j].item()}' for j in range(8)))\n\n# 反归一化以正确显示\ninv_normalize = transforms.Normalize(\n   mean=[-0.1307/0.3081],\n   std=[1/0.3081]\n)\nimages_to_show = torch.stack([inv_normalize(img) for img in images[:8].cpu()])\n\nplt.figure(figsize=(10, 4))\nimshow(torchvision.utils.make_grid(images_to_show))\nplt.show()\n\n训练结果：\nEpoch [1/10], Train Loss: 0.2482, Test Loss: 0.0717, Test Accuracy: 97.75%\nEpoch [2/10], Train Loss: 0.0670, Test Loss: 0.0474, Test Accuracy: 98.47%\nEpoch [3/10], Train Loss: 0.0492, Test Loss: 0.0455, Test Accuracy: 98.54%\nEpoch [4/10], Train Loss: 0.0385, Test Loss: 0.0363, Test Accuracy: 98.86%\nEpoch [5/10], Train Loss: 0.0305, Test Loss: 0.0348, Test Accuracy: 98.84%\nEpoch [6/10], Train Loss: 0.0264, Test Loss: 0.0435, Test Accuracy: 98.73%\nEpoch [7/10], Train Loss: 0.0222, Test Loss: 0.0382, Test Accuracy: 98.81%\nEpoch [8/10], Train Loss: 0.0208, Test Loss: 0.0304, Test Accuracy: 99.11%\nEpoch [9/10], Train Loss: 0.0166, Test Loss: 0.0335, Test Accuracy: 98.94%\nEpoch [10/10], Train Loss: 0.0154, Test Loss: 0.0402, Test Accuracy: 98.82%\nFinished Training\n\n","slug":"PyTorch学习-3-数据处理-项目实战","date":"2025-07-08T05:09:24.000Z","categories_index":"python语法学习","tags_index":"PyTorch","author_index":"犬夜叉"},{"id":"d970942a0c3301df824e3c1c77149df6","title":"PyTorch学习-2-神经网络核心","content":"PyTorch 学习笔记 - 阶段二：神经网络的核心在前面的章节中，我们已经熟悉了 PyTorch 的基本数据结构——张量（Tensor），以及如何在它们之上执行操作。现在，是时候将这些基础知识组合起来，构建、训练和评估一个真正能学习的“大脑”——神经网络。\n本章的目标是让你对使用 PyTorch 构建一个完整神经网络的流程了如指掌。我们将像玩乐高积木一样，一块一块地搭建起复杂的模型。我们将深入探索 torch.nn 模块，它是我们所有模型的“积木盒”；我们还将掌握 torch.optim，它是驱动模型学习的“引擎”。学完本章，你将能自信地将 nn.Module、损失函数和优化器这“三驾马车”协同起来，解决实际问题。\n\n模块 4：torch.nn - 神经网络的“乐高”torch.nn 是 PyTorch 专门为神经网络设计的命名空间。如果你把整个 PyTorch 库想象成一个庞大的工具箱，那么 torch.nn 就是其中专门存放预制零件的区域，里面有螺丝（权重）、横梁（层）、甚至预组装好的模块，你只需要按照蓝图将它们组合起来。\n4.1 nn.Module - 所有模型的基类在 PyTorch 的世界里，所有神经网络模型，无论简单还是复杂，都应该继承自 nn.Module。\n核心理念：一种规范，而非仅仅一个类\n将 nn.Module 理解为一个简单的 Python 类是远远不够的。它是一种精心设计的 规范 和 容器。当你遵循这个规范构建模型时，nn.Module 会在幕后为你处理大量繁琐的工作，其中最核心的一项就是 自动追踪。\n什么是自动追踪？当你将一个 nn.Module 的子模块（比如一个卷积层 nn.Conv2d）或者一个可学习的参数（nn.Parameter）定义为你模型的属性时，nn.Module 会自动将它们“注册”到内部的列表中。这意味着你无需手动维护一个包含模型所有权重和偏置的列表，nn.Module 会帮你打理好一切。这为后续的参数访问、优化、保存和加载提供了巨大的便利。\n让我们通过两个核心方法来解构 nn.Module 的工作方式：__init__(self) 和 forward(self, x)。\n1. __init__(self) 方法详解\n\n职责：定义和初始化模型中所有需要用到的、带有可学习参数的组件（我们称之为“层”，Layers）。\n操作：在这个方法中，你会实例化所有你需要的层，并将它们赋值给类的属性（例如 self.conv1）。这些层本身就是 nn.Module 的子类实例。这就像在开始拼乐高之前，先把所有需要的积木块从盒子中拿出来，摆在面前。\n\n2. forward(self, x) 方法详解\n\n职责：定义数据在前向传播过程中的“流动路径”。它接收输入数据 x，然后明确地描述 x 应该如何依次通过 __init__ 中定义的各个层，最终得到模型的输出。\n操作：在这里，你将 __init__ 中定义的层当作函数来调用，将数据传递给它们。例如 x = self.conv1(x)。这就像是乐高的拼装说明书，告诉你第一步用哪块积木，第二步用哪块，如何将它们连接起来。\n\n代码示例：构建一个简单的网络\n让我们构建一个非常简单的网络，它包含一个线性层和一个 ReLU 激活函数，来直观地理解 __init__ 和 forward 的分工。\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# 1. 定义一个继承自 nn.Module 的类\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        # 必须首先调用父类的 __init__ 方法\n        super(SimpleNet, self).__init__()\n\n        # --- “采购”阶段 ---\n        # 在这里定义我们需要的层。这些层包含了可学习的参数。\n        # self.layer1 是一个 nn.Linear 实例，它接受 10 个输入特征，输出 5 个特征。\n        # PyTorch 会自动追踪 self.layer1 的权重和偏置。\n        self.layer1 = nn.Linear(in_features=10, out_features=5)\n        self.layer2 = nn.Linear(in_features=5, out_features=2)\n\n    def forward(self, x):\n        # --- “拼装”阶段 ---\n        # 定义数据流动的路径。\n        # x 的初始形状: [batch_size, 10]\n\n        # 1. 通过第一个线性层\n        x = self.layer1(x)\n        # x 的形状现在是: [batch_size, 5]\n\n        # 2. 应用 ReLU 激活函数\n        # 注意这里我们使用了 F.relu，这是一个函数式调用。后面会详细解释。\n        x = F.relu(x)\n\n        # 3. 通过第二个线性层\n        x = self.layer2(x)\n        # x 的形状现在是: [batch_size, 2]\n\n        return x\n\n# 实例化我们的模型\nmodel = SimpleNet()\n\n# 打印模型结构，可以看到我们定义的层\nprint(model)\n\n# 创建一个假的输入数据来测试前向传播\n# 假设我们有一个批次，包含 3 个样本，每个样本有 10 个特征\ndummy_input = torch.randn(3, 10)\noutput = model(dummy_input) # 直接调用 model(input) 就会执行 forward 方法\n\nprint(\"\\n输入尺寸:\", dummy_input.shape)\nprint(\"输出尺寸:\", output.shape)\n\n输出：\nSimpleNet(\n  (layer1): Linear(in_features=10, out_features=5, bias=True)\n  (layer2): Linear(in_features=5, out_features=2, bias=True)\n)\n\n输入尺寸: torch.Size([3, 10])\n输出尺寸: torch.Size([3, 2])\ntorch.nn 与 torch.nn.functional 的关系\n你可能已经注意到，在 forward 方法中我们使用了 F.relu 而不是 nn.ReLU。这是一个重要的区别。\n\ntorch.nn 模块（如 nn.Linear, nn.Conv2d, nn.ReLU）：通常是有状态的。它们是 nn.Module 的子类，内部可以包含可学习的参数（如 nn.Linear 的权重）或持久化的状态（如 nn.BatchNorm 的均值和方差）。因此，它们必须在 __init__ 中实例化并作为模型的属性。\ntorch.nn.functional（通常简写为 F）：通常是无状态的。它们是纯粹的函数式操作，不包含任何可学习的参数。例如，relu、max_pool2d、sigmoid 等操作，它们只是对输入张量进行一次计算，没有自己的权重需要维护。\n\n使用场景与最佳实践：\n\n对于有可学习参数的层（如卷积层、线性层），必须使用 nn.Module 的形式，并在 __init__ 中定义。\n对于没有可学习参数的操作（如激活函数、池化操作），你可以自由选择：\n函数式 F.relu：更简洁，通常推荐在 forward 方法中直接使用。\n模块式 nn.ReLU()：需要在 __init__ 中定义 self.relu = nn.ReLU()，然后在 forward 中调用 x = self.relu(x)。这样做的好处是，当使用 print(model) 时，这个激活层会明确地显示在网络结构中，有时能让结构更清晰。\n\n\n\n在实践中，大多数人倾向于对激活函数和池化使用函数式接口，因为它减少了 __init__ 中的代码量。\nmodel.parameters() 和 model.state_dict()\nnn.Module 的自动追踪能力体现在这两个极其有用的方法上。\n\nmodel.parameters():\n\n作用：返回一个包含模型所有可学习参数（requires_grad=True 的张量，主要是权重和偏置）的迭代器。\n用途：这是连接模型和优化器的桥梁。在训练时，我们会将这个迭代器传递给优化器（如 optim.SGD），告诉它：“这些就是你需要计算梯度并更新的参数。”\n\n# 接着上面的 SimpleNet 例子\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nprint(\"\\n模型的可学习参数:\")\nfor name, param in model.named_parameters(): # named_parameters() 更常用，因为它同时返回参数名\n    if param.requires_grad:\n        print(f\"名称: {name}, 尺寸: {param.size()}\")\n\n\n\n输出：\n模型的可学习参数:\n名称: layer1.weight, 尺寸: torch.Size([5, 10])\n名称: layer1.bias, 尺寸: torch.Size([5])\n名称: layer2.weight, 尺寸: torch.Size([2, 5])\n名称: layer2.bias, 尺寸: torch.Size([2])\n\nmodel.state_dict():\n\n作用：返回一个 Python 字典（OrderedDict），它将模型的每个层映射到其参数张量。它包含了模型所有的状态，不仅包括可学习参数，还包括一些非可学习的缓冲区（比如 BatchNorm 层的 running_mean）。\n用途：这是保存和加载模型权重的标准方式。你可以轻松地将这个字典保存到磁盘，并在之后加载它来恢复一个已经训练好的模型的状态。\n\n# 获取模型的状态字典\nstate_dict = model.state_dict()\nprint(\"\\n模型的状态字典 (部分键):\")\nfor key in state_dict.keys():\n    print(key)\n\n# 保存模型权重\ntorch.save(state_dict, 'simple_net_weights.pth')\n\n# 加载模型权重\n# 1. 首先需要创建一个相同结构的模型实例\nnew_model = SimpleNet()\n# 2. 加载状态字典\nnew_model.load_state_dict(torch.load('simple_net_weights.pth'))\n\n注意：state_dict 只保存参数，不保存模型结构。加载时，你必须先有一个与保存时结构完全相同的模型实例。\n\n\n输出：\n模型的状态字典 (部分键):\nlayer1.weight\nlayer1.bias\nlayer2.weight\nlayer2.bias\n&lt;All keys matched successfully&gt;\n4.2 常用层 (Layers) - 分类与理解现在我们来认识一下 torch.nn 积木盒里最常用的一些积木。\n1. 卷积层 (Convolutional Layers)\n\nnn.Conv2d: 计算机视觉任务的基石。它通过在输入图像（或特征图）上滑动一个小的卷积核（filter）来提取局部特征。\n\n核心参数:\nin_channels (int): 输入特征图的通道数。对于第一层，这通常是图像的通道数（RGB为3，灰度为1）。\nout_channels (int): 输出特征图的通道数。这决定了该层要学习多少种不同的特征。\nkernel_size (int or tuple): 卷积核的大小。3 代表 (3, 3) 的方阵。\nstride (int or tuple, optional, default=1): 卷积核在图像上滑动的步长。\npadding (int or tuple, optional, default=0): 在输入图像的边界周围添加的“填充”层数。padding 对于控制输出特征图的尺寸至关重要，特别是当你想保持输入输出尺寸相同时。\n\n\n\n输出尺寸计算公式:Output_size = floor( (Input_size - Kernel_size + 2 * Padding) / Stride ) + 1\n\n\n代码示例:\n    # 假设输入是一个 32x32 的 RGB 图像，批次大小为 1\n    # 输入形状: [1, 3, 32, 32] (N, C_in, H, W)\n    input_image = torch.randn(1, 3, 32, 32)\n\n    # 定义一个卷积层\n    # 输入通道为3，输出通道为16（学习16种特征），卷积核大小为3x3，步长为1，填充为1\n    conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n\n    output_feature_map = conv_layer(input_image)\n\n    print(\"卷积层输入尺寸:\", input_image.shape)\n    print(\"卷积层输出尺寸:\", output_feature_map.shape)\n    # 计算: H_out = floor((32 - 3 + 2*1)/1) + 1 = 32. 尺寸不变是因为 padding=1 抵消了 kernel_size=3 的缩小效应。\n\n输出：\n卷积层输入尺寸: torch.Size([1, 3, 32, 32])\n卷积层输出尺寸: torch.Size([1, 16, 32, 32])\n2. 池化层 (Pooling Layers)\n\nnn.MaxPool2d: 通常跟在卷积层和激活函数之后，用于降维和特征不变性。它将特征图划分为不重叠的区域，并从每个区域中取出最大值。\n作用:\n减少特征图的空间尺寸，从而减少后续层的参数数量和计算量。\n提供一定程度的平移不变性，即目标在图像中轻微移动，池化后的输出仍能保持稳定。\n\n\n核心参数:\nkernel_size (int or tuple): 池化窗口的大小。\nstride (int or tuple, optional, default=kernel_size): 池化窗口的滑动步长。通常设置为与 kernel_size 相同，以实现不重叠的池化。\n\n\n\n\n\n代码示例:\n    # 使用上一步的输出作为输入\n    # 输入形状: [1, 16, 32, 32]\n\n    # 定义一个最大池化层\n    # 池化窗口为 2x2，步长为 2\n    pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    pooled_output = pool_layer(output_feature_map)\n\n    print(\"\\n池化层输入尺寸:\", output_feature_map.shape)\n    print(\"池化层输出尺寸:\", pooled_output.shape)\n    # 尺寸减半，因为 kernel_size=2, stride=2\n\n输出：\n池化层输入尺寸: torch.Size([1, 16, 32, 32])\n池化层输出尺寸: torch.Size([1, 16, 16, 16])\n3. 线性层 (Linear/Fully-Connected Layers)\n\nnn.Linear: 对输入进行线性变换 (y = Wx + b)。它将每个输入神经元连接到每个输出神经元，因此也叫全连接层。通常用在网络的末端，将前面提取的特征整合起来，用于最终的分类或回归。\n\n核心参数:\nin_features (int): 每个输入样本的特征数量。\nout_features (int): 每个输出样本的特征数量。对于分类任务，这通常是类别的数量。\n\n\n\n\n与 nn.Flatten 的配合:卷积/池化层的输出是多维的（如 [N, C, H, W]），而线性层期望的输入是二维的（[N, in_features]）。因此，在从卷积/池化层过渡到线性层之前，必须将特征图“展平”（Flatten）。\n\nnn.Flatten(): 一个方便的层，可以插入到 nn.Sequential 或在 forward 中调用，它会自动将除了批次维度（N）之外的所有维度展平成一个维度。\nx.view(-1, num_features): 一种更手动的展平方式。-1 告诉 PyTorch 自动计算该维度的大小。\n\n\n\n代码示例:\n    # 使用上一步的池化输出\n    # 输入形状: [1, 16, 16, 16]\n    flattened_output = pooled_output.view(1, -1) # 手动展平\n    # 或者 flattened_output = nn.Flatten()(pooled_output)\n\n    num_features = flattened_output.shape[1] # 1 * 16 * 16 * 16 = 4096\n    print(f\"\\n展平后尺寸: {flattened_output.shape}, 特征数量: {num_features}\")\n\n    # 定义一个线性层，假设我们要进行10分类\n    fc_layer = nn.Linear(in_features=num_features, out_features=10)\n\n    final_output = fc_layer(flattened_output)\n\n    print(\"线性层输出尺寸 (logits):\", final_output.shape)\n\n输出：\n展平后尺寸: torch.Size([1, 4096]), 特征数量: 4096\n线性层输出尺寸 (logits): torch.Size([1, 10])\n4. 循环层 (Recurrent Layers)\n\nnn.RNN, nn.LSTM, nn.GRU: 这些是为处理序列数据（如文本、语音、时间序列）而设计的。它们的核心思想是拥有一个“记忆”或隐藏状态，可以在处理序列的每个时间步时传递信息。\nnn.LSTM (Long Short-Term Memory) 是最常用的，因为它通过精巧的“门控”机制（输入门、遗忘门、输出门）有效解决了标准 RNN 的梯度消失/爆炸问题，能够学习长距离依赖。\n输入/输出形状: 这是一个关键点，也是初学者的常见困惑。PyTorch 中 RNN 层的默认输入形状是 (Sequence_Length, Batch_Size, Input_Size)。\nSequence_Length: 序列的长度（例如，一个句子中的单词数）。\nBatch_Size: 一批处理多少个序列。\nInput_Size: 每个时间步的输入特征维度（例如，词嵌入的维度）。\n\n\n\n\n\n5. 辅助层\n\nnn.Dropout: 一种非常重要的正则化技术，用于防止过拟合。\n\n工作原理: 在训练期间，它会以指定的概率 p 随机地将输入张量中的一部分元素置为零（即“丢弃”神经元），并对剩余元素进行缩放（1/(1-p)），以保持总体的期望值不变。在评估期间（调用 model.eval() 后），Dropout 层不起任何作用，直接传递输入。\n作用: 强迫网络学习冗余的表示，而不是依赖于少数几个特定的神经元，从而提高模型的泛化能力。\n\n# p=0.5 表示每个神经元有 50% 的概率被丢弃\ndropout_layer = nn.Dropout(p=0.5)\n\n# 假设有一个激活后的张量\nactivations = torch.randn(1, 10)\nprint(\"Dropout前:\", activations)\n\n# 在训练模式下应用 Dropout\nmodel_is_training = True\nif model_is_training:\n    output_after_dropout = dropout_layer(activations)\nprint(\"Dropout后 (训练模式):\", output_after_dropout) # 会看到一些元素变为0\n\n\n\n输出：\nDropout前: tensor([[-2.2041,  1.3797,  1.0010, -0.8170,  0.6776, -0.4323,  2.0645,  0.7844,\n          0.7845,  0.8712]])\nDropout后 (训练模式): tensor([[-4.4083,  2.7595,  0.0000, -1.6340,  0.0000, -0.0000,  4.1290,  1.5687,\n          0.0000,  1.7424]])\n4.3 激活函数 (Activation Functions)核心作用：为模型引入非线性。如果没有激活函数，一个由多层线性层组成的网络，无论多深，其最终效果都等同于一个单层的线性模型。激活函数打破了这种线性，使得网络能够学习和拟合复杂得多的函数。\n\nnn.ReLU (Rectified Linear Unit): f(x) = max(0, x)\n\n优点:\n计算极其高效。\n在正数区间的梯度为1，有效缓解了梯度消失问题。\n\n\n缺点:\nDying ReLU 问题: 如果一个神经元的输入恒为负，那么它的梯度将永远是0，这个神经元将无法再通过梯度下降进行更新。\n\n\n地位: 现代深度学习中最常用的激活函数。\n\n\nnn.Sigmoid: f(x) = 1 / (1 + exp(-x))\n\n作用: 将任意实数压缩到 (0, 1) 区间。\n用途: 常用于二分类问题的输出层，将输出解释为概率。\n缺点:\n梯度消失: 当输入值非常大或非常小时，其导数接近于0，导致梯度在反向传播时迅速减小，难以训练深层网络。\n输出不以0为中心。\n\n\n\n\nnn.Tanh (Hyperbolic Tangent): f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n\n作用: 将任意实数压缩到 (-1, 1) 区间。\n优点: 输出以0为中心，通常比 Sigmoid 收敛更快。\n缺点: 仍然存在梯度消失问题。\n\n\nnn.Softmax:\n\n作用: 将一个向量（通常是网络的原始输出，称为 logits）转换为一个概率分布，其中每个元素都在 (0, 1) 之间，且所有元素之和为1。\n用途: 专门用于多分类问题的输出层。\n注意: Softmax 通常与交叉熵损失函数配合使用。正如我们将在下一节看到的，nn.CrossEntropyLoss 内部已经包含了 Softmax 操作，所以你不应该在模型的最后一层手动添加 nn.Softmax。\n\n\n\n4.4 损失函数 (Loss Functions)核心作用：衡量模型预测值 (outputs) 与真实标签 (labels) 之间的差距。这个差距（即损失值）是一个标量，它是反向传播的起点。优化器的目标就是通过调整模型参数来最小化这个损失值。\n在 PyTorch 中，损失函数也是 nn.Module 的子类，通常在使用前实例化。\n\nnn.MSELoss (Mean Squared Error Loss):\n\n公式: Loss = mean((outputs - labels)^2)\n用途: 主要用于回归任务，即预测一个连续值（如房价、温度）。\n示例:\ncriterion = nn.MSELoss()\npredicted_prices = torch.tensor([250000.0, 310000.0])\nactual_prices = torch.tensor([260000.0, 300000.0])\nloss = criterion(predicted_prices, actual_prices)\nprint(\"MSE Loss:\", loss.item())\n\n\n\n\n\n\nnn.BCELoss (Binary Cross Entropy Loss):\n\n用途: 用于二分类任务。\n要求:\n模型的输出必须经过 Sigmoid 激活，使其值落在 [0, 1] 区间，代表正类的概率。\n真实标签也应该是 [0, 1] 之间的浮点数。\n\n\n示例:\ncriterion = nn.BCELoss()\n# 模型输出，已经过 Sigmoid\noutputs = torch.tensor([0.9, 0.2, 0.8]) # 样本1是正类的概率为0.9，...\n# 真实标签\nlabels = torch.tensor([1.0, 0.0, 1.0]) # 样本1是正类，样本2是负类，...\nloss = criterion(outputs, labels)\nprint(\"BCE Loss:\", loss.item())\n\n\nnn.BCEWithLogitsLoss: 这是一个更稳定、更推荐的版本，它将 Sigmoid 层和 BCELoss 合二为一。使用它时，你的模型不需要在最后一层加 Sigmoid，直接输出原始的 logits 即可。\n\n\nnn.CrossEntropyLoss:\n\n用途: 用于多分类任务。这是最常用的分类损失函数。\n极其重要: 这个损失函数内部整合了 LogSoftmax 和 NLLLoss (Negative Log Likelihood Loss)。这意味着：\n你不应该在模型的最后一层添加 Softmax 或 LogSoftmax 层。\n模型的输出应该是原始的、未经激活的 logits（每个类别的分数）。\n真实标签应该是类别索引（LongTensor），而不是 one-hot 编码。\n\n\n示例:\ncriterion = nn.CrossEntropyLoss()\n\n# 假设批次大小为3，共有5个类别\n# 模型输出的原始 logits\n# 形状: [batch_size, num_classes]\noutputs = torch.randn(3, 5) \n\n# 真实标签 (类别索引)\n# 形状: [batch_size]\nlabels = torch.tensor([1, 0, 4]) # 第一个样本是第1类，第二个是第0类，...\n\nloss = criterion(outputs, labels)\nprint(\"Cross Entropy Loss:\", loss.item())\n\n\n\n\n\n\n模块 5：优化器与训练循环 (Optimizer &amp; Training Loop)我们已经准备好了模型的蓝图 (nn.Module) 和评判标准 (Loss Function)。现在，我们需要一个“工匠”来根据评判结果，实际地去修改模型的参数，让它变得更好。这个工匠就是优化器 (torch.optim)。\n5.1 优化器 (torch.optim)核心角色：模型参数的“管家”和“更新者”。\n它的工作流程很简单：\n\n在初始化时，你告诉它要管理哪些参数（通过 model.parameters()）。\n在训练的每一步，当所有参数的梯度（.grad）都已经通过 loss.backward() 计算出来后，你调用优化器的 step() 方法。\nstep() 方法会根据其内部定义的优化算法（如 SGD、Adam）和计算出的梯度，来更新它所管理的所有参数。\n\n常用优化器\n\noptim.SGD (Stochastic Gradient Descent):\n\n最基础的优化算法。它沿着梯度的反方向更新参数。\nparam = param - learning_rate * param.grad\n关键参数:\nparams: 传入 model.parameters()。\nlr (learning rate): 学习率，控制每次更新的步长。这是最重要的超参数。\nmomentum (动量): 一个非常有用的参数。引入动量可以帮助优化过程：\n加速收敛: 如果梯度方向保持一致，动量会累积，使得更新速度加快。\n越过局部最小值: 就像一个有质量的球滚下山，动量可以帮助它冲过一些小的凹陷（局部最小值）。\n\n\n\n\n\n\noptim.Adam (Adaptive Moment Estimation):\n\n现代深度学习中最流行和最常用的优化器之一。\n它是一种自适应学习率的算法，会为每个参数维护独立的学习率，并根据梯度的一阶矩（均值）和二阶矩（方差）进行调整。\n优点: 通常收敛速度比 SGD 快，对学习率的设置不如 SGD 那么敏感，在大多数情况下都能取得很好的效果。是很多任务的“首选默认”优化器。\n关键参数:\nparams, lr: 同 SGD。\nbetas: 用于计算梯度均值和方差的衰减率，通常使用默认值 (0.9, 0.999)。\neps: 一个很小的数，用于防止分母为零，通常使用默认值 1e-8。\n\n\n\n\n\n5.2 构建完整的训练流程 - 五步法现在，我们将所有部件组装起来，形成一个完整的训练循环。这个流程是 PyTorch 训练的黄金标准，你必须对每一步都了如指掌。\n准备阶段: 在任何循环开始之前，我们需要实例化所有必要的组件。\n# 假设我们正在做一个图像分类任务 (e.g., CIFAR-10)\n# 1. 准备数据 (在实际项目中，这里会是 DataLoader)\n# 假设批次大小为 64，图像为 3x32x32，10个类别\ndummy_inputs = torch.randn(64, 3, 32, 32)\ndummy_labels = torch.randint(0, 10, (64,))\n\n# 2. 实例化模型\n# 我们需要一个能处理这种输入的模型\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        # 32x32 -> pool -> 16x16 -> pool -> 8x8\n        # 所以展平后的特征数是 32 * 8 * 8\n        self.fc1 = nn.Linear(32 * 8 * 8, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # 展平所有维度，除了批次维度\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x) # 输出 logits\n        return x\n\nmodel = ConvNet()\n\n# 3. 实例化损失函数\ncriterion = nn.CrossEntropyLoss()\n\n# 4. 实例化优化器\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n训练循环\n# 外层循环 (Epochs): 代表整个数据集被完整训练的轮次\nnum_epochs = 10\nfor epoch in range(num_epochs):\n\n    # 在实际项目中，这里会是: for inputs, labels in train_loader:\n    # 我们用虚拟数据模拟一个批次的处理\n\n    # --- 核心训练五步 ---\n\n    # 1. 清零梯度 (Zero the gradients)\n    # 为什么? PyTorch 的 .backward() 会将新计算的梯度 *累加* 到 .grad 属性中。\n    # 如果不清零，当前批次的梯度会与之前所有批次的梯度叠加，这会导致错误的更新方向。\n    # 所以在每次计算新梯度之前，必须将旧的梯度清零。\n    optimizer.zero_grad()\n\n    # 2. 前向传播 (Forward pass)\n    # 将一批数据输入模型，得到预测结果 (logits)。\n    outputs = model(dummy_inputs)\n\n    # 3. 计算损失 (Compute loss)\n    # 使用损失函数比较预测结果和真实标签，得到一个标量损失值。\n    loss = criterion(outputs, dummy_labels)\n\n    # 4. 反向传播 (Backward pass)\n    # PyTorch 的 Autograd 引擎会从 loss 开始，沿着计算图反向传播，\n    # 自动计算出模型中所有可学习参数 (requires_grad=True) 相对于损失的梯度。\n    # 这些梯度值会被存储在对应参数的 .grad 属性中。\n    loss.backward()\n\n    # 5. 更新权重 (Update weights)\n    # 优化器根据其内部定义的算法 (如 Adam)，使用 .grad 中存储的梯度信息，\n    # 去更新它所管理的所有参数 (在初始化时传入的 model.parameters())。\n    optimizer.step()\n\n    # 打印一些信息来观察训练过程\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n随机输出结果：\nEpoch [1/10], Loss: 2.2951\nEpoch [2/10], Loss: 2.2638\nEpoch [3/10], Loss: 2.2311\nEpoch [4/10], Loss: 2.1997\nEpoch [5/10], Loss: 2.1695\nEpoch [6/10], Loss: 2.1350\nEpoch [7/10], Loss: 2.0963\nEpoch [8/10], Loss: 2.0478\nEpoch [9/10], Loss: 1.9958\nEpoch [10/10], Loss: 1.9374\n这五步——zero_grad(), model(), criterion(), backward(), step()——构成了 PyTorch 训练的核心，你需要将它们刻在脑海里。\n5.3 模型评估训练完成后，我们需要在测试集（模型从未见过的数据）上评估其性能。评估过程与训练过程有几个关键区别。\n与训练的区别：评估的唯一目的是检验模型的性能，这个过程不应该更新模型权重，也不应该计算梯度。\n两个必须的操作: model.eval() 和 with torch.no_grad():\n\nmodel.eval():\n\n必须调用！ 这个方法会将模型切换到“评估模式”。\n这有什么影响？它会关闭像 Dropout 这样的层（因为评估时我们希望使用整个网络的能力），并使像 BatchNorm 这样的层使用在整个训练集上学习到的统计数据，而不是当前批次的统计数据。这确保了评估结果是确定性的和可复现的。\n与之对应的是 model.train()，它会将模型切换回训练模式。在训练循环开始前调用 model.train() 是一个好习惯。\n\n\nwith torch.no_grad()::\n\n必须使用！ 这是一个上下文管理器，它会告诉 PyTorch 在这个代码块内不要计算和存储梯度。\n好处:\n节省大量内存: PyTorch 在进行前向传播时，如果需要计算梯度，会保存大量的中间激活值，以便在反向传播时使用。no_grad 禁用了这个过程，极大地减少了内存占用。\n加快计算速度: 省去了构建计算图和梯度计算的开销，使得前向传播更快。\n\n\n\n\n\n评估循环示例\n# 假设我们有一个测试数据加载器 test_loader\n# 我们用虚拟数据模拟\ndummy_test_inputs = torch.randn(100, 3, 32, 32) # 100个测试样本\ndummy_test_labels = torch.randint(0, 10, (100,))\n\n# 1. 切换到评估模式\nmodel.eval()\n\n# 2. 使用 no_grad 上下文管理器\nwith torch.no_grad():\n    correct = 0\n    total = 0\n\n    # 模拟遍历测试集\n    # 在实际项目中: for images, labels in test_loader:\n    outputs = model(dummy_test_inputs)\n\n    # 计算损失（可选，但通常会监控测试集上的损失）\n    test_loss = criterion(outputs, dummy_test_labels)\n\n    # 计算准确率\n    # outputs 的形状是 [100, 10]，每一行是对应样本在10个类别上的logits\n    # torch.max(outputs.data, 1) 会返回每一行的最大值和其索引\n    # 我们关心的是索引，即模型预测的类别\n    _, predicted = torch.max(outputs.data, 1)\n\n    total += dummy_test_labels.size(0)\n    correct += (predicted == dummy_test_labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f'\\n模型在测试集上的准确率: {accuracy:.2f} %')\nprint(f'模型在测试集上的损失: {test_loss.item():.4f}')\n\n# 如果之后还想继续训练，记得切换回训练模式\nmodel.train()\n\n输出：\n# 数据随机生成，准确率在10%左右是正常的\n模型在测试集上的准确率: 13.00 %\n模型在测试集上的损失: 2.3072\n","slug":"PyTorch学习-2-神经网络核心","date":"2025-07-08T01:46:23.000Z","categories_index":"python语法学习","tags_index":"PyTorch","author_index":"犬夜叉"},{"id":"4d6d66bf862bc9de2ae8dbbddfd17c18","title":"PyTorch学习-1.基础入门","content":"PyTorch 学习笔记 - 阶段一：基础入门模块 1：PyTorch 环境与简介1. 什么是 PyTorch？PyTorch 是一个由 Facebook AI 研究院主导开发的、基于 Python 的开源机器学习库。它主要用于两个领域：\n\n替代 NumPy：可以利用 GPU 的强大计算能力进行张量计算。\n深度学习平台：提供极大的灵活性和速度来构建和训练深度学习模型。\n\n核心特点：\n\nPythonic: PyTorch 的 API 设计非常直观，贴近 Python 编程风格，易于上手。\n动态计算图 (Dynamic Computational Graph): 这是 PyTorch 最显著的特点之一。计算图是“在运行时定义”（Define-by-Run）的，意味着图的结构会随着代码的执行而动态构建。这使得调试变得异常简单，并且非常适合处理动态结构的网络，如循环神经网络（RNN）。\n强大的社区: 尤其在学术界，PyTorch 拥有极高的使用率，这意味着你可以轻松找到最新的论文实现和丰富的学习资源。\n\n应用场景:PyTorch 广泛应用于计算机视觉 (CV)、自然语言处理 (NLP)、语音识别等前沿领域。许多顶尖的研究成果都是使用 PyTorch 实现的。\n2. PyTorch vs. TensorFlow这是一个常见的问题。两者都是顶级的深度学习框架，但设计哲学上有所不同。\n\n\n\n\n特性\nPyTorch\nTensorFlow\n\n\n\n\n计算图\n动态图\n静态图 为主，TF 2.x 后也默认采用 Eager Execution\n\n\nAPI 风格\n更贴近 Python，面向对象，直观\n选项更多，API 略显复杂，但生态系统（如 TFX, TF Serving）更完善\n\n\n调试\n非常容易，可使用标准 Python 调试工具 (如 pdb)\n相对困难，需要特定的调试工具\n\n\n社区\n学术界首选，研究和快速原型开发非常流行\n工业界部署方案成熟，拥有庞大的开发者基础\n\n\n学习曲线\n平缓，对初学者友好\n相对陡峭，概念较多\n\n\n\n\n小结: 对于初学者和研究人员来说，PyTorch 通常是入门的首选，因为它更直观、易于调试。TensorFlow 在工业部署和生产环境方面有其传统优势。不过，随着两个框架的互相借鉴，它们的差距正在逐渐缩小。\n3. 环境搭建为了不污染你主机的 Python 环境，我们强烈建议使用虚拟环境。Conda 是管理数据科学包的绝佳工具。\n使用 Conda 创建和激活环境:\n# 1. 创建一个名为 \"pytorch_env\" 的新环境，并指定 Python 版本（推荐 3.8, 3.9 或 3.10）\nconda create -n pytorch_env python=3.9\n\n# 2. 激活这个新环境\nconda activate pytorch_env\n\n安装 PyTorch:\n最重要的一步: 不要直接 pip install pytorch！请务必访问 PyTorch 官网。官网会根据你的操作系统、包管理器（Conda/Pip）、计算平台（CPU/GPU）自动生成最适合你的安装指令。\n示例 (以 Windows/Linux, Conda, CUDA 11.8 为例):\n# 前往官网，你可能会得到类似这样的指令\n# 这是针对支持 NVIDIA GPU 的版本\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# 如果你没有 NVIDIA GPU，请选择 CPU 版本\n# 指令会是这样：\npip3 install torch torchvision torchaudio\n\n4. 第一个 PyTorch 程序 - “Hello, Tensor!”安装完成后，让我们来验证一下。在你的虚拟环境中，打开 Python 解释器或创建一个 .py 文件。\n# main.py\nimport torch\n\n# 打印 PyTorch 版本\nprint(f\"PyTorch Version: {torch.__version__}\")\n\n# 检查 GPU 是否可用\nis_cuda_available = torch.cuda.is_available()\nprint(f\"CUDA (GPU) Available: {is_cuda_available}\")\nif is_cuda_available:\n    print(f\"Current CUDA device: {torch.cuda.get_device_name(0)}\")\n\n# 创建我们的第一个张量！\nx = torch.tensor([[1, 2], [3, 4]])\nprint(\"\\nHello, Tensor!\")\nprint(x)\n\n正常输出：\nPyTorch Version: 2.7.1+cu118\nCUDA (GPU) Available: True\nCurrent CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n\nHello, Tensor!\ntensor([[1, 2],\n        [3, 4]])\n运行这个脚本，如果一切正常，它会打印出你的 PyTorch 版本、GPU 信息（如果有）以及一个 2x2 的张量。\n\n模块 2：核心基石 - 张量 (Tensor)1. 张量是什么？张量 (Tensor) 是 PyTorch 中最基本的数据结构。你可以简单地把它理解为一个多维数组。它是所有模型输入、输出和参数的载体。\n\n0D 张量 (标量): 一个数字。torch.tensor(5)\n1D 张量 (向量): 一个数组。torch.tensor([1, 2, 3])\n2D 张量 (矩阵): 一个二维数组。torch.tensor([[1, 2], [3, 4]])\n3D 张量: 通常用于表示图像 (Height ， Width ， Channels) 或序列数据。\nnD 张量: 更高维度的数据。\n\n2. 创建张量PyTorch 提供了多种创建张量的方法。\nimport torch\nimport numpy as np\n\n# 1. 从 Python 列表或 NumPy 数组创建\ndata = [[1, 2], [3, 4]]\nx_data = torch.tensor(data)\nprint(f\"From list:\\n {x_data}\")\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array) # 使用 from_numpy 效率更高\nprint(f\"From NumPy array:\\n {x_np}\")\n\n# 3. 创建与另一个张量形状和属性相同的张量\nx_ones = torch.ones_like(x_data) # 保留 x_data 的属性 (如 dtype)\nprint(f\"Ones Tensor like x_data:\\n {x_ones}\")\n\nFrom list:\n tensor([[1, 2],\n        [3, 4]])\nFrom NumPy array:\n tensor([[1, 2],\n        [3, 4]])\n# 2. 创建指定形状的张量\nshape = (2, 3,)\n# 未初始化的张量，值是随机的\nrand_tensor = torch.rand(shape) \n# 全 1 张量\nones_tensor = torch.ones(shape)\n# 全 0 张量\nzeros_tensor = torch.zeros(shape)\n# 从标准正态分布中采样的张量\nrandn_tensor = torch.randn(shape)\n\nprint(f\"Random Tensor:\\n {rand_tensor}\")\nprint(f\"Ones Tensor:\\n {ones_tensor}\")\nprint(f\"Zeros Tensor:\\n {zeros_tensor}\")\nprint(f\"Randn Tensor:\\n {randn_tensor}\")\n\nRandom Tensor:\n tensor([[0.0645, 0.5513, 0.6257],\n        [0.6369, 0.9466, 0.3795]])\nOnes Tensor:\n tensor([[1., 1., 1.],\n        [1., 1., 1.]])\nZeros Tensor:\n tensor([[0., 0., 0.],\n        [0., 0., 0.]])\nRandn Tensor:\n tensor([[-0.1138,  0.1148,  0.1198],\n        [ 1.4761, -0.5143, -0.7777]])\n# 3. 创建与另一个张量形状和属性相同的张量\nx_ones = torch.ones_like(x_data) # 保留 x_data 的属性 (如 dtype)\nprint(f\"Ones Tensor like x_data:\\n {x_ones}\")\n\nOnes Tensor like x_data:\n tensor([[1, 1],\n        [1, 1]])\n3. 张量的数据类型 (dtype)每个张量都有一个数据类型 (dtype)。在深度学习中，最常用的是 torch.float32（单精度浮点数）和 torch.long（用于标签或索引）。\n# 默认是 torch.float32\nfloat_tensor = torch.ones(2, 2) \nprint(f\"Default dtype: {float_tensor.dtype}\")\n\n# 指定 dtype\nlong_tensor = torch.tensor([1, 2, 3], dtype=torch.long)\nprint(f\"Long tensor dtype: {long_tensor.dtype}\")\n\n# 转换 dtype\nfloat_tensor_from_long = long_tensor.to(torch.float16) # .float() 是 .to(torch.float32) 的简写\nprint(f\"Converted to float16: {float_tensor_from_long.dtype}\")\n\nDefault dtype: torch.float32\nLong tensor dtype: torch.int64\nConverted to float16: torch.float16\n4. 张量操作这是 PyTorch 的精髓所在，操作与 NumPy 非常相似。\ntensor = torch.ones(4, 4)\n# 将第 1 行（索引为 0）的所有元素变为 2\ntensor[0, :] = 2 \nprint(f\"Original Tensor:\\n {tensor}\\n\")\n\n# 索引和切片 (Slicing)\nfirst_row = tensor[0]\nfirst_col = tensor[:, 0]\nsub_tensor = tensor[1:3, 1:3]\nprint(f\"First Row: {first_row}\")\nprint(f\"First Column: {first_col}\")\nprint(f\"Sub-tensor (2x2):\\n {sub_tensor}\\n\")\n\n# 拼接 (Concatenating)\n# dim=0 按行拼接, dim=1 按列拼接\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(f\"Concatenated Tensor (dim=1):\\n {t1}\\n\")\n\n# 算术运算\n# 矩阵乘法\nmat_mul = tensor.matmul(tensor.T) # tensor.T 是转置\n# 逐元素乘法\nelem_mul = tensor.mul(tensor)\n\nprint(f\"Matrix Multiplication:\\n {mat_mul}\\n\")\nprint(f\"Element-wise Multiplication:\\n {elem_mul}\\n\")\n\n# 数学函数\nsum_val = tensor.sum()\nmean_val = tensor.mean()\n# .item() 用于从只包含一个元素的张量中提取 Python 数值\nprint(f\"Sum of all elements: {sum_val.item()}\")\nprint(f\"Mean of all elements: {mean_val.item()}\")\n\n# 变形 (Reshaping)\n# view() 和 reshape() 功能类似，view() 更高效但对内存连续性有要求\n# reshape() 更灵活\nflat_tensor = tensor.reshape(-1) # -1 表示自动计算该维度的大小\nprint(f\"Flattened tensor: {flat_tensor}\")\nprint(f\"New shape (8x2): \\n{tensor.reshape(8, 2)}\")\n\nOriginal Tensor:\n tensor([[2., 2., 2., 2.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\nFirst Row: tensor([2., 2., 2., 2.])\nFirst Column: tensor([2., 1., 1., 1.])\nSub-tensor (2x2):\n tensor([[1., 1.],\n        [1., 1.]])\n\nConcatenated Tensor (dim=1):\n tensor([[2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n# 算术运算\n# 矩阵乘法\nmat_mul = tensor.matmul(tensor.T) # tensor.T 是转置\n# 逐元素乘法\nelem_mul = tensor.mul(tensor)\n\nprint(f\"Matrix Multiplication:\\n {mat_mul}\\n\")\nprint(f\"Element-wise Multiplication:\\n {elem_mul}\\n\")\n\n# 数学函数\nsum_val = tensor.sum()\nmean_val = tensor.mean()\n# .item() 用于从只包含一个元素的张量中提取 Python 数值\nprint(f\"Sum of all elements: {sum_val.item()}\")\nprint(f\"Mean of all elements: {mean_val.item()}\")\n\nMatrix Multiplication:\n tensor([[16.,  8.,  8.,  8.],\n        [ 8.,  4.,  4.,  4.],\n        [ 8.,  4.,  4.,  4.],\n        [ 8.,  4.,  4.,  4.]])\n\nElement-wise Multiplication:\n tensor([[4., 4., 4., 4.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\nSum of all elements: 20.0\nMean of all elements: 1.25\n# 变形 (Reshaping)\n# view() 和 reshape() 功能类似，view() 更高效但对内存连续性有要求\n# reshape() 更灵活\nflat_tensor = tensor.reshape(-1) # -1 表示自动计算该维度的大小\nprint(f\"Flattened tensor: {flat_tensor}\")\nprint(f\"New shape (8x2): \\n{tensor.reshape(8, 2)}\")\n\nFlattened tensor: tensor([2., 2., 2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\nNew shape (8x2): \ntensor([[2., 2.],\n        [2., 2.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])\n5. 与 NumPy 的交互PyTorch 与 NumPy 之间的转换非常高效，因为当张量在 CPU 上时，它们共享底层的内存地址。\n# Tensor -> NumPy\nnumpy_array = tensor.numpy()\nprint(f\"Type after .numpy(): {type(numpy_array)}\")\n\n# NumPy -> Tensor\nnew_tensor = torch.from_numpy(numpy_array)\nprint(f\"Type after torch.from_numpy(): {type(new_tensor)}\")\n\n# 重点：共享内存\nnp.add(numpy_array, 1, out=numpy_array) # 修改 NumPy 数组\nprint(f\"Original tensor after modifying NumPy array:\\n {tensor}\")\nprint(f\"New tensor after modifying NumPy array:\\n {new_tensor}\")\n# 输出会显示 tensor 和 new_tensor 的值都改变了\n\nType after .numpy(): &lt;class 'numpy.ndarray'&gt;\nType after torch.from_numpy(): &lt;class 'torch.Tensor'&gt;\nOriginal tensor after modifying NumPy array:\n tensor([[3., 3., 3., 3.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\nNew tensor after modifying NumPy array:\n tensor([[3., 3., 3., 3.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n6. GPU 加速这是 PyTorch 相比 NumPy 的巨大优势。要使用 GPU，你需要将张量移动到 GPU 设备上。\n# 1. 检查 GPU 是否可用，并设置设备\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# 2. 创建一个在 CPU 上的张量\ncpu_tensor = torch.ones(2, 2)\nprint(f\"cpu_tensor is on device: {cpu_tensor.device}\")\n\n# 3. 将张量移动到 GPU\n# .to(device) 是推荐的标准做法\ngpu_tensor = cpu_tensor.to(device)\nprint(f\"gpu_tensor is on device: {gpu_tensor.device}\")\n\n# 4. 在 GPU 上执行运算\n# 注意：参与运算的张量必须在同一个设备上！\nresult = gpu_tensor + gpu_tensor\nprint(f\"gpu_tensor + gpu_tensor=\\n{gpu_tensor + gpu_tensor}\")\n\n# 5. 将结果移回 CPU（例如，用于打印或与 NumPy 交互）\nresult_cpu = result.to(\"cpu\")\n# result_cpu.numpy() # 现在可以安全地转换为 NumPy 了\n\n# 尝试在不同设备上的张量进行运算会报错\ncpu_tensor + gpu_tensor # 这行代码会抛出错误\n\nUsing cuda device\ncpu_tensor is on device: cpu\ngpu_tensor is on device: cuda:0\ngpu_tensor + gpu_tensor=\ntensor([[2., 2.],\n        [2., 2.]], device='cuda:0')\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 23\n     19 result_cpu = result.to(\"cpu\")\n     20 # result_cpu.numpy() # 现在可以安全地转换为 NumPy 了\n     21 \n     22 # 尝试在不同设备上的张量进行运算会报错\n---&gt; 23 cpu_tensor + gpu_tensor # 这行代码会抛出错误\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n模块 3：自动求导 (Autograd)torch.autograd 是 PyTorch 的自动求导引擎，它为神经网络的训练提供了动力。\n1. 计算图 (Computational Graph)当你对张量执行任何操作时，PyTorch 都会在后台构建一个计算图。这个图记录了数据（张量）和操作（函数）之间的关系。\n\n前向传播 (Forward Pass): 当你的代码执行时，PyTorch 会记录下所有的操作，构建出这个图。\n反向传播 (Backward Pass): 当你调用 .backward() 时，PyTorch 会沿着这个图反向传播，利用链式法则自动计算出图中每个参数相对于最终输出的梯度。\n\n因为这个图是动态的，所以你可以在每次迭代中使用不同的控制流（如 if, for），PyTorch 依然能正确处理。\n2. requires_grad 属性这是一个布尔值，用于告诉 PyTorch 是否需要追踪对该张量的操作，以便后续计算梯度。\n\nrequires_grad=True: 追踪！这个张量通常是模型的可学习参数（权重 w 和偏置 b）。\nrequires_grad=False: 不追踪。这个张量通常是模型的输入数据、标签或固定的参数。\n\n# 模型参数，需要计算梯度\nw = torch.randn(1, requires_grad=True)\nb = torch.randn(1, requires_grad=True)\n\n# 输入数据，不需要计算梯度\nx = torch.tensor([2.0]) \n\n# 前向传播，构建计算图\ny = w * x + b # y 会自动获得 requires_grad=True\n\nprint(f\"w.requires_grad: {w.requires_grad}\")\nprint(f\"x.requires_grad: {x.requires_grad}\")\nprint(f\"y.requires_grad: {y.requires_grad}\")\n\nw.requires_grad: True\nx.requires_grad: False\ny.requires_grad: True\n3. backward() 方法当你计算出损失（loss，一个标量）后，调用 loss.backward()，autograd 引擎就会自动完成所有梯度计算。\n\n梯度值会被计算并累加到各个参数张量的 .grad 属性中。\n注意: backward() 只能对标量（0D 张量）调用。\n\n# 假设我们有一个损失值\nloss = (y - 5.0) ** 2 # 假设目标值是 5.0\n\n# 反向传播，计算梯度\nloss.backward()\n\n# 查看梯度\n# d(loss)/dw 和 d(loss)/db\nprint(f\"Gradient for w: {w.grad}\")\nprint(f\"Gradient for b: {b.grad}\")\n\n# 注意：梯度是累加的！\n# 如果再执行一次 backward，梯度会翻倍\n# loss.backward() \n# print(f\"Gradient for w after second backward: {w.grad}\")\n# 这就是为什么在训练循环中每次都要清零梯度\n\n执行一次：\nGradient for w: tensor([-17.9509])\nGradient for b: tensor([-8.9754])\n执行两次：\nRuntimeError: Trying to backward through the graph a second time (or directly \naccess saved tensors after they have already been freed). Saved intermediate \nvalues of the graph are freed when you call .backward() or autograd.grad(). \nSpecify retain_graph=True if you need to backward through the graph a second time \nor if you need to access saved tensors after calling backward.\n4. torch.no_grad()在某些情况下，比如模型评估或更新权重时，我们不希望 PyTorch 追踪操作。使用 torch.no_grad() 上下文管理器可以临时禁用梯度追踪。\n好处:\n\n节省内存: 不会存储中间计算结果用于反向传播。\n加快速度: 避免了构建计算图的开销。\n\nprint(f\"w's grad before no_grad: {w.requires_grad}\")\n\nwith torch.no_grad():\n    # 在这个代码块内，所有操作都不会被追踪\n    new_y = w * x + b\n    print(f\"new_y's grad inside no_grad block: {new_y.requires_grad}\")\n\nprint(f\"w's grad after no_grad: {w.requires_grad}\") # w 本身的属性不受影响\n\nw's grad before no_grad: True\nnew_y's grad inside no_grad block: False\nw's grad after no_grad: True\n5. 动手实践：手动实现线性回归让我们用目前所学的所有知识，来解决一个最简单的机器学习问题：y = w * x + b。\nimport torch\n\n# 1. 准备数据\nX_train = torch.tensor([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n                        [9.779], [6.182], [7.59], [2.167], [7.042], \n                        [10.791], [5.313], [7.997], [3.1]], dtype=torch.float32)\n\ny_train = torch.tensor([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n                        [3.366], [2.596], [2.53], [1.221], [2.827], \n                        [3.465], [1.65], [2.904], [1.3]], dtype=torch.float32)\n\n# 2. 初始化参数\n# 我们希望 PyTorch 帮我们找到最优的 w 和 b，所以设置 requires_grad=True\nw = torch.randn(1, requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\n# 3. 设置超参数\nlearning_rate = 0.01\nepochs = 100\n\n# 4. 训练循环\nfor epoch in range(epochs):\n    # a. 前向传播：计算预测值\n    y_pred = X_train.matmul(w) + b\n\n    # b. 计算损失 (MSE - 均方误差)\n    loss = torch.mean((y_pred - y_train) ** 2)\n\n    # c. 反向传播：计算梯度\n    # 在调用 backward() 前，必须清零上一轮的梯度\n    if w.grad is not None:\n        w.grad.zero_()\n    if b.grad is not None:\n        b.grad.zero_()\n\n    loss.backward()\n\n    # d. 更新权重：手动执行梯度下降\n    # 这里我们不希望更新权重的操作被追踪，所以使用 no_grad()\n    with torch.no_grad():\n        w -= learning_rate * w.grad\n        b -= learning_rate * b.grad\n\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n# 5. 查看学习到的参数\nprint(\"\\nTraining finished!\")\nprint(f\"Learned w: {w.item():.3f}\")\nprint(f\"Learned b: {b.item():.3f}\")\n\n# 真实世界的 w 约等于 0.25, b 约等于 0.75。你的结果应该很接近了\n\nEpoch [10/100], Loss: 1.1384\nEpoch [20/100], Loss: 1.1086\nEpoch [30/100], Loss: 1.0803\nEpoch [40/100], Loss: 1.0534\nEpoch [50/100], Loss: 1.0279\nEpoch [60/100], Loss: 1.0036\nEpoch [70/100], Loss: 0.9805\nEpoch [80/100], Loss: 0.9587\nEpoch [90/100], Loss: 0.9379\nEpoch [100/100], Loss: 0.9181\n\nTraining finished!\nLearned w: 0.242\nLearned b: 0.639\n这个简单的例子完美地展示了 Autograd 的作用：我们只定义了前向传播和损失函数，PyTorch 就自动为我们处理了复杂的梯度计算，让我们能专注于模型的设计和训练过程。\n","slug":"PyTorch学习-1-基础入门","date":"2025-07-07T13:25:37.000Z","categories_index":"python语法学习","tags_index":"PyTorch","author_index":"犬夜叉"},{"id":"b34a2d20ebaf2f1ab87598c5d48f72ca","title":"NumPy 学习笔记：第二阶段 - 核心技能","content":"NumPy 学习笔记：第二阶段 - 核心技能5. 数组的数学运算掌握 NumPy 的数学运算是利用其强大性能的关键。核心思想是向量化，即直接对整个数组进行操作，而不是使用 Python 的 for 循环来遍历每个元素。这不仅代码更简洁，执行效率也天差地别。\n5.1 元素级运算 (Element-wise Operations)元素级运算是指对数组中的每一个元素独立地执行相同的操作。当两个数组形状相同时，算术运算符和通用函数（ufunc）会自动执行元素级运算。\na) 算术运算\n你可以像操作普通数字一样，对 NumPy 数组使用标准的算术运算符。\nimport numpy as np\n\n# 创建两个形状相同的数组\na = np.array([[1, 2, 3], [4, 5, 6]])\nb = np.array([[10, 11, 12], [13, 14, 15]])\n\nprint(f\"数组 a:\\n{a}\\n\")\nprint(f\"数组 b:\\n{b}\\n\")\n\n# 元素级加法\nprint(f\"a + b:\\n{a + b}\\n\")\n\n# 元素级减法\nprint(f\"b - a:\\n{b - a}\\n\")\n\n# 元素级乘法\nprint(f\"a * b:\\n{a * b}\\n\")\n\n# 元素级除法\nprint(f\"b / a:\\n{b / a}\\n\")\n\n# 元素级幂运算\nprint(f\"a ** 2:\\n{a ** 2}\\n\")\n\n输出：\n数组 a:\n[[1 2 3]\n [4 5 6]]\n\n数组 b:\n[[10 11 12]\n [13 14 15]]\n\na + b:\n[[11 13 15]\n [17 19 21]]\n\nb - a:\n[[9 9 9]\n [9 9 9]]\n\na * b:\n[[10 22 36]\n [52 70 90]]\n\nb / a:\n[[10.    5.5   4.  ]\n [ 3.25  2.8   2.5 ]]\n\na ** 2:\n[[ 1  4  9]\n [16 25 36]]\nb) 通用函数 (Universal Functions, ufunc)\nNumPy 提供了大量优化的数学函数，它们也以元素级的方式作用于数组。\n包括np.sqrt, np.exp, np.sin, np.log等等\narr = np.array([1, 4, 9, 16])\nprint(f\"原始数组: {arr}\\n\")\n\n# 计算每个元素的平方根\nprint(f\"平方根 (np.sqrt): {np.sqrt(arr)}\")\n\n# 计算每个元素的指数 (e^x)\nprint(f\"指数 (np.exp): {np.exp(arr)}\")\n\n# 计算每个元素的正弦值\ntrig_arr = np.array([0, np.pi/2, np.pi])\nprint(f\"三角函数数组: {trig_arr}\")\nprint(f\"正弦值 (np.sin): {np.sin(trig_arr)}\")\n\n# 计算每个元素的自然对数\nlog_arr = np.array([1, np.e, 10]) # np.e 是自然常数 e\nprint(f\"\\n对数数组: {log_arr}\")\nprint(f\"自然对数 (np.log): {np.log(log_arr)}\")\n\n输出：\n原始数组: [ 1  4  9 16]\n\n平方根 (np.sqrt): [1. 2. 3. 4.]\n指数 (np.exp): [2.71828183e+00 5.45981500e+01 8.10308393e+03 8.88611052e+06]\n三角函数数组: [0.         1.57079633 3.14159265]\n正弦值 (np.sin): [0.0000000e+00 1.0000000e+00 1.2246468e-16]\n\n对数数组: [ 1.          2.71828183 10.        ]\n自然对数 (np.log): [0.         1.         2.30258509]\n5.2 聚合运算 (Aggregation)聚合运算是指对一组值（例如整个数组或数组的某一行/列）执行计算，最终返回一个单一值的操作。\na) 对整个数组进行聚合\ndata = np.array([[1, 2, 3], [4, 5, 6]])\nprint(f\"数据数组:\\n{data}\\n\")\n\n# 计算所有元素的总和\nprint(f\"总和 (data.sum()): {data.sum()}\") # 等价于 np.sum(data)\n\n# 计算所有元素的最小值\nprint(f\"最小值 (data.min()): {data.min()}\")\n\n# 计算所有元素的最大值\nprint(f\"最大值 (data.max()): {data.max()}\")\n\n# 计算所有元素的平均值\nprint(f\"平均值 (data.mean()): {data.mean()}\")\n\n# 计算所有元素的标准差\nprint(f\"标准差 (data.std()): {data.std()}\")\n\n输出：\n数据数组:\n[[1 2 3]\n [4 5 6]]\n\n总和 (data.sum()): 21\n最小值 (data.min()): 1\n最大值 (data.max()): 6\n平均值 (data.mean()): 3.5\n标准差 (data.std()): 1.707825127659933\nb) 指定轴向的聚合 (Axis-specific Aggregation)\n这是 NumPy 中一个极其重要的概念。通过指定 axis 参数，你可以沿着数组的特定维度（轴）进行聚合。\n\naxis=0: 沿着行的方向进行操作，可以理解为“压缩行”，对每一列进行聚合。\naxis=1: 沿着列的方向进行操作，可以理解为“压缩列”，对每一行进行聚合。\n\n图解:对于一个 2x3 的数组 [[a, b, c], [d, e, f]]：\n\nsum(axis=0) 的结果是 [a+d, b+e, c+f] (形状变为 (3,))\nsum(axis=1) 的结果是 [a+b+c, d+e+f] (形状变为 (2,))\n\ndata = np.array([[1, 2, 3], [4, 5, 6]])\nprint(f\"数据数组:\\n{data}\\n\")\n\n# 沿 axis=0 (对每一列) 求和\ncol_sums = data.sum(axis=0)\nprint(f\"沿 axis=0 求和 (各列之和): {col_sums}\")\nprint(f\"结果形状: {col_sums.shape}\\n\")\n\n# 沿 axis=1 (对每一行) 求和\nrow_sums = data.sum(axis=1)\nprint(f\"沿 axis=1 求和 (各行之和): {row_sums}\")\nprint(f\"结果形状: {row_sums.shape}\")\n\n输出：\n数据数组:\n[[1 2 3]\n [4 5 6]]\n\n沿 axis=0 求和 (各列之和): [5 7 9]\n结果形状: (3,)\n\n沿 axis=1 求和 (各行之和): [ 6 15]\n结果形状: (2,)\n6. 广播 (Broadcasting)广播是 NumPy 最强大也最核心的功能之一。它描述了 NumPy 在处理形状不同的数组时如何进行算术运算，而无需显式地复制数据。这使得代码更简洁，内存使用更高效。\n6.1 广播的核心规则当对两个数组进行操作时，NumPy 会逐个比较它们的维度（从末尾开始）。要满足广播条件，必须符合以下两个规则：\n\n规则一: 如果两个数组的维度数不同，那么在维度较小的数组的形状前面补 1，直到它们的维度数相同。\n规则二: 在任何一个维度上，如果两个数组的该维度大小相同，或者其中一个数组的大小为 1，那么它们在该维度上是兼容的。\n\n如果这两个规则都满足，就可以进行广播。计算结果的每个维度的大小是输入数组在该维度上的最大值。如果不满足，NumPy 会抛出 ValueError。\n6.2 广播的实际应用情况一：数组与标量（单个数值）的运算这是最简单也最常见的广播。\narr = np.array([[1, 2, 3], [4, 5, 6]])\nscalar = 10\n\n# 数组与标量相加\nresult = arr + scalar\nprint(f\"数组 arr (shape: {arr.shape}):\\n{arr}\\n\")\nprint(f\"标量 scalar: {scalar}\\n\")\nprint(f\"广播结果 (shape: {result.shape}):\\n{result}\")\n\n输出：\n数组 arr (shape: (2, 3)):\n[[1 2 3]\n [4 5 6]]\n\n标量 scalar: 10\n\n广播结果 (shape: (2, 3)):\n[[11 12 13]\n [14 15 16]]\n广播过程:\n\narr 的形状是 (2, 3)。scalar 可以看作形状为 () 的数组。\n规则一: 将 scalar 的形状补齐为 (1, 1)，再补齐为 (2, 3) 以匹配 arr。\n规则二: 在每个维度上，将大小为 1 的维度“拉伸”以匹配另一个数组。最终 scalar 被广播成 [[10, 10, 10], [10, 10, 10]]，然后与 arr 进行元素级相加。\n\n情况二：二维数组与一维数组的运算\nmatrix = np.arange(12).reshape(3, 4) # 形状 (3, 4)\nvector = np.array([10, 20, 30, 40])  # 形状 (4,)\n\n# 将 vector 加到 matrix 的每一行\nresult = matrix + vector\nprint(f\"矩阵 matrix (shape: {matrix.shape}):\\n{matrix}\\n\")\nprint(f\"向量 vector (shape: {vector.shape}): {vector}\\n\")\nprint(f\"广播结果 (shape: {result.shape}):\\n{result}\")\n\n输出：\n矩阵 matrix (shape: (3, 4)):\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n\n向量 vector (shape: (4,)): [10 20 30 40]\n\n广播结果 (shape: (3, 4)):\n[[10 21 32 43]\n [14 25 36 47]\n [18 29 40 51]]\n广播过程:\n\nmatrix 形状 (3, 4)，vector 形状 (4,)。\n规则一: 给 vector 的形状前面补 1，变为 (1, 4)。\n规则二: 现在比较 (3, 4) 和 (1, 4)。\n末尾维度：大小都是 4，兼容。\n第一个维度：大小分别为 3 和 1，兼容。\n\n\nNumPy 将 vector (形状 (1, 4)) 沿着大小为 1 的轴（axis=0）“拉伸”3次，使其行为与形状为 (3, 4) 的数组一致，然后与 matrix 相加。\n\n情况三：两个不同形状的数组假设我们要给一个列向量加上一个行向量。\ncol_vector = np.array([[10], [20], [30]]) # 形状 (3, 1)\nrow_vector = np.array([1, 2, 3, 4])      # 形状 (4,)，广播时视为 (1, 4)\n\nresult = col_vector + row_vector\nprint(f\"列向量 (shape: {col_vector.shape}):\\n{col_vector}\\n\")\nprint(f\"行向量 (shape: {row_vector.shape}): {row_vector}\\n\")\nprint(f\"广播结果 (shape: {result.shape}):\\n{result}\")\n\n输出：\n列向量 (shape: (3, 1)):\n[[10]\n [20]\n [30]]\n\n行向量 (shape: (4,)): [1 2 3 4]\n\n广播结果 (shape: (3, 4)):\n[[11 12 13 14]\n [21 22 23 24]\n [31 32 33 34]]\n广播过程:\n\ncol_vector 形状 (3, 1)，row_vector 形状 (4,)。\n规则一: row_vector 形状补齐为 (1, 4)。\n规则二: 现在比较 (3, 1) 和 (1, 4)。\n末尾维度：大小分别为 1 和 4，兼容。\n第一个维度：大小分别为 3 和 1，兼容。\n\n\n结果数组的形状是 (max(3, 1), max(1, 4))，即 (3, 4)。col_vector 沿着 axis=1 广播，row_vector 沿着 axis=0 广播，最终两者都被扩展成 (3, 4) 的形状进行相加。\n\n广播失败的例子\na = np.array([[1, 2, 3]]) # 形状 (1, 3)\nb = np.array([[4, 5]])    # 形状 (1, 2)\n\ntry:\n    c = a + b\nexcept ValueError as e:\n    print(f\"广播失败，错误信息: {e}\")\n\n输出：\n广播失败，错误信息: operands could not be broadcast together with shapes (1,3) (1,2)\n原因: 比较 (1, 3) 和 (1, 2)。末尾维度的大小分别为 3 和 2，既不相等，也没有一个是 1，因此不满足广播规则。\n7. 高级索引 (Advanced Indexing)当基本的切片（如 arr[0:5]）无法满足复杂的选取需求时，高级索引就派上了用场。它提供了基于条件或索引列表来访问和修改数据的强大能力。\n7.1 布尔索引 (Boolean Indexing)布尔索引允许我们使用一个布尔类型的数组（通常称为掩码 (mask)）来筛选另一个数组中的元素。这在数据清洗和筛选中极为常用。\n核心思想： 创建一个与原数组形状相同的布尔数组，其中 True 值对应的位置就是我们希望选取的元素。\nimport numpy as np\n\n# 创建一个示例数组\ndata = np.arange(12).reshape(3, 4)\nprint(f\"原始数组:\\n{data}\\n\")\n\n# 1. 创建一个简单的布尔掩码\n# 条件：选取所有大于 5 的元素\nmask = data > 5\nprint(f\"布尔掩码 (data > 5):\\n{mask}\\n\")\n\n# 2. 使用掩码进行索引\n# 这会返回一个一维数组，其中只包含所有满足条件的元素\nselected_data = data[mask]\nprint(f\"使用掩码选取的结果: {selected_data}\")\nprint(\"注意：布尔索引返回的是数据的副本 (Copy)，而不是视图。\\n\")\n\n# 3. 结合逻辑运算符进行复杂筛选\n# 条件：选取大于 3 且小于 8 的元素\n# 注意：必须使用 & (与), | (或), ~ (非) 位运算符，且每个条件要用括号括起来\ncomplex_mask = (data > 3) & (data  3) & (data \n输出：\n原始数组:\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n\n布尔掩码 (data &gt; 5):\n[[False False False False]\n [False False  True  True]\n [ True  True  True  True]]\n\n使用掩码选取的结果: [ 6  7  8  9 10 11]\n注意：布尔索引返回的是数据的副本 (Copy)，而不是视图。\n\n复杂条件掩码 ((data &gt; 3) &amp; (data &lt; 8)):\n[[False False False False]\n [ True  True  True  True]\n [False False False False]]\n\n复杂筛选结果: [4 5 6 7]\n\n将所有偶数替换为 -1 后的数组:\n[[-1  1 -1  3]\n [-1  5 -1  7]\n [-1  9 -1 11]]\n7.2 花式索引 (Fancy Indexing)花式索引使用一个整数数组或列表来指定要访问的元素的索引。这使得我们可以选取任意不连续的元素、重复元素或以任意顺序排列元素。\n重要概念： 花式索引总是返回数据的副本 (Copy)，而不是视图。\n# 创建一个示例数组\narr = np.arange(10) * 10  # [ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\nprint(f\"原始一维数组: {arr}\\n\")\n\n# 1. 在一维数组中使用花式索引\n# 选取索引为 1, 3, 8 的元素\nindices = [1, 3, 8]\nprint(f\"选取索引为 [1, 3, 8] 的元素: {arr[indices]}\\n\")\n\n# 2. 在多维数组中使用花式索引\nmatrix = np.arange(9).reshape(3, 3)\nprint(f\"原始二维数组:\\n{matrix}\\n\")\n\n# 选取特定的行，例如第 0 行和第 2 行\nselected_rows = matrix[[0, 2]]\nprint(f\"选取第 0 行和第 2 行:\\n{selected_rows}\\n\")\n\n# 选取特定的点 (行和列索引配对)\n# 选取 (0, 1), (1, 2), (2, 0) 三个位置的元素\nrows = [0, 1, 2]\ncols = [1, 2, 0]\nselected_points = matrix[rows, cols]\nprint(f\"选取点 (0,1), (1,2), (2,0): {selected_points}\\n\")\n\n# 3. 使用花式索引进行赋值\n# 将索引为 1, 3, 8 的元素修改为 -99\narr[[1, 3, 8]] = -99\nprint(f\"使用花式索引赋值后: {arr}\")\n\n输出：\n原始一维数组: [ 0 10 20 30 40 50 60 70 80 90]\n\n选取索引为 [1, 3, 8] 的元素: [10 30 80]\n\n原始二维数组:\n[[0 1 2]\n [3 4 5]\n [6 7 8]]\n\n选取第 0 行和第 2 行:\n[[0 1 2]\n [6 7 8]]\n\n选取点 (0,1), (1,2), (2,0): [1 5 6]\n\n使用花式索引赋值后: [  0 -99  20 -99  40  50  60  70 -99  90]\n8. 数组形状操作在数据预处理和模型输入准备中，我们经常需要改变数组的维度和结构，而 NumPy 提供了多种高效的工具来完成这些任务。\n8.1 reshape(): 改变形状reshape() 在不改变原始数据的情况下，返回一个具有新形状的数组。\n\n新形状的元素总数必须与原形状的元素总数相同。\nreshape() 尽可能返回一个视图 (View)，以避免数据复制，提高效率。\n可以使用 -1 作为维度参数，让 NumPy 自动计算该维度的大小。\n\na = np.arange(12)\nprint(f\"原始数组 (shape: {a.shape}): {a}\\n\")\n\n# 重塑为 3x4 的矩阵\nb = a.reshape(3, 4)\nprint(f\"重塑后的 3x4 矩阵:\\n{b}\\n\")\n\n# 使用 -1 自动计算维度\n# 原始数组有 12 个元素，指定 2 列，行数自动计算为 6\nc = a.reshape(-1, 2)\nprint(f\"自动计算行数 (-1, 2):\\n{c}\")\n\n输出：\n原始数组 (shape: (12,)): [ 0  1  2  3  4  5  6  7  8  9 10 11]\n\n重塑后的 3x4 矩阵:\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n\n自动计算行数 (-1, 2):\n[[ 0  1]\n [ 2  3]\n [ 4  5]\n [ 6  7]\n [ 8  9]\n [10 11]]\n8.2 ravel() 与 flatten(): 数组展平这两个函数都能将多维数组转换为一维数组，但有一个关键区别：\n\nravel(): “散开”，尽可能返回一个视图 (View)，修改它可能会影响原始数组。\nflatten(): “压平”，总是返回一个副本 (Copy)，修改它绝不会影响原始数组。\n\nmatrix = np.arange(6).reshape(2, 3)\nprint(f\"原始矩阵:\\n{matrix}\\n\")\n\n# 使用 ravel()\nraveled_arr = matrix.ravel()\nraveled_arr[0] = 100 # 修改视图\nprint(f\"Ravel 展平结果: {raveled_arr}\")\nprint(f\"原始矩阵被修改了:\\n{matrix}\\n\")\n\n# 恢复原始矩阵\nmatrix[0, 0] = 0\n\n# 使用 flatten()\nflattened_arr = matrix.flatten()\nflattened_arr[0] = 200 # 修改副本\nprint(f\"Flatten 展平结果: {flattened_arr}\")\nprint(f\"原始矩阵未被修改:\\n{matrix}\")\n\n输出：\n原始矩阵:\n[[0 1 2]\n [3 4 5]]\n\nRavel 展平结果: [100   1   2   3   4   5]\n原始矩阵被修改了:\n[[100   1   2]\n [  3   4   5]]\n\nFlatten 展平结果: [200   1   2   3   4   5]\n原始矩阵未被修改:\n[[0 1 2]\n [3 4 5]]\n选择建议： 如果不希望修改原数组，或者不确定后续操作，使用 flatten() 更安全。如果注重性能且清楚自己在做什么，可以使用 ravel()。\n8.3 transpose() 或 .T 属性: 数组转置转置操作会交换数组的轴。对于二维数组，就是交换行和列。转置返回的是一个视图 (View)。\nmatrix = np.arange(6).reshape(2, 3)\nprint(f\"原始矩阵 (2x3):\\n{matrix}\\n\")\n\n# 使用 .T 属性进行转置\ntransposed_matrix = matrix.T\nprint(f\"转置后的矩阵 (3x2):\\n{transposed_matrix}\")\n\n输出：\n原始矩阵 (2x3):\n[[0 1 2]\n [3 4 5]]\n\n转置后的矩阵 (3x2):\n[[0 3]\n [1 4]\n [2 5]]\n8.4 resize(): 就地修改形状resize() 直接在原地 (in-place) 修改数组的形状，并且不要求新旧形状的元素总数一致。\n\n如果新尺寸更大，会用原数组的重复内容来填充。\n如果新尺寸更小，会截断原数组。\n\na = np.arange(4)\nprint(f\"原始数组: {a}\\n\")\n\n# resize 会直接修改 a，没有返回值\na.resize(2, 3)\nprint(f\"Resize 到 (2, 3) 后 (用 0 填充，因为没有足够元素重复):\\n{a}\")\n# 注意：如果 NumPy 认为有其他变量引用了该内存，可能会报错。\n# np.resize(array, new_shape) 是一个更安全的选择，它返回副本。\n\n输出：\n原始数组: [0 1 2 3]\n\nResize 到 (2, 3) 后 (用 0 填充，因为没有足够元素重复):\n[[0 1 2]\n [3 0 0]]\n8.5 数组的拼接与分割a) 拼接 (Concatenation)\n\nnp.concatenate((a, b, ...), axis=...): 沿指定轴连接一系列数组。\nnp.vstack((a, b)): 垂直堆叠 (Vertical Stack)，等价于 concatenate 的 axis=0。\nnp.hstack((a, b)): 水平堆叠 (Horizontal Stack)，等价于 concatenate 的 axis=1。\n\na = np.array([[1, 2], [3, 4]])\nb = np.array([[5, 6]])\nprint(f\"数组 a:\\n{a}\\n数组 b:\\n{b}\\n\")\n\n# 垂直拼接 (axis=0)\nv_stacked = np.vstack((a, b))\nprint(f\"垂直拼接 vstack (axis=0):\\n{v_stacked}\\n\")\n\n# 水平拼接 (需要 b 的形状匹配)\nc = np.array([[5], [6]])\nh_stacked = np.hstack((a, c))\nprint(f\"数组 c:\\n{c}\\n水平拼接 hstack (axis=1):\\n{h_stacked}\\n\")\n\n输出：\n数组 a:\n[[1 2]\n [3 4]]\n数组 b:\n[[5 6]]\n\n垂直拼接 vstack (axis=0):\n[[1 2]\n [3 4]\n [5 6]]\n\n数组 c:\n[[5]\n [6]]\n水平拼接 hstack (axis=1):\n[[1 2 5]\n [3 4 6]]\nb) 分割 (Splitting)\n\nnp.split(arr, N, axis=...): 将数组沿指定轴平均分割成 N份。\nnp.split(arr, [idx1, idx2, ...], axis=...): 在指定的索引位置进行分割。\nnp.vsplit(arr, N): 垂直分割，等价于 split 的 axis=0。\nnp.hsplit(arr, N): 水平分割，等价于 split 的 axis=1。\n\narr = np.arange(12).reshape(4, 3)\nprint(f\"待分割的数组:\\n{arr}\\n\")\n\n# 垂直分割成 2 份\nparts = np.vsplit(arr, 2)\nprint(f\"垂直分割成 2 份:\\n第一部分:\\n{parts[0]}\\n第二部分:\\n{parts[1]}\\n\")\n\n# 水平分割成 3 份\nparts_h = np.hsplit(arr, 3)\nprint(f\"水平分割成 3 份:\\n第一部分:\\n{parts_h[0]}\\n第二部分:\\n{parts_h[1]}\")\n\n输出：\n待分割的数组:\n[[ 0  1  2]\n [ 3  4  5]\n [ 6  7  8]\n [ 9 10 11]]\n\n垂直分割成 2 份:\n第一部分:\n[[0 1 2]\n [3 4 5]]\n第二部分:\n[[ 6  7  8]\n [ 9 10 11]]\n\n水平分割成 3 份:\n第一部分:\n[[0]\n [3]\n [6]\n [9]]\n第二部分:\n[[ 1]\n [ 4]\n [ 7]\n [10]]\n","slug":"NumPy-学习笔记：第二阶段-核心技能","date":"2025-07-07T09:23:26.000Z","categories_index":"python语法学习","tags_index":"python,Numpy","author_index":"犬夜叉"},{"id":"ba03d7c748f55c669d509a2713c39e98","title":"NumPy 学习笔记：第一阶段 - 入门基础","content":"NumPy 学习笔记：第一阶段 - 入门基础1. 准备工作与基本概念1.1 NumPy 是什么？NumPy（Numerical Python 的缩写）是 Python 语言的一个开源扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。它是几乎所有 Python 数据科学和机器学习库（如 Pandas, SciPy, Matplotlib, Scikit-learn）的底层核心。\n1.2 为什么选择 NumPy 而不是 Python 列表？虽然 Python 内置的列表（List）可以存储数据，但在进行大规模数值计算时，NumPy 的 ndarray 对象具有压倒性的优势：\n\n性能 (Performance): NumPy 数组在底层是使用 C 语言实现的，这使得其在处理数值运算时比纯 Python 代码快几个数量级。它避免了 Python 循环中的类型检查和其它开销，实现了所谓的“向量化”计算。\n内存效率 (Memory Efficiency): 由于 NumPy 数组中的元素类型是固定的（dtype），它在内存中以紧凑、连续的方式存储数据，相比 Python 列表（需要为每个元素存储类型信息和引用），占用的内存要小得多。\n便捷的数学运算 (Convenience): NumPy 提供了大量为数组设计的数学函数和操作。你可以对整个数组执行数学运算，而无需编写复杂的循环。例如，两个数组可以直接相加，这在 Python 列表中是无法直接做到的。\n\n1.3 安装与导入首先，确保已经安装了 NumPy。如果尚未安装，可以使用 pip 进行安装：\npip install numpy\n\n在 Python 脚本或 Jupyter Notebook 中，我们通常使用 np 作为 NumPy 的标准别名来导入它，这是一种广泛遵循的社区惯例。\nimport numpy as np\n\n2. NumPy 核心：ndarray 对象ndarray（N-dimensional array，N维数组）是 NumPy 的核心数据结构。它是一个多维的、包含相同类型元素的数组。\n让我们创建一个简单的数组，并查看它的核心属性：\n# 创建一个 2x3 的二维数组 (2行3列)\narr = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(\"数组内容:\\n\", arr)\nprint(\"-\" * 20)\n\n# 1. ndim: 数组的维度（轴）数\nprint(f\"维度数 (ndim): {arr.ndim}\")\n\n# 2. shape: 数组的维度，返回一个元组\nprint(f\"形状 (shape): {arr.shape}\")\n\n# 3. size: 数组中元素的总数\nprint(f\"总元素数 (size): {arr.size}\")\n\n# 4. dtype: 数组中元素的数据类型\nprint(f\"数据类型 (dtype): {arr.dtype}\")\n\n# 5. itemsize: 每个元素的字节大小\nprint(f\"每个元素的字节大小 (itemsize): {arr.itemsize}\")\n\n# 6. data: 包含数组实际元素的内存缓冲区（通常我们不直接操作它）\nprint(f\"内存缓冲区地址 (data): {arr.data}\")\n\n输出:\n数组内容:\n [[1 2 3]\n [4 5 6]]\n--------------------\n维度数 (ndim): 2\n形状 (shape): (2, 3)\n总元素数 (size): 6\n数据类型 (dtype): int64\n每个元素的字节大小 (itemsize): 8\n内存缓冲区地址 (data): &lt;memory at 0x000001F0BF600AD0&gt;\n3. 创建数组 (Array Creation)掌握创建数组的多种方法是使用 NumPy 的基础。\n3.1 从 Python 列表或元组创建np.array() 是最基础的创建方式。\n# 从列表创建\nlist_data = [1, 2, 3.5, 4]\narr_from_list = np.array(list_data)\nprint(f\"从列表创建的数组: {arr_from_list}, 类型: {arr_from_list.dtype}\")\n\n# 从嵌套列表创建二维数组\nnested_list = [[1, 2], [3, 4]]\narr_2d = np.array(nested_list)\nprint(f\"创建的二维数组:\\n{arr_2d}\")\n\n输出：\n从列表创建的数组: [1.  2.  3.5 4. ], 类型: float64\n创建的二维数组:\n[[1 2]\n [3 4]]\n注意: NumPy 会自动推断最合适的数据类型（在这个例子中是 float64，因为列表中有 3.5）。\n3.2 使用内置函数创建特定数组这些函数非常适合用于创建已知形状和值的数组。\n# 创建一个 3x4 的全零数组\nzeros_arr = np.zeros((3, 4))\nprint(f\"全零数组:\\n{zeros_arr}\\n\")\n\n# 创建一个 2x3 的全一数组，并指定数据类型为整数\nones_arr = np.ones((2, 3), dtype=np.int16)\nprint(f\"全一数组:\\n{ones_arr}\\n\")\n\n# 创建一个 2x2 的空数组（内容是未初始化的，取决于内存状态）\n# 注意：empty 并不安全，通常用于性能要求极高的场景\nempty_arr = np.empty((2, 2))\nprint(f\"空数组 (内容随机):\\n{empty_arr}\\n\")\n\n输出：\n全零数组:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n全一数组:\n[[1 1 1]\n [1 1 1]]\n\n空数组 (内容随机):\n[[1.  2. ]\n [3.5 4. ]]\n3.3 创建序列数组# np.arange(start, stop, step)\n# 创建一个从 0 到 9 的数组\nrange_arr = np.arange(10)\nprint(f\"arange(10): {range_arr}\\n\")\n\n# 创建一个从 2 到 10，步长为 2 的数组\nrange_step_arr = np.arange(2, 11, 2)\nprint(f\"arange(2, 11, 2): {range_step_arr}\\n\")\n\n# np.linspace(start, stop, num)\n# 在 0 和 1 之间创建 5 个等差的数\nlinspace_arr = np.linspace(0, 1, 5)\nprint(f\"linspace(0, 1, 5): {linspace_arr}\\n\")\n\n# np.logspace(start, stop, num)\n# 在 10^0 和 10^2 之间创建 3 个等比的数\nlogspace_arr = np.logspace(0, 2, 3) # 结果是 [10^0, 10^1, 10^2]\nprint(f\"logspace(0, 2, 3): {logspace_arr}\\n\")\n\n输出：\narange(10): [0 1 2 3 4 5 6 7 8 9]\n\narange(2, 11, 2): [ 2  4  6  8 10]\n\nlinspace(0, 1, 5): [0.   0.25 0.5  0.75 1.  ]\n\nlogspace(0, 2, 3): [  1.  10. 100.]\n3.4 创建随机数组np.random 模块非常实用。\n# 创建一个 2x3 的数组，元素为 [0.0, 1.0) 之间的均匀分布随机数\nrand_arr = np.random.rand(2, 3)\nprint(f\"均匀分布随机数组:\\n{rand_arr}\\n\")\n\n# 创建一个 2x3 的数组，元素为符合标准正态分布（均值为0，方差为1）的随机数\nrandn_arr = np.random.randn(2, 3)\nprint(f\"标准正态分布随机数组:\\n{randn_arr}\\n\")\n\n# 创建一个 3x5 的数组，元素为 [1, 10) 之间的随机整数\nrandint_arr = np.random.randint(1, 10, size=(3, 5))\nprint(f\"随机整数数组:\\n{randint_arr}\\n\")\n\n输出：\n均匀分布随机数组:\n[[0.01137494 0.45945179 0.96286648]\n [0.30871745 0.81260161 0.75919671]]\n\n标准正态分布随机数组:\n[[ 0.59965387 -0.45936121  2.07231621]\n [-0.032432   -1.19755955  0.08568854]]\n\n随机整数数组:\n[[5 6 4 7 4]\n [1 1 5 1 2]\n [8 3 4 4 7]]\n4. 基本索引与切片 (Indexing and Slicing)这是 NumPy 操作中最高频的部分。\n4.1 一维数组与 Python 列表非常相似。\narr1d = np.arange(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(f\"一维数组: {arr1d}\")\n\n# 获取索引为 2 的元素\nprint(f\"元素 arr1d[2]: {arr1d[2]}\")\n\n# 切片：获取索引从 2 到 4 的元素（不包括 5）\nprint(f\"切片 arr1d[2:5]: {arr1d[2:5]}\")\n\n# 修改切片内容\narr1d[2:5] = 100\nprint(f\"修改切片后: {arr1d}\")\n\n输出：\n一维数组: [0 1 2 3 4 5 6 7 8 9]\n元素 arr1d[2]: 2\n切片 arr1d[2:5]: [2 3 4]\n修改切片后: [  0   1 100 100 100   5   6   7   8   9]\n4.2 多维数组使用 [row, col] 的方式进行索引。\narr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(f\"二维数组:\\n{arr2d}\")\n\n# 获取第 1 行，第 2 列的元素（索引从0开始）\nelement = arr2d[1, 2]\nprint(f\"\\n元素 arr2d[1, 2]: {element}\") # 输出 6\n\n# 获取某一行（例如第 0 行）\nrow0 = arr2d[0] # 也可以写成 arr2d[0, :]\nprint(f\"第 0 行: {row0}\")\n\n# 获取某一列（例如第 1 列）\ncol1 = arr2d[:, 1]\nprint(f\"第 1 列: {col1}\")\n\n# 切片获取子数组（第 0, 1 行 和 第 1, 2 列交叉的区域）\nsub_arr = arr2d[:2, 1:]\nprint(f\"子数组 arr2d[:2, 1:]:\\n{sub_arr}\")\n\n输出：\n二维数组:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n元素 arr2d[1, 2]: 6\n第 0 行: [1 2 3]\n第 1 列: [2 5 8]\n子数组 arr2d[:2, 1:]:\n[[2 3]\n [5 6]]\n4.3 重要概念：视图 (View) vs 副本 (Copy)这是 NumPy 的一个核心特性，也是初学者容易出错的地方。对数组的切片操作返回的是原始数组的“视图”，而不是一个全新的副本。 这意味着修改视图会直接影响到原始数组。这样做是为了性能和内存考虑。\n示例：视图 (View)\n# 创建一个原始数组\noriginal_arr = np.arange(5)\nprint(f\"原始数组: {original_arr}\")\n\n# 创建一个切片（视图）\nslice_view = original_arr[2:4]\nprint(f\"切片视图: {slice_view}\")\n\n# 修改视图中的元素\nslice_view[0] = 999\nprint(f\"修改视图后，视图变为: {slice_view}\")\nprint(f\"原始数组也被修改了: {original_arr}\") # 注意看原始数组的变化\n\n输出:\n原始数组: [0 1 2 3 4]\n切片视图: [2 3]\n修改视图后，视图变为: [999   3]\n原始数组也被修改了: [  0   1 999   3   4]\n如果你不希望修改原始数组，就需要显式地创建一个副本 (Copy)。\n示例：副本 (Copy)\n# 创建一个原始数组\noriginal_arr_2 = np.arange(5)\nprint(f\"\\n原始数组 2: {original_arr_2}\")\n\n# 使用 .copy() 方法创建副本\narr_copy = original_arr_2[2:4].copy()\nprint(f\"切片副本: {arr_copy}\")\n\n# 修改副本中的元素\narr_copy[0] = 777\nprint(f\"修改副本后，副本变为: {arr_copy}\")\nprint(f\"原始数组 2 没有改变: {original_arr_2}\") # 原始数组安然无恙\n\n输出:\n原始数组 2: [0 1 2 3 4]\n切片副本: [2 3]\n修改副本后，副本变为: [777   3]\n原始数组 2 没有改变: [0 1 2 3 4]\n小结: 这个入门阶段可以打下坚实的基础。理解 ndarray 的属性、熟练创建各种数组、并掌握索引和切片的用法（特别是视图与副本的区别）是进行后续学习的关键。\n","slug":"NumPy-学习笔记：第一阶段-入门基础","date":"2025-07-07T08:45:52.000Z","categories_index":"python语法学习","tags_index":"python,Numpy","author_index":"犬夜叉"},{"id":"31d49180bc69c8b21b5ea22ba59ea29a","title":"BLIP代码详解（一）","content":"一、项目结构概述BLIP 是一个用于统一视觉语言理解和生成的预训练项目，其代码库结构清晰，功能模块划分明确，以下是对其主要结构的详细介绍：\n1. 根目录文件\n配置与依赖：cog.yaml 用于定义项目的配置，requirements.txt 列出了项目所需的依赖库，通过 pip install -r requirements.txt 即可安装。\n训练与评估脚本：包含多个训练和评估脚本，如 train_caption.py 用于图像文本字幕任务的训练，eval_nocaps.py 用于在 NoCaps 数据集上评估微调后的模型。\n演示文件：demo.ipynb 是一个交互式演示的 Colab 笔记本，无需 GPU 即可运行，展示了图像字幕、视觉问答等功能。\n\n2. 子目录\nconfigs/：存放各种任务的配置文件，如 caption_coco.yaml、retrieval_coco.yaml 等，用于设置模型参数、数据集路径等。\ntransform/：包含randaugment的代码，用于数据增强\nmodels/：包含项目的核心模型代码，如 blip_pretrain.py、blip_nlvr.py 等，定义了不同任务的模型结构和前向传播方法。\ndata/：用于存放数据集或与数据处理相关的代码。\n\n二、models/blip.py详解blip.py实现了多模态模型相关功能。定义BLIP_Base和BLIP_Decoder类，前者可提取图像、文本或多模态特征，后者用于图像描述。还包含分词器初始化、视觉编码器创建、加载预训练模型等辅助函数。 blip.py实现了多模态模型相关功能。\n2.1 BLIP_Base 函数功能介绍BLIP_Base 类是一个用于图像和文本特征提取的 PyTorch 模块。__init__ 方法初始化视觉编码器、分词器和文本编码器，依据 vit 模型大小和图像尺寸创建视觉编码器，加载配置文件设置文本编码器。forward 方法根据 mode 参数执行不同操作：当 mode 为 'image' 时，通过视觉编码器提取图像特征并返回；当 mode 为 'text' 时，通过文本编码器提取文本特征并返回；当 mode 为 'multimodal' 时，结合图像和文本信息，返回多模态特征。 \n具体代码# 定义 BLIP_Base 类，继承自 PyTorch 的 nn.Module 类，用于构建基础的多模态特征提取模型\nclass BLIP_Base(nn.Module):\n    def __init__(self,                 \n                 # 编码器 - 解码器混合模型的配置文件路径，默认为 'configs/med_config.json'\n                 med_config = 'configs/med_config.json',  \n                 # 输入图像的大小，默认为 224\n                 image_size = 224,\n                 # 视觉变换器（Vision Transformer）的模型大小，可选 'base' 或 'large'，默认为 'base'\n                 vit = 'base',\n                 # 是否使用梯度检查点，默认为 False\n                 vit_grad_ckpt = False,\n                 # 梯度检查点的层数，默认为 0\n                 vit_ckpt_layer = 0,                 \n                 ):\n\n        # 调用父类 nn.Module 的构造函数\n        super().__init__()\n\n        # 调用 create_vit 函数创建视觉编码器，并获取视觉编码器的嵌入维度\n        # create_vit 函数根据传入的 vit 型号、图像大小、是否使用梯度检查点等参数创建视觉编码器\n        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n        # 调用 init_tokenizer 函数初始化分词器，用于将文本转换为模型可处理的输入格式\n        self.tokenizer = init_tokenizer()   \n        # 从 JSON 文件中加载 BERT 模型的配置\n        med_config = BertConfig.from_json_file(med_config)\n        # 将 BERT 模型的编码器宽度设置为视觉编码器的嵌入维度\n        med_config.encoder_width = vision_width\n        # 根据配置创建文本编码器，add_pooling_layer=False 表示不使用池化层\n        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)  \n\n    # 定义前向传播方法，根据不同的模式处理图像和文本数据\n    def forward(self, image, caption, mode):\n\n        # 确保 mode 参数为 'image'、'text' 或 'multimodal' 之一，否则抛出异常\n        assert mode in ['image', 'text', 'multimodal'], \"mode parameter must be image, text, or multimodal\"\n        # 使用分词器对输入的文本描述进行分词，并将分词结果转换为 PyTorch 张量\n        # 然后将张量移动到与图像数据相同的设备上（如 GPU 或 CPU）\n        text = self.tokenizer(caption, return_tensors=\"pt\").to(image.device) \n\n        if mode == 'image':    \n            # 当 mode 为 'image' 时，仅使用视觉编码器处理图像数据\n            # 将输入的图像传入视觉编码器，得到图像的嵌入表示\n            image_embeds = self.visual_encoder(image)             \n            # 返回图像的嵌入表示\n            return image_embeds\n\n        elif mode == 'text':\n            # 当 mode 为 'text' 时，仅使用文本编码器处理文本数据\n            # 将分词后的文本输入和注意力掩码传入文本编码器\n            # return_dict = True 表示以字典形式返回输出结果\n            # mode = 'text' 表示使用文本处理模式\n            text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n                                            return_dict = True, mode = 'text')  \n            # 返回文本编码器的最后一层隐藏状态\n            return text_output.last_hidden_state\n\n        elif mode == 'multimodal':\n            # 当 mode 为 'multimodal' 时，同时处理图像和文本数据\n            # 将输入的图像传入视觉编码器，得到图像的嵌入表示\n            image_embeds = self.visual_encoder(image)    \n            # 创建与图像嵌入表示相同形状的注意力掩码，用于在文本编码器中指示哪些位置是有效的图像信息\n            # image_embeds.size()[:-1] 表示取 image_embeds 张量除最后一个维度之外的形状。例如，如果 image_embeds 的形状是 (batch_size, num_patches, embed_dim)，那么 image_embeds.size()[:-1] 就是 `(batch_size, num_patches)`。\n            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)      \n\n            # 将文本输入的第一个 token 设置为编码器的特殊 token “[ENC]”\n            text.input_ids[:,0] = self.tokenizer.enc_token_id\n            # 将文本输入、注意力掩码、图像嵌入表示和图像注意力掩码传入文本编码器\n            # 让文本编码器能够结合图像信息进行处理\n            output = self.text_encoder(text.input_ids,\n                                       attention_mask = text.attention_mask,\n                                       encoder_hidden_states = image_embeds,\n                                       encoder_attention_mask = image_atts,      \n                                       return_dict = True,\n                                      )              \n            # 返回文本编码器的最后一层隐藏状态，即多模态特征\n            return output.last_hidden_state\n\n运行逻辑当创建 BLIP_Base 类的实例时，会执行 __init__ 方法进行初始化操作，具体步骤如下：\n\n接收参数：接收几个参数，包括 med_config（编码器 - 解码器混合模型的配置文件路径）、image_size（输入图像的大小）、vit（视觉变换器的模型大小）、vit_grad_ckpt（是否使用梯度检查点）和 vit_ckpt_layer（梯度检查点的层数）。\n创建视觉编码器：调用 create_vit 函数，根据传入的 vit 和 image_size 等参数创建视觉编码器 self.visual_encoder，同时获取其嵌入维度 vision_width。\n初始化分词器：调用 init_tokenizer 函数初始化一个 BERT 分词器 self.tokenizer。\n加载并配置 BERT 模型：从 JSON 文件中加载 BERT 模型的配置 med_config，并将其 encoder_width 属性设置为视觉编码器的嵌入维度。\n创建文本编码器：使用配置好的 med_config 创建一个 BERT 模型 self.text_encoder，并设置不添加池化层。\n\nforward 方法用于执行前向传播过程，接收三个参数：image（输入图像）、caption（文本描述）和 mode（指定处理模式），具体步骤如下：\n\n模式检查：使用 assert 语句确保 mode 参数的值为 'image'、'text' 或 'multimodal' 之一，否则抛出异常。\n文本编码：使用分词器对输入的文本描述 caption 进行编码，并将编码后的张量移动到与输入图像相同的设备上。\n根据模式执行不同操作：\n模式为 'image'：将输入图像传入视觉编码器 self.visual_encoder，得到图像的嵌入特征 image_embeds 并返回。\n模式为 'text'：将编码后的文本输入到文本编码器 self.text_encoder 中，同时传入注意力掩码，以字典形式返回输出。最后返回文本编码器输出的最后一层隐藏状态。\n模式为 'multimodal'：\n首先，将输入图像传入视觉编码器，得到图像的嵌入特征 image_embeds，并创建一个与图像嵌入特征形状匹配的全 1 注意力掩码 image_atts。\n然后，将编码后文本的第一个标记替换为 enc_token_id。\n最后，将文本输入、注意力掩码、图像嵌入特征和图像注意力掩码一起传入文本编码器，以字典形式返回输出，并返回其最后一层隐藏状态。\n\n\n\n\n\n综上所述，BLIP_Base 类可以根据不同的模式对输入的图像和文本进行特征提取，支持单独提取图像特征、文本特征以及融合图像和文本的多模态特征。 \n2.2 BLIP_Decoder 函数功能介绍这段代码定义了 BLIP_Decoder 类，用于图像描述生成任务。  \n\n初始化：在 __init__ 方法中，接收配置文件路径、图像大小等参数，创建视觉编码器和文本解码器，初始化分词器，并记录提示信息。 \n前向传播：forward 方法接收图像和描述文本，将图像编码为特征，处理文本并设置目标标签，输入文本解码器计算语言模型损失。 \n生成描述：generate 方法根据图像生成描述文本，支持核采样和束搜索两种策略。根据配置生成输入序列，调用文本解码器生成输出，最后将输出解码为文本并去除提示信息。\n\n具体代码class BLIP_Decoder(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/med_config.json',  \n                 image_size = 384,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,\n                 prompt = 'a picture of ',\n                 ):\n        \"\"\"\n        初始化 BLIP_Decoder 类。\n\n        Args:\n            med_config (str): 编码器 - 解码器混合模型的配置文件路径\n            image_size (int): 输入图像的大小\n            vit (str): 视觉变换器（Vision Transformer）的模型大小\n            vit_grad_ckpt (bool): 是否使用梯度检查点来减少内存使用\n            vit_ckpt_layer (int): 梯度检查点的层数\n            prompt (str): 用于图像描述生成的提示文本\n        \"\"\"            \n        super().__init__()\n\n        # 创建视觉编码器，并获取其嵌入维度\n        self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer)\n        # 初始化分词器\n        self.tokenizer = init_tokenizer()   \n        # 从 JSON 文件中加载 BERT 模型的配置\n        med_config = BertConfig.from_json_file(med_config)\n        # 设置编码器的宽度为视觉编码器的嵌入维度\n        med_config.encoder_width = vision_width\n        # 创建文本解码器   \n        self.text_decoder = BertLMHeadModel(config=med_config)    \n\n        # 保存提示文本\n        self.prompt = prompt\n        # 计算提示文本的长度（去除特殊标记）\n        self.prompt_length = len(self.tokenizer(self.prompt).input_ids) - 1\n\n    def forward(self, image, caption):\n        \"\"\"\n        前向传播方法，用于计算语言模型的损失。\n        Args:\n            image (torch.Tensor): 输入的图像张量\n            caption (list): 与图像对应的文本描述列表\n\n        Returns:\n            torch.Tensor: 语言模型的损失\n        \"\"\"\n        # 通过视觉编码器提取图像的嵌入特征\n        image_embeds = self.visual_encoder(image) \n        # 创建图像注意力掩码，形状与图像嵌入特征相同\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n\n        # 使用分词器对文本描述进行编码，填充到最长长度，截断到最大长度为 40，并转换为 PyTorch 张量 , 具体的tokenizer代码见下面\n        text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors=\"pt\").to(image.device) \n\n        # 注：text[:, 0] 会选择 text 中每一行的第 0 个元素，最终返回一个一维的 torch.Tensor ， text在这里形状为(batch_size, sequence_length)\n        # 这里是将每一个batch的输入文本的第一个标记设置为开始标记\n        text.input_ids[:, 0] = self.tokenizer.bos_token_id\n\n        # 创建解码器的目标标签，将填充标记替换为 -100\n        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)         \n        # 将提示部分的目标标签也设置为 -100，不参与损失计算\n        # 实现方法为将每一个batch的decoder_targets除了前prompt_length的内容保留下来，其他都变为-100\n        decoder_targets[:, :self.prompt_length] = -100\n\n        # 将输入文本、注意力掩码、图像嵌入特征等输入到文本解码器中\n        decoder_output = self.text_decoder(text.input_ids, \n                                           attention_mask = text.attention_mask, \n                                           encoder_hidden_states = image_embeds,\n                                           encoder_attention_mask = image_atts,    \n                                           labels = decoder_targets,\n                                           return_dict = True,   \n                                          )   \n        # 获取语言模型的损失\n        loss_lm = decoder_output.loss\n\n        return loss_lm\n\n    def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):\n        \"\"\"\n        根据输入图像生成文本描述。\n        Args:\n            image (torch.Tensor): 输入的图像张量\n            sample (bool): 是否使用核采样，默认为 False，即使用束搜索\n            num_beams (int): 束搜索的束数\n            max_length (int): 生成文本的最大长度\n            min_length (int): 生成文本的最小长度\n            top_p (float): 核采样的概率阈值\n            repetition_penalty (float): 重复惩罚因子\n\n        Returns:\n            list: 生成的文本描述列表\n        \"\"\"\n        # 通过视觉编码器提取图像的嵌入特征\n        image_embeds = self.visual_encoder(image)\n\n        if not sample:\n            # 如果不使用采样，将图像嵌入特征重复 num_beams 次\n            image_embeds = image_embeds.repeat_interleave(num_beams, dim=0)\n\n        # 创建图像注意力掩码，形状与图像嵌入特征相同\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n        # 构建传递给文本解码器的额外参数\n        model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\": image_atts}\n\n        # 为每个图像创建提示文本列表\n        prompt = [self.prompt] * image.size(0)\n        # 使用分词器对提示文本进行编码，并转换为 PyTorch 张量\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(image.device) \n        # 将输入提示文本的第一个标记设置为开始标记\n        input_ids[:, 0] = self.tokenizer.bos_token_id\n        # 去除提示文本的最后一个标记\n        input_ids = input_ids[:, :-1] \n\n        if sample:\n            # 核采样\n            outputs = self.text_decoder.generate(input_ids=input_ids,\n                                                  max_length=max_length,\n                                                  min_length=min_length,\n                                                  do_sample=True,\n                                                  top_p=top_p,\n                                                  num_return_sequences=1,\n                                                  eos_token_id=self.tokenizer.sep_token_id,\n                                                  pad_token_id=self.tokenizer.pad_token_id, \n                                                  repetition_penalty=1.1,                                            \n                                                  **model_kwargs)\n        else:\n            # 束搜索\n            outputs = self.text_decoder.generate(input_ids=input_ids,\n                                                  max_length=max_length,\n                                                  min_length=min_length,\n                                                  num_beams=num_beams,\n                                                  eos_token_id=self.tokenizer.sep_token_id,\n                                                  pad_token_id=self.tokenizer.pad_token_id,     \n                                                  repetition_penalty=repetition_penalty,\n                                                  **model_kwargs)            \n\n        # 用于存储生成的文本描述\n        captions = []    \n        for output in outputs:\n            # 对输出的标记序列进行解码，去除特殊标记\n            caption = self.tokenizer.decode(output, skip_special_tokens=True)    \n            # 去除提示文本，只保留生成的描述部分\n            captions.append(caption[len(self.prompt):])\n        return captions\n\n运行逻辑当创建 BLIP_Decoder 类的实例时，会执行 __init__ 方法进行初始化操作，具体步骤如下：\n\n接收参数：接收几个参数，包括 med_config（编码器 - 解码器混合模型的配置文件路径）、image_size（输入图像的大小）、vit（视觉变换器的模型大小）、vit_grad_ckpt（是否使用梯度检查点）、vit_ckpt_layer（梯度检查点的层数）和 prompt（用于生成描述的提示文本）。\n创建视觉编码器：调用 create_vit 函数，根据传入的 vit 和 image_size 等参数创建视觉编码器 self.visual_encoder，同时获取其嵌入维度 vision_width。\n初始化分词器：调用 init_tokenizer 函数初始化一个 BERT 分词器 self.tokenizer。\n加载并配置 BERT 模型：从 JSON 文件中加载 BERT 模型的配置 med_config，并将其 encoder_width 属性设置为视觉编码器的嵌入维度。\n创建文本解码器：使用配置好的 med_config 创建一个 BertLMHeadModel 作为文本解码器 self.text_decoder。\n保存提示文本及长度：保存传入的提示文本 self.prompt，并计算提示文本的长度（去除特殊标记），保存为 self.prompt_length。\n\nforward 方法用于计算语言模型的损失，接收两个参数：image（输入图像）和 caption（图像对应的文本描述），具体步骤如下：\n\n提取图像特征：将输入图像传入视觉编码器 self.visual_encoder，得到图像的嵌入特征 image_embeds，并创建一个与图像嵌入特征形状匹配的全 1 注意力掩码 image_atts。\n对文本描述进行编码：使用分词器对输入的文本描述 caption 进行编码，填充到最长长度，截断到最大长度为 40，并将编码后的张量移动到与输入图像相同的设备上。\n设置起始标记：将编码后文本的第一个标记替换为开始标记 bos_token_id。\n创建解码器的目标标签：将编码后文本中的填充标记替换为 -100，并将提示部分的目标标签也设置为 -100，以避免这些部分参与损失计算。\n计算损失：将文本输入、注意力掩码、图像嵌入特征和图像注意力掩码一起传入文本解码器，同时传入目标标签，以字典形式返回输出，并获取语言模型的损失 loss_lm。\n返回损失：返回计算得到的语言模型损失 loss_lm。\n\ngenerate 方法用于根据输入图像生成文本描述，接收几个参数，包括 image（输入图像）、sample（是否使用核采样）、num_beams（束搜索的束数）、max_length（生成文本的最大长度）、min_length（生成文本的最小长度）、top_p（核采样的概率阈值）和 repetition_penalty（重复惩罚因子），具体步骤如下：\n\n提取图像特征：将输入图像传入视觉编码器 self.visual_encoder，得到图像的嵌入特征 image_embeds。\n处理束搜索：如果不使用采样（即使用束搜索），将图像嵌入特征重复 num_beams 次。\n创建图像注意力掩码：创建一个与图像嵌入特征形状匹配的全 1 注意力掩码 image_atts，并构建传递给文本解码器的额外参数 model_kwargs。\n构建输入提示：为每个图像创建提示文本列表，并使用分词器对提示文本进行编码，将编码后的第一个标记替换为开始标记 bos_token_id，并去除提示文本的最后一个标记。\n生成文本：根据 sample 参数的值，选择使用核采样或束搜索进行文本生成。\n解码并处理生成结果：对生成的标记序列进行解码，去除特殊标记，并去除提示文本，只保留生成的描述部分，将其存储在列表 captions 中。\n返回生成结果：返回存储生成描述的列表 captions。\n\n2.3 blip_decoder、blip_feature_extractor 函数blip_decoder这个函数的主要功能是创建一个 BLIP_Decoder 模型实例。BLIP_Decoder 模型通常用于图像描述生成任务，它结合了视觉编码器和文本解码器，能够根据输入的图像生成相应的文本描述。如果 pretrained 参数不为空，函数会尝试从指定的路径或 URL 加载预训练模型的权重，并确保所有的模型参数都成功加载。最后返回创建好的模型实例。\n# 定义一个函数 blip_decoder，用于创建并返回一个 BLIP_Decoder 模型实例\n# pretrained 参数用于指定预训练模型的路径或 URL，默认为空字符串，表示不使用预训练模型\n# **kwargs 是一个可变参数，用于接收其他传递给 BLIP_Decoder 类初始化方法的参数\ndef blip_decoder(pretrained='',**kwargs):\n    # 创建一个 BLIP_Decoder 类的实例，将可变参数传递给该类的初始化方法\n    model = BLIP_Decoder(**kwargs)\n    # 检查 pretrained 参数是否不为空，如果不为空，则表示需要加载预训练模型\n    if pretrained:\n        # 调用 load_checkpoint 函数，传入模型实例和预训练模型的路径或 URL\n        # 该函数会返回加载预训练模型后的模型实例和一个包含加载信息的对象 msg\n        model,msg = load_checkpoint(model,pretrained)\n        # 使用 assert 语句检查加载信息对象 msg 中的 missing_keys 列表长度是否为 0\n        # 如果不为 0，说明有模型参数没有成功加载，会抛出 AssertionError 异常\n        assert(len(msg.missing_keys)==0)\n    # 返回创建好的模型实例\n    return model\n\nblip_feature_extractor这个函数的主要功能是创建一个 BLIP_Base 模型实例。BLIP_Base 模型主要用于图像和文本的特征提取，它可以分别处理图像和文本，也可以处理多模态的输入。同样，如果 pretrained 参数不为空，函数会尝试从指定的路径或 URL 加载预训练模型的权重，并确保所有的模型参数都成功加载。最后返回创建好的模型实例。\n# 定义一个函数 blip_feature_extractor，用于创建并返回一个 BLIP_Base 模型实例\n# pretrained 参数用于指定预训练模型的路径或 URL，默认为空字符串，表示不使用预训练模型\n# **kwargs 是一个可变参数，用于接收其他传递给 BLIP_Base 类初始化方法的参数\ndef blip_feature_extractor(pretrained='',**kwargs):\n    # 创建一个 BLIP_Base 类的实例，将可变参数传递给该类的初始化方法\n    model = BLIP_Base(**kwargs)\n    # 检查 pretrained 参数是否不为空，如果不为空，则表示需要加载预训练模型\n    if pretrained:\n        # 调用 load_checkpoint 函数，传入模型实例和预训练模型的路径或 URL\n        # 该函数会返回加载预训练模型后的模型实例和一个包含加载信息的对象 msg\n        model,msg = load_checkpoint(model,pretrained)\n        # 使用 assert 语句检查加载信息对象 msg 中的 missing_keys 列表长度是否为 0\n        # 如果不为 0，说明有模型参数没有成功加载，会抛出 AssertionError 异常\n        assert(len(msg.missing_keys)==0)\n    # 返回创建好的模型实例\n    return model\n\n2.4 init_tokenizer、create_vit 函数init_tokenizer这个函数的主要功能是初始化一个基于 bert-base-uncased 的 BERT 分词器，并为其添加特殊标记。具体来说，它添加了一个开始标记 [DEC] 和一个额外的特殊标记 [ENC]，并将 [ENC] 标记的 ID 存储在分词器的 enc_token_id 属性中。这样做的目的是为了在后续的文本处理中能够正确识别和处理这些特殊标记，例如在图像 - 文本任务中，[ENC] 标记可能用于指示文本的编码开始位置，[DEC] 标记可能用于指示文本的解码开始位置。\n# 定义一个函数 init_tokenizer，用于初始化一个 BERT 分词器并添加特殊标记\ndef init_tokenizer():\n    # 从预训练的 'bert-base-uncased' 模型加载 BERT 分词器\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    # 向分词器添加一个开始标记（bos_token），标记为 '[DEC]'\n    tokenizer.add_special_tokens({'bos_token':'[DEC]'})\n    # 向分词器添加额外的特殊标记，这里添加了 '[ENC]'\n    tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})\n    # 获取 '[ENC]' 标记的 ID 并存储在分词器的 enc_token_id 属性中\n    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]\n    # 返回初始化好的分词器\n    return tokenizer\n\ncreate_vit这个函数的主要功能是根据输入的参数创建一个 Vision Transformer 模型。它接收几个参数，包括 vit（指定模型大小）、image_size（指定输入图像的大小）、use_grad_checkpointing（是否使用梯度检查点）、ckpt_layer（梯度检查点的层数）和 drop_path_rate（随机深度的比率）。根据 vit 参数的值，函数会创建一个基础版本或大型版本的视觉变换器模型。基础版本的模型具有 12 层、12 个注意力头和 768 的嵌入维度，而大型版本的模型具有 24 层、16 个注意力头和 1024 的嵌入维度。最后，函数返回创建好的视觉编码器模型和其嵌入维度，以便后续在多模态模型中使用。\n# 定义一个函数 create_vit，用于创建视觉变换器（Vision Transformer）模型\n# vit 参数指定视觉变换器的模型大小，可选值为 'base' 或 'large'\n# image_size 参数指定输入图像的大小\n# use_grad_checkpointing 参数指定是否使用梯度检查点，默认为 False\n# ckpt_layer 参数指定梯度检查点的层数，默认为 0\n# drop_path_rate 参数指定随机深度（drop path）的比率，默认为 0\ndef create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):\n    # 使用 assert 语句确保 vit 参数的值为 'base' 或 'large'，否则抛出异常\n    assert vit in ['base', 'large'], \"vit parameter must be base or large\"\n    # 如果 vit 参数为 'base'\n    if vit=='base':\n        # 设置视觉编码器的嵌入维度为 768\n        vision_width = 768\n        # 创建一个基础版本的视觉变换器模型\n        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, \n                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n                                           drop_path_rate=0 or drop_path_rate\n                                          )\n    # 如果 vit 参数为 'large'\n    elif vit=='large':\n        # 设置视觉编码器的嵌入维度为 1024\n        vision_width = 1024\n        # 创建一个大型版本的视觉变换器模型\n        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, \n                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n                                           drop_path_rate=0.1 or drop_path_rate\n                                          )\n    # 返回创建好的视觉编码器模型和其嵌入维度\n    return visual_encoder, vision_width\n\n2.5 is_url、load_checkpoint 函数is_url函数用于判断一个字符串是一个有效的 URL 还是一个本地文件名。通过 urlparse 函数解析字符串，检查其协议部分是否为 http 或 https，如果是则返回 True，否则返回 False。\n# 定义一个函数 is_url，用于判断传入的字符串是一个有效的 URL 还是一个本地文件名\n# url_or_filename: 待判断的字符串，可以是 URL 或本地文件路径\ndef is_url(url_or_filename):\n    # 使用 urlparse 函数解析传入的字符串\n    parsed = urlparse(url_or_filename)\n    # 检查解析结果的 scheme（协议）是否为 \"http\" 或 \"https\"\n    # 如果是，则返回 True，表示这是一个 URL；否则返回 False，表示这可能是一个本地文件名\n    return parsed.scheme in (\"http\", \"https\")\n\nload_checkpoint用于从指定的 URL 或本地文件路径加载预训练模型的权重。具体步骤如下：\n\n首先调用 is_url 函数判断输入是 URL 还是本地文件，根据不同情况下载或直接加载模型的检查点数据。\n从检查点数据中提取模型的状态字典。\n对视觉编码器的位置嵌入进行插值处理，以适应不同的输入图像大小。\n遍历模型的状态字典，删除形状不匹配的键值对。\n使用 load_state_dict 函数将处理后的状态字典加载到模型中，允许部分加载。\n最后返回加载了权重的模型实例和加载信息。\n\n# 定义一个函数 load_checkpoint，用于从指定的 URL 或本地文件路径加载预训练模型的权重\n# model: 要加载权重的模型实例\n# url_or_filename: 预训练模型权重的 URL 或本地文件路径\ndef load_checkpoint(model,url_or_filename):\n    # 调用 is_url 函数判断 url_or_filename 是否为 URL\n    if is_url(url_or_filename):\n        # 如果是 URL，使用 download_cached_file 函数下载文件并缓存\n        # check_hash=False 表示不检查文件的哈希值\n        # progress=True 表示显示下载进度\n        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n        # 使用 torch.load 函数从缓存文件中加载模型的检查点数据\n        # map_location='cpu' 表示将数据加载到 CPU 上\n        checkpoint = torch.load(cached_file, map_location='cpu') \n    # 如果不是 URL，检查它是否是一个本地文件\n    elif os.path.isfile(url_or_filename):        \n        # 如果是本地文件，直接使用 torch.load 函数从文件中加载模型的检查点数据\n        # map_location='cpu' 表示将数据加载到 CPU 上\n        checkpoint = torch.load(url_or_filename, map_location='cpu') \n    # 如果既不是 URL 也不是本地文件，则抛出运行时错误\n    else:\n        raise RuntimeError('checkpoint url or path is invalid')\n\n    # 从检查点数据中提取模型的状态字典\n    state_dict = checkpoint['model']\n\n    # 对视觉编码器的位置嵌入进行插值处理\n    # interpolate_pos_embed 函数用于调整位置嵌入的大小以适应不同的输入图像大小\n    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) \n    # 检查模型的状态字典中是否存在 'visual_encoder_m.pos_embed' 键\n    if 'visual_encoder_m.pos_embed' in model.state_dict().keys():\n        # 如果存在，则对该位置嵌入也进行插值处理\n        state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],\n                                                                         model.visual_encoder_m)    \n    # 遍历模型的状态字典中的所有键\n    for key in model.state_dict().keys():\n        # 检查状态字典中是否存在相同的键\n        if key in state_dict.keys():\n            # 如果存在，检查该键对应的值的形状是否与模型中的形状一致\n            if state_dict[key].shape!=model.state_dict()[key].shape:\n                # 如果形状不一致，则从状态字典中删除该键值对\n                del state_dict[key]\n\n    # 使用 load_state_dict 函数将处理后的状态字典加载到模型中\n    # strict=False 表示允许部分加载，即不要求状态字典中的所有键都与模型中的键完全匹配\n    msg = model.load_state_dict(state_dict,strict=False)\n    # 打印加载检查点的信息\n    print('load checkpoint from %s'%url_or_filename)  \n    # 返回加载了权重的模型实例和加载信息\n    return model,msg\n\n","slug":"BLIP代码详解（一）","date":"2025-07-06T09:34:34.000Z","categories_index":"代码详解","tags_index":"BLIP,代码详解","author_index":"犬夜叉"},{"id":"8c1dec7477178f4dfc0f70f9822eceb4","title":"BLIP论文精读","content":"\n\n\n\n\n\n摘要\n视觉语言预训练（VLP）已推动许多视觉语言任务的性能提升。然而，大多数现有预训练模型仅在基于理解的任务或基于生成的任务中表现优异。此外，性能提升在很大程度上通过扩大从网络收集的含噪图像-文本对数据集实现，而这类数据作为监督来源并非最优。本文提出BLIP，一种可灵活迁移至视觉语言理解与生成任务的新型VLP框架。BLIP通过引导字幕生成（由字幕生成器生成合成字幕，过滤器去除噪声字幕）有效利用含噪网络数据。我们在图像-文本检索（平均召回率@1提升2.7%）、图像字幕生成（CIDEr提升2.8%）和视觉问答（VQA分数提升1.6%）等广泛视觉语言任务上取得了最先进的结果。BLIP在以零样本方式直接迁移至视频语言任务时也展现出强大的泛化能力。相关代码、模型和数据集已开源。\n\n\n\n1. Introduction1.1. 现有视觉语言预训练（VLP）的局限性视觉语言预训练在推动众多视觉语言任务（如图像-文本检索、视觉问答、图像字幕生成等）的发展上取得了显著的成功。 然而，当前主流的预训练方法存在两大核心局限性：\n\n模型架构的局限性 (Model Perspective):\n\n现有的模型通常采用两种主流架构：一种是基于编码器的模型（Encoder-based），例如 CLIP 和 ALBEF。 这类模型通过将图像和文本编码到同一个特征空间，非常擅长处理基于“理解”的任务，如图像-文本检索和匹配，因为它们能有效地学习图像和文本之间的对齐关系。 但是，由于其结构天然不包含生成模块，将它们直接应用于文本生成任务（如图像字幕生成）时，会显得非常不自然和低效，通常需要额外增加解码器或进行复杂的改造。\n另一种是基于编码器-解码器的模型（Encoder-Decoder），例如 SimVLM。 这种架构天生适合处理“生成”类任务，因为解码器可以直接根据图像信息生成文本序列。 然而，研究发现，这类模型在需要对图像和文本进行全局特征匹配的检索类任务上，表现却不尽如人意。\n核心矛盾: 这种架构上的分化导致了没有一个单一的模型能够同时在“理解”和“生成”两大类任务上都达到顶尖水平。模型的设计往往需要在两者之间做出取舍。\n\n\n训练数据的局限性 (Data Perspective):\n\n为了获得强大的泛化能力，最先进的VLP模型（如 CLIP, ALBEF, SimVLM）普遍依赖于从网络上自动爬取的大规模图像-文本对作为训练数据。这些数据集的规模动辄千万甚至上亿，通过扩大数据规模确实带来了性能的提升。\n然而，这种方式获取的文本数据（通常是网页图片的“alt-text”标签）充满了噪声。这些文本往往与图像内容不完全相关，可能包含无关的广告信息、上下文描述，或者描述非常笼统、不准确。 例如，一张“日落时分公园里的面包店”的图片，其网络文本可能仅仅是“blue sky bakery in sunset park”，非常简洁但缺乏视觉细节。\n核心问题: 这种充满噪声的文本是一种“次优”的监督信号（suboptimal source of supervision）。 直接使用这些噪声数据进行训练，会严重影响模型学习精确的图文对齐关系，从而限制了模型性能的上限。\n\n\n\n1.2. BLIP 的提出与核心贡献为了系统性地解决上述模型和数据两个层面的挑战，该研究提出了 BLIP (Bootstrapping Language-Image Pre-training) 框架。 BLIP 的目标是创建一个既能统一理解与生成任务，又能有效利用并净化网络噪声数据的VLP新范式。 其核心贡献体现在以下两个方面：\n\n(a) 模型创新：多模态编码器-解码器混合模型 (Multimodal mixture of Encoder-Decoder, MED)\n\n这是一种全新的、为多任务预训练和灵活迁移学习设计的模型架构。 MED 的精妙之处在于它能够根据不同的任务需求，灵活地切换其工作模式，主要有三种形态：\n单模态编码器 (Unimodal Encoder): 独立地对图像和文本进行编码，用于学习两者各自的表征。\n图像接地的文本编码器 (Image-grounded Text Encoder): 在标准的文本编码器基础上，通过交叉注意力机制（Cross-Attention）将图像信息注入到文本表征中，从而生成深度融合的多模态表征，专门用于理解任务。\n图像接地的文本解码器 (Image-grounded Text Decoder): 同样以图像信息为条件，但其文本处理部分采用自回归的方式（causal self-attention），用于生成连贯的文本描述。\n\n\n为了充分发挥 MED 架构的潜力，BLIP 设计了三个协同工作的预训练目标，在一个统一的框架下进行联合优化：\n图文对比学习 (Image-Text Contrastive Learning, ITC): 旨在拉近正样本图文对的特征表示，推远负样本对，从而对齐视觉和语言的特征空间。\n图文匹配 (Image-Text Matching, ITM): 这是一个二分类任务，让模型学习判断一个图文对是否匹配，旨在捕捉更细粒度的图文对应关系。\n图像条件下的语言建模 (Image-conditioned Language Modeling, LM): 训练模型在给定图像的条件下，生成对应的文本描述，赋予模型生成能力。\n\n\n\n\n(b) 数据创新：字幕生成与过滤 (Captioning and Filtering, CapFilt)\n\n这是一种新颖的、用于从噪声图文对中进行“数据自举”（Dataset Bootstrapping）的方法，其核心思想是“以模型净化数据，再用净化后的数据训练更好的模型”。\n该过程包含两个关键模块，它们都是由一个预训练好的 MED 模型微调而来：\n字幕器 (Captioner): 这是一个图像接地的文本解码器，其任务是为网络图片生成高质量的、描述性的合成字幕 (synthetic captions)。例如，对于一张网络图片，除了原有的噪声文本，字幕器会生成一个新的、更详细的描述，如“a chocolate cake with cream frosting and chocolate sprinkles on top”。\n过滤器 (Filter): 这是一个图像接地的文本编码器，其任务是判断图文对是否匹配。它会同时对原始的网络文本和新生成的合成字幕进行筛选，移除所有它认为与图像内容不匹配的“噪声”文本。 这一步确保了最终用于训练的数据质量。\n\n\n通过 CapFilt 机制，BLIP 能够从海量的网络数据中提炼出一个规模可观且质量更高的训练集，从而为训练高性能的 VLP 模型奠定坚实的数据基础。 \n\n\n\n如图所示，字幕器根据图片生成对该图片的合成字幕，之后再通过过滤器来进行评估，是保留原网络字母还是更新为合成的字幕。\n1.3. 关键成果通过模型和数据的双重创新，BLIP 在广泛的视觉语言任务上取得了最先进的（state-of-the-art）成果，全面验证了其框架的有效性和优越性。\n\n显著的性能提升: 在图文检索任务上，平均召回率（Recall@1）提升了 2.7%；在图像字幕生成任务上，CIDEr 分数提升了 2.8%；在视觉问答（VQA）任务上，VQA 分数提升了 1.6%。\n强大的泛化能力: BLIP 不仅在传统的图文任务上表现出色，还展现了极强的零样本（zero-shot）迁移能力，无需任何针对性训练，直接应用于视频-语言任务（如文本到视频检索和视频问答）时，也取得了顶尖性能。 \n\n2. Related Work2.1. 视觉语言预训练 (Vision-language Pre-training, VLP)\n核心目标与数据来源: VLP 的核心思想是在大规模的图文对数据上预训练一个通用模型，然后将这个模型微调应用于各种下游的视觉和语言任务。由于高质量的人工标注图文数据集（如COCO）规模有限且成本高昂，绝大多数现代 VLP 方法都转向使用从网络上自动爬取的数据集，例如 Conceptual Captions, ALIGN, SBU Captions 等。这些数据集通过简单的规则过滤来收集，但文本中普遍存在的噪声问题是不可避免的。\n\n对噪声数据的忽视与本文的贡献: 尽管噪声普遍存在，但其负面影响在很大程度上被通过不断扩大数据集规模所带来的性能增益所掩盖了。许多研究更关注于模型架构的改进，而忽视了数据质量这一根本问题。本研究明确指出，充满噪声的网络文本对于学习视觉-语言对齐是一种次优的选择，并提出了 CapFilt 机制，这是一种更有效、更智能地利用这些海量网络数据的方式，从根本上提升了数据质量。\n\n统一框架的挑战与本文的方案: VLP 领域的一个长期挑战是如何设计一个单一的模型架构，使其能够同时胜任基于“理解”的任务（如检索）和基于“生成”的任务（如字幕生成）。以往的尝试，无论是纯编码器模型、纯编码器-解码器模型，还是简单的统一编码器-解码器模型，都难以两全其美，各有侧重和局限。本文提出的 多模态编码器-解码器混合模型 (MED) 通过其灵活的功能切换能力，以及高效的参数共享策略，成功地在一个统一的框架内实现了对各类下游任务的卓越支持，同时保持了预训练过程的简洁和高效。\n\n\n2.2. 知识蒸馏 (Knowledge Distillation, KD)\n基本概念: 知识蒸馏是一种模型压缩和性能提升技术，其目标是通过让一个（通常较小的）“学生模型”模仿一个（通常较大的）“教师模型”的输出来进行学习，从而将教师模型的知识迁移给学生模型。自蒸馏（Self-distillation）是其中的一种特殊情况，即教师和学生模型的结构和尺寸相同，已被证明在图像分类和近期的VLP领域（如ALBEF）中是有效的。\n\nCapFilt 作为一种新颖的知识蒸馏: 传统的知识蒸馏方法大多是让学生模型模仿教师模型的类别预测概率（soft labels）。本文提出的 CapFilt 可以被看作是在VLP场景下一种更高级、更有效的知识蒸馏形式。它不是传递简单的预测分数，而是传递更丰富的语义知识：\n\n字幕器 (Captioner) 的作用类似于一个“生成式教师”，它将其对图像的理解“蒸馏”成一段段语义丰富的合成字幕。这些字幕相比于简单的类别标签，包含了更复杂的场景、物体和关系知识。\n过滤器 (Filter) 的作用则像一个“判别式教师”，它将其判断图文是否匹配的“知识”“蒸馏”出来，通过筛选行为告诉主模型什么样的图文对是高质量的。\n通过这种“生成”与“过滤”相结合的方式，CapFilt 实现了一种全新的、基于语义内容的知识蒸馏，从而有效地提升了预训练数据的质量。\n\n\n\n2.3. 数据增强 (Data Augmentation, DA)\n背景: 数据增强在计算机视觉领域是一种应用广泛且非常有效的技术。然而，对于自然语言处理（NLP）任务，由于语言的离散性和结构性，进行有效的数据增强更具挑战性。近年来，一个新兴的方向是利用预训练好的生成式语言模型来为各种NLP任务（特别是在数据稀疏的场景下）合成新的训练样本。\n\n本文工作的独特性: 之前利用生成模型进行数据增强的工作主要集中在纯文本、小规模或低资源的NLP任务上。本文首次将这一思想成功地应用到了大规模的视觉-语言预训练场景中。通过生成海量的合成字幕，本研究证明了“生成式数据增强”不仅适用于低资源场景，同样能为超大规模的多模态预训练带来显著的性能优势，为解决VLP中的数据瓶颈问题开辟了新的道路。\n\n\n3. 方法 (Method)该研究提出的BLIP是一个旨在从充满噪声的图文对中学习的统一视觉语言预训练（VLP）框架。 本节将首先详细介绍其创新的模型架构——多模态编码器-解码器混合模型（MED）及其预训练目标，然后阐述用于数据集自举（bootstrapping）的字幕生成与过滤（CapFilt）机制。\n3.1. 模型架构 (Model Architecture)BLIP的模型架构设计旨在创建一个能够同时处理理解与生成任务，并且在两者之间灵活切换的统一模型。\n\n\n图像编码器 (Image Encoder):\n\n模型采用 Vision Transformer 作为图像编码器。 这一选择与近期先进的方法保持一致，相比于早期工作中依赖预训练物体检测器来提取区域特征的方式，ViT在计算上更为友好。\n其工作原理是将输入图像分割成一系列的固定大小的图块（patches），然后将这些图块线性投影后编码为一串嵌入序列（sequence of embeddings）。 在序列的开头，会额外加入一个 [CLS] (classification) token，其在经过Transformer编码后的最终输出嵌入被用作整个图像的全局特征表示。\n\n\n多模态编码器-解码器混合模型 (Multimodal Mixture of Encoder-Decoder, MED):\n\n为了实现理解与生成能力的统一，研究者提出了MED，这是一个能够以三种不同功能模式运行的多任务模型。\n模式1：单模态编码器 (Unimodal Encoder):\n在此模式下，模型分别对图像和文本进行独立编码。\n文本编码器基于BERT架构实现。 在文本输入的起始位置同样会添加一个 [CLS] token，用于在编码后汇总整个句子的信息。该模式主要服务于需要独立图文表征的任务，例如对比学习。\n\n\n模式2：图像接地的文本编码器 (Image-grounded Text Encoder):\n此模式用于深度融合视觉和语言信息，以完成理解类任务。\n其实现方式是在文本编码器的每个Transformer块中，在自注意力（Self-Attention, SA）层和前馈网络（Feed Forward Network, FFN）之间，插入一个额外的交叉注意力（Cross-Attention, CA）层。这个交叉注意力层使得文本token能够“关注”并融合来自图像编码器的视觉特征。\n为了得到最终的图文融合表征，一个任务专用的 [Encode] token会被添加到文本输入中，该token在经过编码后的输出嵌入，即被用作代表该图文对的多模态表征。\n\n\n模式3：图像接地的文本解码器 (Image-grounded Text Decoder):\n此模式用于生成类任务，例如图像字幕。\n其结构与图像接地的文本编码器非常相似，但核心区别在于将编码器中的双向自注意力 (bi-directional self-attention) 层替换为因果自注意力 (causal self-attention) 层。 [102] 这种因果注意力机制确保了在预测当前词元时，模型只能访问到它之前已经生成的词元，这是自回归文本生成的关键。\n一个特殊的 [Decode] token被用作序列开始的信号，引导模型开始生成过程，并以一个序列结束（end-of-sequence）token来标志生成的终点。 [103]\n\n\n\n\n\n\n\n\n\n\n\n\n补充BLIP中使用的注意力机制\n双向自注意力（Bi Self-Att）双向自注意力（Bi-directional Self-Attention）是一种允许模型在处理序列时同时考虑整个上下文的注意力机制。在标准的Transformer自注意力中，每个输入位置可以通过关注序列中所有位置来更新自己的表示。\n\n具体来说：\n给定一个输入序列  ，对于每个位置 ，模型计算它与所有 （ j = 1  到 n ）的注意力权重。\n这些权重通过查询（Query）、键（Key）和值（Value）的点积计算得出，最终生成  的新表示。\n因为没有信息流限制，信息可以在序列中双向传播，这就是“双向”的含义。\n\n\n功能：\n捕捉全局上下文：双向自注意力能让模型综合整个序列的信息。例如，在文本中，一个词的含义可能依赖于它前后的词，双向自注意力可以捕捉这种关系。\n适用于编码任务：这种机制非常适合需要理解完整输入的场景，比如编码器中生成深层表示。\n\n\n在BLIP中的应用：\nText Encoder：BLIP的Text Encoder使用双向自注意力来处理纯文本输入。通过这种方式，模型可以理解文本的全局语义，例如句子的整体意思。\nImage-grounded Text Encoder：在这个模块中，双向自注意力用于处理文本序列，同时通过交叉注意力与图像特征交互，进一步丰富文本表示。\n\n\n\n\n因果自注意力（Causal Self-Att）因果自注意力（Causal Self-Attention）是一种限制信息流的注意力机制，确保模型在处理序列时只能访问当前位置之前的信息。这种机制常见于自回归任务（例如语言生成）。\n\n具体来说：\n对于输入序列 ，在计算  的表示时，模型只能关注  到  的位置，而不能看到  到 。\n实现方法通常是在注意力矩阵中加入掩码（Mask），将未来位置的权重设为零，从而阻止信息从“未来”流向“现在”。\n\n\n功能：\n保持因果性：因果自注意力模拟了序列生成的自然顺序，确保模型在预测下一个元素时不会“偷看”未来的信息。\n适用于解码任务：在Transformer的解码器中，这种机制被用来逐步生成序列，例如从左到右生成文本。\n\n\n在BLIP中的应用：\nImage-grounded Text Decoder：BLIP的这个模块负责生成文本（例如图像描述）。因果自注意力确保在生成每个词时，模型只依赖于之前生成的词和图像特征，而不会提前使用未生成的词。这种设计符合生成任务的逻辑。\n\n\n\n\n交叉注意力（Cross Attention）交叉注意力（Cross Attention）是一种在两个不同序列之间建立联系的注意力机制。与自注意力关注自身序列不同，交叉注意力让一个序列（查询）去关注另一个序列（键和值）。\n\n具体来说：\n假设有两个序列：查询序列 （例如文本）和键-值序列 （例如图像特征）。\n 中的每个位置会计算与  中所有位置的注意力权重，并从  中提取信息来更新自己的表示。\n这种机制不限制信息流，允许两个模态之间的自由交互。\n\n\n功能：\n跨模态交互：交叉注意力特别适合多模态任务（例如图像-文本任务），因为它能让一个模态的信息影响另一个模态。\n信息融合：通过这种机制，模型可以将图像和文本的特征结合起来，生成更丰富的表示。\n\n\n在BLIP中的应用：\nImage-grounded Text Encoder：在这个模块中，文本序列作为查询（Query），图像特征作为键（Key）和值（Value）。交叉注意力让文本表示能够融合图像信息，从而更好地理解图像-文本对的语义。\nImage-grounded Text Decoder：在生成文本时，交叉注意力让模型在每一步生成中都能参考图像特征，帮助生成与图像内容相关的描述。\n\n\n\n\n\n3.2. 预训练目标 (Pre-training Objectives)为了充分训练MED模型，BLIP设计了三个预训练目标并进行联合优化，其中包括两个理解类目标和一个生成类目标。这种多任务学习是高效的：对于每个输入的图文对，计算成本高昂的图像编码器只需进行一次前向传播，而文本转换器部分则分别激活三种不同功能，进行三次前向传播来计算对应的三个损失函数。\n\n图文对比损失 (Image-Text Contrastive Loss, ITC):\n\n该损失激活单模态编码器。\n其目标是通过鼓励正样本（匹配的）图文对在特征空间中拥有相似的表示，同时拉远负样本（不匹配的）对的表示，来对齐视觉和语言的特征空间。这已被证明是提升视觉语言理解能力的有效方法。\n具体实现上，BLIP沿用了ALBEF中的方法，引入一个动量编码器（momentum encoder）来产生更稳定的特征，并利用动量编码器生成的相似度分数创建软标签（soft labels），以此作为训练目标。这种软标签机制可以解释批次内负样本中可能存在的“假阴性”（即实际上也匹配的图文对）问题。\n\n\n图文匹配损失 (Image-Text Matching Loss, ITM):\n\n该损失激活图像接地的文本编码器。\n其目标是学习能够捕捉视觉和语言之间细粒度对齐关系的多模态表征。\nITM被设计成一个二元分类任务：模型利用一个ITM头（一个简单的线性层）来预测输入的图文对是正样本（匹配）还是负样本（不匹配）。\n为了提升训练的有效性，模型采用了ALBEF中的难负样本挖掘 (hard negative mining) 策略。具体来说，在同一个批次中，那些与图像在对比学习特征空间中相似度更高（即更具迷惑性）的文本负样本，会被优先选择用于计算ITM损失。\n\n\n语言建模损失 (Language Modeling Loss, LM):\n\n该损失激活图像接地的文本解码器。\n其目标是训练模型具备在给定图像的条件下生成相应文本描述的能力。\n它优化的是一个交叉熵损失函数，以自回归的方式最大化模型生成真实文本的概率。在计算损失时，应用了0.1的标签平滑来防止模型过拟合。\n与VLP中广泛使用的掩码语言建模相比，LM损失直接赋予了模型将视觉信息转化为连贯文本字幕的生成能力。\n\n\n参数共享策略:\n\n为了在实现多任务学习的同时保持训练效率，文本编码器和文本解码器共享了除自注意力（SA）层之外的所有参数。\n这一设计的核心理由是：编码任务（理解当前输入）和解码任务（预测下一个词元）之间的本质差异主要体现在SA层的功能上（双向 vs. 因果）。而嵌入层、交叉注意力（CA）层和前馈网络（FFN）在两种任务中扮演的角色相似，因此共享这些层的参数不仅能减少模型总参数量、提升训练效率，还能从多任务学习中受益。\n\n\n\n\n\n\n\n\n\n损失函数详解\n图文对比损失（Image-Text Contrastive Loss, ITC）图文对比损失（ITC）是一种对比学习损失，旨在通过优化单模态编码器（视觉Transformer和文本Transformer），使正样本图文对（匹配的图像和文本）在特征空间中的表示相似，而负样本对（不匹配的图像和文本）表示远离。这种方法通过对齐图像和文本的特征空间，提升模型在跨模态任务中的表现。\n\n具体计算：\n假设我们有一个批次（batch）包含  个图文对  ，其中  是图像， 是对应的文本。\n视觉Transformer将图像  编码为特征向量 ，文本Transformer将文本  编码为特征向量 。\nITC的目标是最大化正样本对  的相似度（例如余弦相似度），同时最小化负样本对的相似度。\n损失函数通常基于InfoNCE（Noise-Contrastive Estimation）形式：  , 其中  是余弦相似度， 是温度参数（控制相似度的缩放）, 第一项是图像到文本的对比损失，第二项是文本到图像的对比损失。\n\n\n功能：\nITC通过对比学习对齐图像和文本的特征空间，使模型能够学习到模态无关的表示，适用于检索任务（例如图像-文本检索）。动量编码器是什么？在训练过程中，我们有一个不断更新的“在线编码器”（Online Encoder），它的参数通过反向传播实时更新。动量编码器（Momentum Encoder）是这个在线编码器的“影子克隆”，但它更新得非常缓慢。\n\n\n工作原理: \n它的参数不通过反向传播来更新。相反，它的参数是“在线编码器”参数的移动平均值。\n , 其中  是一个接近1的动量系数（比如0.995）。这意味着动量编码器在每一步只吸收一小部分在线编码器的新知识，主体上保持了过去一段时间的稳定状态。\n作用: 它提供了一个更稳定的训练目标。如果让在线编码器自己跟自己学（即正样本对的目标特征也由快速变化的在线编码器生成），训练会很不稳定，就像追逐一个不断移动的目标。动量编码器像一个“更沉稳、更有智慧的老师”，为在线编码器提供了一个缓慢变化、更一致的特征作为学习目标，从而稳定了对比学习的训练过程。软标签 (Soft Labels) 是什么？在传统的对比学习中，标签是“硬”的：对于一张图片，与它配对的文本是正样本（标签为1），所有其他文本都是负样本（标签为0）。但这里有个问题：在一个批次中，一张“猫”的图片可能被标记为与另一段描述“一只在睡觉的小猫”的文本为负样本（因为它们不是原始配对），但实际上它们语义上高度相关, 强行将它们的相似度推向0是有害的。\n\n\n工作原理: \n“软标签”就是为了解决这个问题。它不再使用[1, 0, 0, …]这样的硬标签。对于一个给定的图文对(I, T)，它的目标相似度（即标签）是通过动量编码器计算出来的图文相似度 。\n\n\n作用:\n考虑潜在正样本: 如果一个“负样本对”其实在语义上很相似，那么动量编码器算出的相似度会是一个大于0的数值（比如0.3）。模型的目标就是把在线编码器算出的相似度也拉近到0.3，而不是粗暴地推向0。这相当于告诉模型：“虽然它们不是最佳拍档，但也有点关系，别把它们推得太远。”\n提供更丰富的监督信号: 相比于非0即1的硬标签，软标签为模型优化提供了更平滑、更丰富的梯度信息。\n\n\n\n\n\n图文匹配损失 (ITM)图文匹配损失（ITM）是一个二分类任务，目标是让模型学习判断一个图文对是否匹配（正样本）或不匹配（负样本）。它通过基于图像的文本编码器（Image-grounded Text Encoder）生成多模态表示，并使用一个线性层（ITM头）预测匹配概率。\n\n\n具体计算：\n对于一个图文对 ，基于图像的文本编码器生成多模态特征 。\nITM头（线性层）将  映射到一个二分类概率：  , 其中  表示图文对是正样本的概率。\n损失函数是二分类交叉熵：  ，其中  表示正样本， 表示负样本。\n\n\n功能：\nITM损失帮助模型学习图像和文本之间的细粒度对齐，增强多模态表示的能力，适用于任务如视觉问答（VQA）和图像-文本检索。困难负样本挖掘策略 (Hard Negative Mining) 是什么？在一个批次中，我们可以轻易地为一张图片找到很多“简单负样本”。比如，图片是一只狗，负样本文本是“一辆蓝色的汽车”，模型很容易就能判断它们不匹配。用这种简单的样本训练，模型学不到精细的辨别能力。\n\n\n工作原理: \n困难负样本挖掘是一种“择优录取”的训练策略。它专门挑选那些最容易让模型混淆的负样本来训练。\n首先，对于批次中的每一张图片，利用ITC计算出的图文相似度，找出与它最相似的非配对文本。\n这个最相似的文本就是这张图片的“困难负样本”。例如，对于“一只黄色的狗在草地上奔跑”的图片，困难负样本可能是“一只棕色的狗在公园里玩耍”，而不是“一艘船在海上航行”。\n在计算ITM损失时，模型被强制要求区分“正样本”和这些“困难负样本”。\n\n\n\n\n作用: \n强迫模型学习图文之间更细粒度的对应关系。模型不能只看到“狗”，它必须学会分辨“黄色的狗”和“棕色的狗”，“草地”和“公园”，从而真正理解图像和文本的深层语义。\n\n\n\n\n\n语言建模损失 (LM)语言建模损失（LM）用于训练基于图像的文本解码器（Image-grounded Text Decoder），目标是根据图像生成连贯的文本描述。它是一个自回归任务，优化文本序列的似然概率。\n\n\n具体计算：\n假设输入图像  和对应的文本序列 。\n解码器以自回归方式生成文本，预测下一个词的概率：  , 其中  表示前  个词， 是图像特征。\n损失函数是交叉熵损失：   , 其中  是模型预测的词  的概率。\n\n\n功能：\nLM损失使模型能够根据图像生成连贯的文本描述，适用于图像描述生成（Image Captioning）等任务。\n相比掩码语言建模，LM更适合生成任务，因为它直接优化序列的生成能力。标签平滑（Label Smoothing）是什么？标签平滑是一种正则化技术，用于避免模型对预测过于自信，防止过拟合。\n\n\n具体实现：\n在标准交叉熵损失中，目标词的标签是one-hot向量（例如 。\n标签平滑将one-hot标签替换为平滑分布，例如：   , 其中  是平滑系数（BLIP中设为0.1）， 是词汇表大小。 \n例如，若词汇表大小 ，目标词的one-hot标签为 ，平滑后变为 。\n\n\n\n\n损失函数使用平滑后的标签计算交叉熵。为什么用标签平滑？\n防止过拟合：平滑标签减少了模型对单一正确答案的过度依赖，提高泛化能力。\n处理数据噪声：在图像-文本数据中，文本描述可能有噪声（例如不完全准确的标注），标签平滑可以缓解这种影响。\n\n\n\n3.3. 字幕生成与过滤 (CapFilt)CapFilt机制旨在解决网络图文数据质量不佳的问题。高质量的人工标注数据集（例如COCO）数量有限，而网络上自动收集的替代文本（alt-text）则充满噪声。\n\n\n核心流程: CapFilt引入了两个模块：一个用于生成字幕的Captioner和一个用于移除噪声的Filter。如图3所示，这两个模块都是从同一个预训练好的MED模型初始化，然后在高质量的COCO数据集上分别进行轻量化的微调。\n字幕器 (Captioner):\n它本质上是一个图像接地的文本解码器。\n它通过在COCO上进行LM目标微调，学会了为图像生成高质量的描述。 \n在CapFilt流程中，对于每一张网络图片 ，Captioner都会生成一条新的合成字幕 (synthetic caption) 。\n\n\n过滤器 (Filter): \n它是一个图像接地的文本编码器。\n它通过在COCO上进行ITC和ITM目标微调，学会了精确判断图文是否匹配。\n在流程中，Filter会对原始的网络文本  和新生成的合成字幕  进行双重检验。如果Filter的ITM头预测某个文本与图像不匹配，那么该文本就会被视为噪声并被丢弃。\n\n\n数据集自举 (Dataset Bootstrapping):\n最后，经过过滤后保留下来的网络图文对和合成图文对，会与原始的人工标注数据集（如COCO）合并，形成一个全新的、规模更大且质量更高的预训练数据集。\n这个经过“自举”优化的新数据集，将被用来从头开始训练一个新的、性能更强的BLIP模型。 \n\n\n\n4. 实验与讨论 (Experiments and Discussions)本章节首先介绍预训练的详细配置，然后通过一系列消融实验，深入分析和验证所提出方法的有效性。\n4.1. 预训练详情 (Pre-training Details)\n实现与硬件:\n模型使用 PyTorch 框架实现，并在两个拥有16个GPU的节点上进行预训练。\n\n\n模型初始化:\n图像Transformer（ViT）的权重初始化自一个在ImageNet上预训练过的ViT模型。\n文本Transformer的权重则初始化自  的权重。\n\n\n模型变体:\n实验中探索了两种不同规模的视觉骨干网络：ViT-B/16 和 ViT-L/16。\n除非特别指明，论文中报告的所有名为“BLIP”的结果均默认使用ViT-B作为骨干网络。\n\n\n训练超参数:\n优化器采用 AdamW，权重衰减（weight decay）设置为 0.05。\n预训练共进行 20 个 epoch。\n批量大小（batch size）根据模型规模设定，ViT-B为2880，ViT-L为2400。\n学习率采用预热（warm-up）策略，ViT-B的峰值学习率为 ，ViT-L为 ，之后学习率以0.85的速率进行线性衰减。\n\n\n图像分辨率:\n在预训练阶段，使用  分辨率的随机图像裁剪。\n在下游任务微调阶段，为了获得更好的性能，将图像分辨率提升至 。\n\n\n预训练数据集:\n模型使用了与ALBEF方法相同的1400万张图像数据集进行基础预训练。\n该数据集由高质量的人工标注数据集（COCO和Visual Genome）和规模更大但噪声更多的网络数据集（Conceptual Captions、Conceptual 12M和SBU captions）混合而成。\n为了验证方法在大规模噪声数据上的可扩展性，实验还额外使用了一个包含1.15亿张图像的LAION数据集，该数据集的文本噪声程度更高。\n由于LAION数据集规模巨大，在预训练时每个epoch只使用其1/5的数据。\n\n\n\n4.2. CapFilt 的效果分析为了验证字幕生成与过滤（CapFilt）机制的有效性，本节在图像-文本检索和图像字幕生成等下游任务上进行了一系列对比实验。\n\n\n核心作用:\n实验结果表明，在1400万图像数据集上，无论是单独使用字幕器（Captioner）还是单独使用过滤器（Filter），相较于直接使用原始噪声文本的基线模型，性能均有可见的提升。\n当字幕器和过滤器协同工作时，它们的效果能够相互补充，共同带来比基线模型显著的性能飞跃，这充分证明了CapFilt机制的强大作用。\n\n\n可扩展性:\nCapFilt的效果可以随着数据和模型规模的扩大而进一步增强。当应用于更大的数据集（129M图像）和更大的视觉骨干（ViT-L）时，性能提升更为明显，这验证了其在数据和模型两个维度上的良好可扩展性。\n一个值得注意的发现是，使用更强大的ViT-L模型作为字幕器和过滤器，来为ViT-B基础模型的预训练准备数据，同样可以提升最终基础模型的性能。这表明，强大的CapFilt模块可以将其知识有效地“蒸馏”到较小的模型中。\n\n\n定性示例:\n图4直观地展示了CapFilt的工作流程。对于给定的图像，字幕器能够生成全新的、描述性更强的文本（），而过滤器则能有效识别并移除原始网络文本（）和合成文本中的噪声。\n例如，一张图片的原网络文本是“在我家附近的桥上”，这与图片内容（一群鸟飞过湖面）不符，被过滤器拒绝；而模型合成的文本“a flock of birds flying over a lake at sunset”则因与图像匹配而被接受。\n另一个例子中，一张盆栽植物的图片，其网络文本是关于奥地利的一栋房子，被过滤器拒绝；而模型合成的文本“a potted plant sitting on top of a pile of rocks”则被接受。\n\n\n\n\n4.3. 多样性是合成字幕的关键\n\n生成策略对比:\n在CapFilt中，合成字幕是通过核采样 (Nucleus Sampling) 的方式生成的。这是一种随机解码方法，它从一个累积概率超过特定阈值p（实验中p=0.9）的词元集合中进行采样，从而为生成过程引入了随机性和多样性。\n实验将核采样与束搜索 (Beam Search) 进行了比较，后者是一种确定性解码方法，旨在搜索并生成整体概率最高的文本序列。\n\n\n结果与分析:\n实验结果显示，尽管核采样生成的文本被过滤器判定为噪声的比例更高（拒绝率为25%，高于束搜索的19%），但使用这些文本训练的模型在各项下游任务上均表现出明显更优的性能。\n研究者对此的推断是，多样性是根本原因。核采样能够生成更多样化、更出人意料的字幕，这些字幕包含了模型可以学习的新知识和新颖的表达方式。\n相比之下，束搜索倾向于生成“安全”的、在数据集中非常常见的字幕，这些字幕能提供给模型的额外信息较少。因此，模型从信息量更大、更多样化的文本中获益更多。\n\n\n\n\n\n\n\n\n\nBeam Search v.s. Nucleus Sampling\n\n集束搜索 (Beam Search)\n核心思想: 集束搜索是一种启发式的搜索算法，是贪心搜索的一种改进。它不是只“押注”在最好的一个选择上，而是在每一步都保留几种可能性，以期在全局上找到一个概率更高的句子。\n工作原理:\n设定宽度 k: 首先确定一个叫做“集束宽度”的超参数 k（比如 k=3）。这代表在每一步，我们都将保留 k 个最可能的候选句子。\n第一步: 模型计算出词汇表中所有词作为第一个词的概率，并选择概率最高的 k 个词，构成 k 个候选序列的起点。\n后续步骤: 在第 t 步，模型会基于之前的 k 个候选序列，分别预测下一个词的概率。这样会产生很多新的候选序列（理论上是 k * 词汇表大小 个）。\n剪枝: 从所有这些新的候选序列中，计算每个序列的整体概率（通常是各词元对数概率之和），然后只保留总概率最高的 k 个序列。\n重复: 不断重复第3和第4步，直到所有 k 个序列都生成了结束符（[EOS]）或者达到了预设的最大长度。\n最终选择: 最后，从完成的 k 个候选序列中，选择整体概率最高的那一个作为最终输出。\n\n\n特点与评价:\n确定性 (Deterministic): 对于给定的模型和输入，只要 k 值不变，每次运行的结果都是完全相同的。\n追求最高概率: 它的目标是找到一个近似全局最优的高概率序列。\n缺点 (如文中所述): 它倾向于生成那些在训练数据中反复出现、非常“安全”和常见的句子。这会导致生成的文本缺乏多样性、创造性，甚至有时会陷入重复循环（比如 “I think I think I think…”）。因为它总是选择最可能、最不出错的路径，所以很难产生“惊喜”。\n\n\n\n\n\n\n\n核采样 (Nucleus Sampling / Top-p Sampling)\n核心思想: 核采样不追求找到唯一“最好”的句子，而是试图模拟人类说话的方式——在多个合理的选项中进行随机选择。它通过限制选择范围来避免选到那些不合逻辑的词，同时又保留了一定的随机性。\n工作原理:\n设定阈值 p: 首先确定一个累积概率阈值 p（比如文中的 p=0.9）。\n计算概率并排序: 在生成每一步时，模型会计算词汇表中所有词的概率分布，并从高到低进行排序。\n确定候选集（“核”）: 从概率最高的词开始，依次将它们的概率相加，直到这个累积概率刚好超过阈值 p。所有这些被累加的词元就构成了一个候选集，这个集合被称为“核”（Nucleus）。\n重新归一化与采样: 模型会在这个“核”中对各个词元的概率进行重新归一化（使它们的概率和为1），然后随机从这个集合中抽样一个词作为输出。\n\n\n举例理解: 假设p=0.9，模型预测下一个词的概率如下： the (0.5), a (0.2), one (0.15), his (0.05), house (0.04), …\nP(“the”) = 0.5  (累积概率 0.5)\nP(“the”) + P(“a”) = 0.7  (累积概率 0.7)\nP(“the”) + P(“a”) + P(“one”) = 0.85 (累积概率 0.85)\nP(“the”) + P(“a”) + P(“one”) + P(“his”) = 0.9 (累积概率 0.9，达到阈值)\n此时，候选集（“核”）就是 {the, a, one, his}。模型将只在这4个词中进行随机抽样，而完全忽略house等其他词。\n\n\n特点与评价:\n随机性: 每次运行都可能产生不同的结果，因为最后一步是抽样。\n动态候选集: 候选集的大小是动态变化的。如果模型非常确定下一个词（某个词概率很高），候选集可能很小；如果模型不太确定（多个词概率相近），候选集就会变大，从而增加多样性。\n优点 (如文中所述): 它能生成更多样、更自然、更具创造性的文本。因为它不是死板地选择最优解，所以能探索更多可能性，从而为模型（如CapFilt）提供包含新知识的、更高质量的训练数据。\n\n\n\n\n\n\n\n4.4. 参数共享与解耦本节探讨了模型设计中的两个关键策略：预训练期间的参数共享和CapFilt微调期间的参数解耦。\n\n\n预训练中的参数共享:\nBLIP的默认策略是让文本编码器和解码器共享除自注意力（SA）层之外的所有参数。\n消融实验的结果表明，这是在性能和效率之间取得最佳平衡的策略。\n如果共享所有层（包括SA层），模型性能会因为编码（需要双向理解上下文）和解码（需要因果预测）任务之间的内在冲突而显著下降。\n如果完全不共享任何参数，虽然性能与默认策略相近，但模型参数量会大幅增加，从而降低了训练和推理的效率。\n\n\n\n\n\nCapFilt中的参数解耦:\n在CapFilt流程中，字幕器和过滤器是从同一个预训练模型初始化后，独立进行微调的，这一过程被称为解耦。\n实验对比了这种解耦策略与在微调时依然让字幕器和过滤器共享参数的策略。\n结果显示，共享参数会导致最终模型在下游任务上的性能下降。\n其主要原因被归结为确认偏差 (confirmation bias)。当参数共享时，过滤器和字幕器的耦合过于紧密，导致过滤器对字幕器产生的噪声字幕“不够严格”，不容易将其过滤掉。这一点可以从噪声拒绝率的显著降低（从25%降至8%）得到验证。因此，将两者解耦，让它们独立微调，是保证过滤效果、提升数据质量的关键。\n\n\n\n5. 与最先进技术的比较 (Comparison with State-of-the-arts)本章节将BLIP模型与现有的视觉语言预训练（VLP）方法，在一系列广泛的下游任务上进行全面比较，以验证其性能。值得注意的是，实验基准中省略了SNLI-VE任务，因为其测试数据据报道存在噪声问题。\n5.1. 图文检索 (Image-Text Retrieval)\n任务与数据集: 该任务评估模型在给定文本时检索相关图像（Text-to-Image Retrieval, IR）和在给定图像时检索相关文本（Image-to-Text Retrieval, TR）的能力。实验在COCO和Flickr30K这两个标准数据集上进行。\n微调与推理策略:\n微调阶段，模型使用图文对比损失（ITC）和图文匹配损失（ITM）进行联合优化。\n为了在推理时兼顾速度和精度，采用了两阶段的检索策略：\n候选筛选: 首先，利用计算速度较快的图文特征相似度（通过ITC学习得到）快速地从整个库中筛选出前k个最相关的候选者。\n重排序: 然后，仅对这k个候选者计算计算成本更高但更精确的图文匹配（ITM）分数，并以此进行重排序，得到最终结果。\n\n\n在COCO数据集上，k设为256；在Flickr30K上，k设为128。\n\n\n结果分析:\n微调后检索 (Finetuned Retrieval): 如表5所示，BLIP的性能全面超越了现有方法。特别是在使用相同的1400万预训练图像时，BLIP在COCO数据集上的平均Recall@1指标比之前的最佳模型ALBEF高出2.7%。\n零样本检索 (Zero-shot Retrieval): 如表6所示，将在COCO上微调好的模型直接迁移到Flickr30K上进行测试（零样本设置），BLIP同样以巨大优势超越了包括CLIP和ALIGN在内的现有方法。\n\n\n\n\n5.2. 图像字幕生成 (Image Captioning)\n任务与数据集: 该任务要求模型为给定图像生成描述性文本。实验在COCO数据集上进行微调和评估，并在NoCaps数据集上进行评估，以测试模型对新物体的泛化能力。\n微调与推理策略: 模型使用语言建模损失在COCO训练集上进行微调。在推理时，研究者发现，在待生成的字幕开头添加提示语“a picture of”（一张……的图片）可以略微提升效果。\n结果分析:\n如表7所示，使用1400万图像预训练的BLIP，其性能显著优于使用相似规模数据预训练的其他方法。\n使用12900万图像预训练的BLIP，其性能与使用2亿图像的LEMON模型相当。\n尤其重要的是，BLIP在实现高性能的同时，推理效率远高于LEMON等方法，因为它不依赖于计算成本高昂的物体检测器，并且使用的输入图像分辨率更低（ vs ）。\n\n\n\n\n5.3. 视觉问答 (Visual Question Answering, VQA)\n任务与数据集: VQA任务要求模型根据图像内容回答一个相关问题。实验在VQA2.0数据集上进行。\n任务范式: 与许多将VQA视为多答案分类任务的方法不同，BLIP创新地将其视为一个答案生成 (answer generation) 任务。这种范式使得模型能够处理开放式的问题，而不是从预设的答案列表中选择。\n模型架构与微调: 如图5(a)所示，在微调时，模型被重组：图像和问题首先被编码为多模态嵌入，然后该嵌入被送入一个答案解码器中，以自回归的方式生成答案。模型使用LM损失进行端到端的微调。\n结果分析:\n如表8所示，使用1400万图像预训练的BLIP，在VQA测试集上的得分比ALBEF高出1.64%。\n使用12900万图像预训练的BLIP，其性能甚至优于使用了13倍训练数据（1.8B）和更复杂视觉骨干网络的SimVLM。\n\n\n\n\n5.4. 自然语言视觉推理 (Natural Language Visual Reasoning, )\n任务与数据集: 该任务要求模型判断一个句子是否准确地描述了一对（两张）图像，需要模型具备跨图像的推理能力。\n模型架构修改: 为了处理双图像输入，研究者对预训练模型做了一个简单而高效的修改。如图5(b)所示，在图像接地的文本编码器的每个Transformer块中，设置了两个并行的交叉注意力（CA）层，分别处理两张输入图像。这两个CA层由同一个预训练权重初始化。它们的输出在一个合并层（Merge Layer）中被融合，然后再送入前馈网络（FFN）。这种设计比之前的方法在计算上更高效。\n结果分析: 如表8所示，BLIP的性能优于所有现有方法，除了进行了额外定制化预训练的ALBEF。一个有趣的发现是，增加更多的网络图像数据对任务的性能提升不大，这可能是由于网络数据与该任务所需的数据领域之间存在较大差异。\n\n5.5. 视觉对话 (Visual Dialog, VisDial)\n任务与数据集: 该任务将VQA扩展到多轮对话场景，模型需要根据图像、当前问题以及之前的对话历史来预测答案。实验遵循判别式设定，即从一个候选答案池中选出正确答案。\n模型架构与微调: 如图5(c)所示，图像和其标题的嵌入被拼接后，通过交叉注意力送入一个对话编码器。该编码器结合问题和对话历史，使用ITM损失进行训练，以判断候选答案对于当前对话上下文是否正确。\n结果分析: 如上面的表9所示，BLIP在VisDial v1.0验证集上取得了最先进的性能。\n\n5.6. 向视频-语言任务的零样本迁移\n核心思想: 为了验证BLIP学到的图文表征的强大泛化能力，研究者将其直接应用于视频-语言任务，而不进行任何针对视频数据的微调。\n视频处理策略: 采用了一种非常简单的方法来处理视频输入：从每个视频中均匀采样N帧（检索任务N=8，问答任务N=16），然后将这些帧的特征拼接成一个单一的序列输入给模型。这种方法完全忽略了视频中的时序信息。\n结果分析:\n尽管存在图像和视频之间的领域差异，并且缺乏任何时序建模，BLIP在文本到视频检索（表10）和视频问答（表11）两个任务上均取得了惊人的、最先进的零样本性能。\n尤其是在文本到视频检索任务上，零样本的BLIP性能甚至比那些在目标视频数据集上进行过完全微调的模型还要高出12.4% (Recall@1)，这强有力地证明了BLIP框架学习到的视觉-语言对齐能力的高度通用性。\n\n\n\n\n6. 额外的消融研究 (Additional Ablation Study)本章节通过额外的消融实验，进一步论证CapFilt机制的有效性并非源于其他无关因素。\n\nCapFilt的提升是否源于更长的训练时间？\n问题: 由于CapFilt生成的自举数据集比原始数据集包含更多的文本样本，因此在训练相同epoch数的情况下，其实际训练步数更多。性能提升是否仅仅是因为训练时间更长？\n实验设计: 为了验证这一点，研究者将原始的噪声网络文本进行复制，使其每个epoch的训练样本数与自举数据集完全相同。\n结果与结论: 如表12所示，仅仅在原始噪声数据上进行更长时间的训练并不能带来性能提升。这证明了CapFilt的有效性来自于其提升了数据质量，而非简单地增加了训练时长。\n\n\n\n\n\n使用自举数据集时是否需要训练新模型？\n问题: 在获得自举数据集后，是应该用它从头训练一个新模型，还是可以直接在生成该数据集的旧模型上继续训练？\n实验设计: 实验比较了“从头训练新模型”与“在旧模型上继续训练”两种策略。\n结果与结论: 如表13所示，从头训练一个新模型的效果优于在旧模型上继续训练。这一发现与知识蒸馏领域的普遍认知相符，即学生模型通常不应从教师模型的权重初始化，以避免陷入次优解，并能更好地从高质量数据中学习。在更高质量的数据集上从零开始，能让模型探索更优的参数空间。\n\n\n\n\n","slug":"BLIP论文精读","date":"2025-07-06T01:10:15.000Z","categories_index":"论文精读","tags_index":"BLIP,多模态","author_index":"犬夜叉"},{"id":"b18ad5e5297b60924e424af2ef6c52c5","title":"Flamingo论文精读","content":"Introduction1. 核心挑战与现有方法的局限性多模态机器学习领域面临一个核心的开放性挑战：如何构建能够仅凭少数几个标注样本就迅速适应新任务的模型。 这种快速学习能力是智能的一个关键特征，即在接收到简短指令后能学会执行新任务。 \n目前，计算机视觉领域最广泛应用的范式仍然是“预训练-微调”（pre-training and fine-tuning）。 这个过程通常包括两个阶段：\n\n预训练阶段：在一个大规模的、有监督的数据集上预先训练模型。\n微调阶段：在目标任务的特定数据集上对预训练好的模型进行参数微调。 \n\n然而，这种主流范式存在显著的缺点：\n\n数据依赖性强：成功的微调往往需要成千上万个为特定任务标注的数据点，获取这些数据成本高昂。\n\n高昂的调优成本：针对每个新任务，都需要进行仔细的超参数调整，这是一个繁琐且耗费计算资源的过程。\n\n资源密集：整个微调过程需要大量的计算资源。\n\n\n为了克服微调的限制，近年来出现了一些新的方法，但它们同样有其局限性：\n\n对比学习模型 (Contrastive Models)：像CLIP这样的多模态视觉语言模型，通过对比学习目标进行训练，实现了对新任务的“零样本”适应，无需微调。 它们的工作原理是为文本和图像学习一个共享的嵌入空间，并计算两者之间的相似度分数。 这种机制限制了它们的应用场景，主要适用于分类等选择题式的任务，即从一个预先给定的有限选项中做出选择。 关键的缺陷在于，这类模型无法生成语言，这使得它们不适用于更开放式的任务，例如视觉问答（VQA）或图像描述（Captioning），因为这些任务需要模型生成自由形式的文本答案。 \n\n视觉条件下的语言生成模型 (Visually-conditioned Language Generation Models)：虽然有一些研究探索了根据视觉输入生成文本的模型，但这些模型在数据量较少（即小样本）的情况下，尚未展现出足够好的性能。 \n\n\n图1：从Flamingo-80B获得的输入和输出精选示例。Flamingo可通过少样本提示快速适应各种图像/视频理解任务。此外，Flamingo原生支持多图像视觉对话。\n2. Flamingo模型的提出与核心能力为了解决上述挑战，本研究引入了一个名为Flamingo的视觉语言模型系列。 Flamingo的核心突破在于其卓越的小样本学习（few-shot learning）能力。 \n\n工作范式：Flamingo通过“提示（prompting）”来适应新任务。用户只需向模型提供几个包含输入/输出对的样本（例如，&lt;图片，对应描述&gt; 或 &lt;图片，问答对&gt;），模型就能理解任务要求并对新的查询图片或视频生成相应的文本输出。 这种方式极大地降低了对大量标注数据的依赖。\n\n卓越的性能：在一个包含16个不同的视觉和语言任务的广泛评测中，Flamingo展现了最先进的小样本学习性能。 更为引人注目的是，在其中的6个任务上，Flamingo仅使用极少数的样本（例如32个），其性能就超越了在数千甚至数万倍任务专属数据上进行微调的现有最先进模型。 \n\n\n\n3. 设计思想：将大语言模型的能力扩展至多模态领域Flamingo的设计灵感主要来源于近年来在大型语言模型领域取得的巨大成功，例如GPT-3等模型展现出的强大的小样本学习能力。 \n\nLMs的小样本学习机制：一个强大的大型语言模型能够仅通过其文本接口来执行多种任务。具体做法是，将任务的几个示例（examples）和新的查询输入（query input）一起打包成一个文本提示（prompt），然后模型会自回归地生成一个续写，这个续写就是对查询的预测输出。 \n\n将该机制迁移至视觉任务：本研究证明，同样的方法论可以被成功地应用于图像和视频理解任务。 像分类、描述生成、视觉问答等任务，都可以被重新定义和构建为以视觉输入为条件的文本预测问题。 \n\n与纯语言模型的关键区别：与仅处理文本的LM不同，视觉任务需要模型能够处理一个多模态的提示（multimodal prompt），这个提示中包含了与文本交错在一起的图像或视频。 Flamingo模型的核心能力之一就是处理这种任意交错的视觉和文本序列。 \n\n\n4. Flamingo的架构理念与关键组件Flamingo模型是一个能够接收文本、图像、视频交错序列作为输入，并生成自由文本作为输出的视觉条件自回归文本生成模型。 其架构设计的核心理念是有效地连接和利用两个强大的、预训练好的互补模型，同时保留它们在各自领域学到的丰富知识。\n\n利用预训练模型：Flamingo架构的基础是两个预训练且被冻结（frozen）的模型：\n\n一个视觉模型，负责“感知”视觉场景。 \n\n一个大型语言模型，负责执行基本的推理和文本生成。 \n\n\n\n创新的桥接设计：在冻结的视觉和语言模型之间，引入了全新设计的、从零开始训练的架构组件。 这种“桥接”方式至关重要，因为它可以在不破坏原有模型知识（防止灾难性遗忘）的前提下，高效地将视觉信息融入到语言模型的处理流程中。 \n\n高效处理高分辨率视觉输入：为了处理高分辨率的图像或视频，Flamingo采用了一个基于Perceiver的架构。 该模块可以将视觉编码器产生的大量、可变数量的视觉特征，压缩并重采样成一小组固定数量的“视觉令牌（visual tokens）”。 这一设计极大地提高了处理效率。\n\n\n\n5. 训练策略的关键性大型语言模型的强大性能很大程度上归功于其在海量文本数据上的训练，这赋予了它们通用的文本生成能力，从而在接收到任务提示时表现出色。 \n\n训练数据的重要性：与此类似，本研究证明，Flamingo模型的训练方式对其最终的性能至关重要。 \n\n独特的训练数据：模型在一个精心挑选的、大规模、多模态的网络语料库上进行训练。这些数据的一个关键特征是包含了任意交错的文本和图像，这与传统的成对（image-text pair）数据有本质区别。 \n\n实现小样本能力的关键：正是这种在真实网络页面数据上的训练，才赋予了Flamingo强大的上下文小样本学习（in-context few-shot learning）能力。 经过这样的训练后，模型无需任何针对特定任务的微调，就能直接通过小样本提示的方式适应新的视觉任务。 \n\n\nApproach本节详细阐述了Flamingo模型的架构设计、工作原理与训练策略。Flamingo是一个视觉语言模型，其核心功能是接收以任意方式穿插的文本与图像/视频序列作为输入，并以自回归的方式生成自由形式的文本作为输出。模型设计的指导思想是高效地桥接两个强大的、已预训练好的独立模型——一个用于视觉感知的模型和一个用于语言理解与生成的模型，从而在不破坏各自预训练知识的前提下，实现强大的多模态处理能力。\n2.1 视觉处理与Perceiver Resampler模块视觉信息的处理是整个模型的第一步，其目标是将原始的像素输入转化为紧凑且固定长度的、可供语言模型利用的表征。\n\n视觉编码器 (Vision Encoder)：\n\n模型选择：采用了一个预训练好且在整个Flamingo训练过程中保持冻结的Normalizer-Free ResNet (NFNet) 模型（具体为F6版本）。冻结视觉编码器可以保留其强大的、泛化的视觉特征提取能力，并节省大量计算资源。\n预训练方式：该视觉编码器是在大规模图文对数据集上通过对比学习目标独立预训练的。这种训练方式旨在让模型学习到一种通用的视觉表示，使其能够理解图像内容并与文本描述对齐。\n特征输出：对于图像输入，编码器输出其网络末端的一个二维空间特征图。这个特征图保留了图像的空间信息，随后被展平为一维的特征序列。对于视频输入，首先以1帧/秒的速率对视频进行采样，然后独立地对每一帧进行编码，得到一系列的帧特征。为了融入时序信息，模型会为这些帧特征添加一个可学习的时序嵌入，形成一个三维时空特征网格。最后，这个三维特征同样被展平为一维序列，准备送入下一模块。\n\n\nPerceiver Resampler (感知器重采样器)：\n\n核心功能与目的：该模块是连接视觉编码器和冻结语言模型的关键桥梁。它的核心任务是接收来自视觉编码器的大量且可变长度的视觉特征（无论是来自高分辨率图像还是长视频），并将其压缩成少数固定数量的视觉令牌，在本研究中固定为64个。\n解决的痛点：直接将高维度的视觉特征输入到大型语言模型中进行注意力计算，其计算成本会非常高昂。Perceiver Resampler通过显著减少视觉令牌的数量，极大地降低了后续视觉-文本交叉注意力（cross-attention）的计算复杂度，使得整个模型更加高效。\n工作机制：其设计借鉴了Perceiver和DETR等模型的思想。它定义了一组预设数量的、可学习的潜在输入查询（latent input queries）。这些查询向量作为“信息汇总器”，通过一个Transformer结构中的交叉注意力机制，去“观察”和“查询”由视觉编码器生成的大量视觉特征。通过这个过程，它们将视觉特征中的核心信息“蒸馏”并吸收到自身中。最终，这些经过信息蒸馏的查询向量的输出，就构成了那一小组固定数量的视觉令牌。实验证明，这种方法比使用简单的多层感知机或标准的Transformer来进行特征池化效果更优。\n\n\n\n2.2 在视觉表征上对冻结语言模型进行条件化模型的文本生成能力由一个强大的、预训练好的冻结语言模型提供。为了让这个LM能够“看到”图像内容，需要将Perceiver Resampler产生的视觉令牌有效地融入其处理流程中。\n\n\nGATED XATTN-DENSE（门控交叉注意力-稠密连接）层：\n\n非侵入式集成：为了保留LM强大的预训练知识并防止“灾难性遗忘”（即模型在学习新知识时忘记旧知识），Flamingo不直接微调LM的参数。相反，它在原始LM的各个预训练层之间，插入了若干个全新的、从零开始训练的模块，即GATED XATTN-DENSE层。\n结构与功能：\n交叉注意力 (XATTN)：这是实现视觉与语言融合的核心。在这一层中，注意力机制的查询（Query, Q）来自于前一个冻结LM层的文本表征，而键（Key, K）和值（Value, V）则来自于Perceiver Resampler输出的视觉令牌。这使得在生成每一个文本词元时，模型都能够有针对性地“关注”视觉输入中的相关区域。\n稠密连接 (DENSE)：交叉注意力层之后连接一个标准的前馈神经网络（Feed-Forward Network），进行进一步的特征转换。\n门控机制 (GATED)：这是一个对训练稳定性和最终性能至关重要的设计。新添加模块的输出并不会直接与原始的文本表示相加。相反，它会经过一个tanh门控机制。该机制通过一个可学习的标量参数  来控制新模块的输出贡献。这个  被初始化为0。\n\n\n初始化优势：由于在初始化时 ，所以 tanh(α) 也为0，导致整个新添加模块的输出在训练开始时为零。这意味着，在训练初期，整个Flamingo模型的输出与那个未经改动的、冻结的LM完全相同。这保证了训练的稳定启动，避免了随机初始化的新层对强大预训练模型的干扰。随着训练的进行，模型会逐渐学习调整  的值，从而平滑地、自适应地将视觉信息融合进来。\n\n\n模型规模：研究团队基于Chinchilla系列语言模型构建了三种不同规模的Flamingo：Flamingo-3B, Flamingo-9B, 和 Flamingo-80B。在扩大模型尺寸时，主要是增加了冻结LM的参数量以及可训练的GATED XATTN-DENSE模块的数量，而视觉编码器和Perceiver Resampler的尺寸在不同规模的模型间保持不变。\n\n\n2.3 多视觉输入支持：逐图像/视频的注意力掩码为了让模型能够处理包含多个视觉输入的提示（例如，在小样本学习场景下，提示中包含多个&lt;图像, 文本&gt;对），并正确地将文本与对应的视觉输入关联起来，Flamingo采用了一种精巧的注意力掩码策略。\n\n工作机制：在进行文本到图像的交叉注意力计算时，模型采用了因果掩码（causal masking）。具体来说，当模型正在预测某个文本词元时，它的交叉注意力模块被限制为只能关注（attend to）在交错序列中紧邻于它之前的那个图像/视频所对应的视觉令牌。模型不能直接通过交叉注意力“回顾”更早出现的其他所有图像。\n信息流的保持：尽管交叉注意力被限制在单个最近的图像上，但对先前所有图像的信息依赖性并不会丢失。这些信息是通过语言模型内部的自注意力机制来间接维持的。当模型处理完一个&lt;图像, 文本&gt;对后，视觉信息已经影响了生成的文本，这些文本作为历史信息存储在LM的状态中。在后续步骤里，LM可以通过自注意力机制访问和利用这些包含了早前视觉信息的状态。\n核心优势：这种“单次只看一张图”的交叉注意力方案，不仅计算高效，更重要的是赋予了模型极佳的泛化能力。它使得模型可以无缝地处理任意数量的视觉输入，即使在训练时接触的图像数量有限（例如，训练时最多使用5张图），在推理时也能从多达32个图文对中获益。\n\n2.4 在混合视觉与语言数据集上的训练Flamingo的小样本学习能力严重依赖于其独特的训练数据和策略。模型在一个混合了三种从网络上爬取的数据集上进行训练，未使用任何专为机器学习目的而人工标注的数据。\n\n数据集构成：\n\nM3W (MultiModal MassiveWeb)：这是一个包含约4300万个网页的交错图文数据集。通过解析网页的DOM树结构，将图像以&lt;image&gt;标签的形式插入到其在原文中相应位置的文本流中，从而构建出自然的、图文混排的训练样本。这是训练模型理解上下文和进行小样本学习的关键。\n图文对数据集：这部分由两块组成，一是公开的ALIGN数据集（18亿图文对），二是团队自己收集的、描述更长更优质的LTIP数据集（3.12亿图文对）。\n视频文本对数据集 (VTP)：一个包含2700万个短视频及其文本描述的数据集。\n\n\n语法对齐与多目标训练：\n\n为了统一训练格式，所有图文对和视频文本对数据都被处理成与M3W相似的语法，即在文本描述前后分别加上&lt;image&gt;和&lt;EOC&gt;（end of chunk）特殊标记。\n论文通过最小化给定视觉输入时每个数据集的文本期望负对数似然加权和来训练模型： , 其中是输入文本的第个语言标记，是前面的标记集合，是交错序列中先于标记的图像/视频集合，和分别表示第个数据集及其权重。调整每个数据集的权重是提升性能的关键。该论文对所有数据集的梯度进行累积，实验表明该方法优于“轮询”策略。\n在优化策略上，团队发现梯度累积的方式，即计算完所有数据集的梯度后再进行一次统一的参数更新，比轮流在单个数据集上训练的“round-robin”方法效果更佳。\n\n\n\n2.5 利用小样本上下文学习进行任务适配模型一旦训练完成，就可以通过上下文学习（in-context learning）的方式快速适应新的视觉任务，而无需任何参数更新。\n\n提示构建 (Prompting)：为了解决一个新任务，用户需要构建一个多模态提示。这个提示由若干个“支持样本”和一个“查询样本”组成。例如，可以这样构建提示：&lt;图片1&gt; &lt;答案1&gt; &lt;图片2&gt; &lt;答案2&gt; ... &lt;查询图片&gt;。模型在看到这个提示后，会续写出针对查询图片的答案。\n评估方式：\n对于开放式任务（如VQA），使用集束搜索解码策略来生成最可能的自由文本答案。\n对于封闭式任务（如多项选择），模型会分别计算每个选项作为答案的对数似然概率（log-likelihood），并选择概率最高的那个作为最终答案。\n\n\n零样本泛化：研究还探索了一种特殊的零样本设置，即在提示中只提供任务的纯文本示例（不附带图像），以测试模型是否能仅从文本描述中理解任务的格式和要求。\n\nExperiments本部分旨在通过一系列广泛的实验来评估Flamingo模型的性能，核心目标是验证其在多样化且具有挑战性的任务上快速适应的能力。\n实验设置与评估策略\n\n评测基准：为了全面评估模型，实验覆盖了16个当前流行的多模态基准数据集。这些数据集涵盖了图像和视频理解的多个方面，包括图像描述、视频描述、视觉问答、视频问答、视觉对话以及多项选择题等。\n\n开发集与留出集（Held-out Set）：为了保证评估的公正性和科学性，实验将这16个基准分为了两组：\n\n开发集（DEV Set）：包含5个基准（COCO, OKVQA, VQAv2, MSVDQA, VATEX）。这组数据集在研究过程中被用来验证模型的设计决策和调整超参数。研究者承认，由于模型在开发阶段“看到”了这些任务，其在这些基准上的最终性能评估可能存在偏向性（即可能被高估），但这种做法在领域内是普遍的。\n留出集：包含其余的11个基准。这组数据集在模型设计和超参数选择的整个过程中都未被使用。它们仅在最后阶段被用来评估模型的最终性能，从而为模型的小样本学习能力提供一个无偏的、更可信的估计。\n\n\n数据划分：为了在开发集上进行严谨的评估并避免数据泄露，每个开发集基准都被划分为四个子集：验证集的支持样本（用于开发阶段构建提示）、验证集的查询样本（用于开发阶段评估）、测试集的支持样本（用于最终报告构建提示）、测试集的查询样本（用于最终报告评估）。对于留出集，则只需要测试集的支持和查询样本。\n\n评估超参数：为了证明模型的通用性，所有评估用的超参数（如解码策略等）在全部16个基准上都保持固定。根据任务的性质（例如是生成式还是选择式），会采用四种预设的提示模板中的一种。\n\n\n3.1 小样本学习性能这是实验的核心部分，旨在衡量模型在仅有少量标注样本的情况下学习新任务的能力。\n\n与先前小样本方法的对比：实验结果表明，在所有16个评测基准上，Flamingo的性能都显著超越了以往所有已发表的零样本或小样本方法。这一成就仅需每个任务提供极少数（例如4个）的示例即可实现，充分展示了模型高效且实用的任务适应能力。\n\n\n\n与完全微调（Fine-tuned）方法的对比：更引人注目的是，Flamingo的性能不仅在小样本领域领先，甚至能与那些在成千上万、乃至数十万个任务专属标注数据上进行完全微调的当前最先进（SOTA）模型相媲美。在其中的6个任务上，Flamingo仅凭一个通用的、未经微调的模型和32个任务样本，就超越了这些经过大量数据微调的SOTA模型。\n\n泛化能力验证：模型在11个留出基准上的强劲表现，证实了其设计和训练方法的泛化能力。这表明模型的优异性能并非过拟合于开发集，而是具备广泛适用性的。\n\n规模效应分析（Scaling Analysis）：\n\n模型参数规模：性能随着模型参数量的增加而稳步提升，即Flamingo-80B优于Flamingo-9B，后者又优于Flamingo-3B。这与在大型语言模型领域观察到的“规模法则”（Scaling Law）相一致，即更大的模型通常具备更强的学习能力。\n样本数量（Number of Shots）：对于同一个模型，其性能随着在提示中提供的上下文样本（shots）数量的增加而提高。\n协同效应：研究发现，最大的模型（Flamingo-80B）能更好地利用更多的上下文样本。这表明模型规模和上下文信息量之间存在协同效应，更大的模型能更有效地从示例中学习。\n架构灵活性的体现：一个非常关键的发现是，尽管模型在训练阶段最多只接触过包含5张图像的序列，但在推理（评估）时，它却能有效利用多达32张图像或视频的上下文信息并持续提升性能。这有力地证明了Flamingo架构（特别是其逐图像的注意力掩码机制）的灵活性和卓越的泛化能力。\n\n\n\n3.2 Flamingo作为预训练模型进行微调的性能尽管小样本学习是核心焦点，本部分也探索了当有充足标注数据时，将Flamingo作为预训练模型进行传统微调的潜力。\n\n微调方法：研究团队对最大规模的Flamingo模型，在有大量标注数据的任务上进行微调。微调过程采用了一个较短的训练周期和较小的学习率。一个关键的改动是，在微调期间解冻了视觉主干网络。这使得模型的视觉部分也能针对特定任务进行调整，例如适应更高分辨率的图像输入。\n\n微调结果：通过微调，模型的性能在之前小样本学习的基础上得到了进一步提升。在那些之前通过小样本学习未能达到SOTA的9个任务中，微调后的Flamingo在其中的5个任务上（VQAv2, VATEX, VizWiz, MSRVTTQA, HatefulMemes）刷新了最先进记录（SOTA）。\n\n\n\n3.3 消融研究为了理解模型各个组件和设计选择的重要性，研究者进行了一系列消融实验。这些实验主要在较小的Flamingo-3B模型上进行，以节省计算成本。\n\n训练数据组合的重要性：\n\n移除交错图文数据集会导致模型性能出现超过17%的灾难性下降，这证明了在自然混排的图文数据上进行训练对于培养小样本学习能力至关重要。\n移除传统的图文对数据集同样会使性能下降近10%，这表明交错数据和成对数据是互补的，两者都不可或缺。\n移除视频-文本数据集会对所有视频相关任务的性能产生负面影响。\n\n\n视觉条件化架构的关键设计：\n\n门控机制：在融合视觉和文本信息时使用的tanh门控机制至关重要。移除这个机制不仅导致性能下降4.2%，还会引发训练过程的不稳定。\n交叉注意力架构：实验证明，本文提出的GATED XATTN-DENSE架构优于其他可替代的方案，如vanilla交叉注意力或一些“嫁接”（grafting）方法。\n\n\n计算与性能的权衡：\n\n交叉注意力频率：在语言模型的每一层之间都插入新的交叉注意力模块能获得最佳性能，但这会显著增加训练的参数量和时间复杂度。实验发现，每隔4层插入一个模块是一个极佳的权衡点，它能将训练速度提升66%，而整体性能仅有1.9%的微小下降。基于这个发现，更大的模型采用了这种稀疏插入策略以在硬件限制下实现最优配置。\nResampler架构：Perceiver Resampler在性能和速度上均优于使用普通MLP或Transformer作为重采样器的替代方案。\n\n\n视觉编码器的影响：使用一个强大的视觉编码器非常重要。团队自己预训练的NFNet-F6编码器显著优于公开的CLIP ViT-L/14模型，证明高质量的视觉特征是模型性能的基石。\n\n冻结语言模型的必要性：这是整个方法论中最关键的发现之一。\n\n如果从零开始训练语言模型部分，性能会暴跌12.9%。\n更重要的是，即便是微调（而非冻结）预训练好的语言模型，也会导致8%的显著性能下降。这清晰地揭示了“灾难性遗忘”现象：在适应新的多模态目标时，语言模型会忘记其预训练阶段学到的宝贵的、通用的语言知识。因此，冻结语言模型是保证性能的必要手段，是一种比将纯文本预训练数据混入多模态训练中更优的策略。\n\n\n\n\nRelated Work本节将Flamingo模型置于现有研究的广阔背景之下，阐述其与相关工作的联系与区别，主要涵盖三个领域：语言建模与小样本适应、视觉与语言的交叉研究，以及网络规模的训练数据集。\n1. 语言建模与小样本适应\n技术背景：近年来，基于Transformer架构的语言模型取得了巨大进步，“预训练-再适应”已成为标准范式。Flamingo正是构建在这一坚实基础之上，其核心语言模块采用了强大的Chinchilla 70B大型语言模型。\n\n模型适应技术的多样性：将预训练好的大型语言模型适配到新任务上有多种技术路径：\n\n适配器模块（Adapter Modules）：在预训练模型的层与层之间插入一些小型的、可训练的神经网络模块，在适配新任务时只训练这些适配器，而保持主干模型冻结。\n部分参数微调：只微调模型中一小部分的参数（例如，只微调偏置项 bias），在保持大部分参数不变的情况下实现任务适配。\n提示优化（Prompt Optimization）：保持模型完全不变，而是通过梯度下降等方法来学习最优的、能够引导模型产生正确输出的输入提示。\n上下文学习（In-context Learning）：这是Flamingo采用的思路，其灵感直接来源于GPT-3等模型。这种方法无需任何梯度更新或参数修改，仅通过在模型的输入中提供几个任务示例，就能引导模型在推理时执行新任务。\n\n\nFlamingo的选择与定位：相较于其他需要复杂梯度优化的方法，Flamingo选择了更为简洁的上下文学习路径。它避开了基于度量学习（metric learning，旨在学习一个好的样本间相似度函数）或元学习（meta-learning，即“学会如何学习”，旨在让模型能从少量数据中快速学习）等更为复杂的少样本学习框架，转而将纯文本领域的上下文学习成功地推广到了多模态领域。\n\n\n2. 当语言与视觉相遇\nBERT的深远影响：语言模型（尤其是BERT）的成功极大地启发了视觉语言领域的研究。大量V-L模型借鉴了BERT的架构，使用Transformer来融合视觉和文本特征。然而，这些模型与Flamingo的一个根本区别在于，它们大多需要在下游任务上进行微调才能获得良好性能，而Flamingo则专注于无需微调的小样本学习。\n\n对比学习模型：这是V-L领域的另一大分支（如CLIP）。这类模型通过对比学习来对齐图像和文本的表示，从而计算它们之间的相似度。虽然Flamingo的视觉编码器本身是使用对比学习预训练的，但Flamingo模型整体的功能远超于此。对比学习模型只能进行“打分”，无法生成自由形式的文本，而Flamingo的核心能力之一正是生成性。\n\n自回归视觉语言模型：Flamingo属于能够自回归生成文本的VLM家族。在它出现的同时期，也有其他一些工作探索了将多种视觉任务统一表述为文本生成问题的范式。\n\n基于冻结语言模型的研究趋势：为了防止在多模态训练中破坏大型语言模型预训练好的强大能力（即“灾难性遗忘”），近期的一系列工作开始探索冻结语言模型的方案。Flamingo正是这一思想的践行者和集大成者。\n\nFlamingo的核心创新：尽管存在上述种种相关工作，Flamingo的独特性和核心创新在于，它是首个能够处理任意交错（arbitrarily interleaved）的图像、视频和文本序列的语言模型。这与之前大多数只能处理单个图像/视频与文本对的模型相比，是一个质的飞跃，使其能够处理更复杂、更自然的真实世界多模态场景。\n\n\n3. 网络规模的视觉与语言训练数据集\n数据瓶颈：高质量、人工标注的视觉语言数据集（如COCO, VQA）规模通常在数万到数十万级别，获取成本高昂，这限制了模型的扩展性。\n网络数据的利用：为了突破这一瓶颈，许多研究转向从互联网上自动爬取海量的、自然存在的图文对数据。\nFlamingo的贡献：Flamingo不仅利用了这种大规模的图文对数据，还进一步证明了训练数据的形态至关重要。它的一个关键贡献是证明了在包含交错图文的完整网页上进行训练的巨大价值。这种将网页视为一个单一、连贯的多模态序列的训练方式，是其强大上下文学习能力的关键来源。\n与同期工作的比较：同期的CM3模型也使用了网页数据进行训练。但两者目标不同：CM3旨在生成HTML标记语言，而Flamingo将任务简化为生成纯文本；在评估上，CM3更侧重于语言任务，而Flamingo的重点是验证在各类视觉任务上的小样本学习能力。\n\n","slug":"flamingo论文精读","date":"2025-07-05T02:32:50.000Z","categories_index":"论文精读","tags_index":"多模态,小样本","author_index":"犬夜叉"},{"id":"5cac6105e8bc407ec91f6896ed602917","title":"ALBEF论文精读","content":"\n\n\n\n\n\n摘要\n大规模视觉语言表示学习已在各类视觉语言任务上展现出显著改进。现有大多数方法采用基于Transformer的多模态编码器来联合建模视觉标记（基于区域的图像特征）和文本标记。由于视觉标记与文本标记处于未对齐的空间，多模态编码器难以学习图像-文本的交互关系。在本文中，我们提出一种对比损失，用于在通过跨模态注意力融合图像和文本表示之前先进行对齐（即ALBEF），这使得视觉语言表示学习更具语义根基。与大多数现有方法不同，我们的方法无需边界框标注，也不需要高分辨率图像。为提升从噪声Web数据中学习的能力，我们提出动量蒸馏（Momentum Distillation），这是一种自训练方法，通过动量模型生成的伪目标进行学习。我们从互信息最大化的视角对ALBEF进行了理论分析，表明不同训练任务可解释为为图像-文本对生成“视图”的不同方式。ALBEF在多个下游视觉语言任务上实现了SOTA性能：在图像-文本检索任务中，ALBEF超越了在数据规模大若干数量级的数据集上预训练的方法；在VQA和NLVR2任务中，ALBEF相比SOTA方法分别实现了2.37%和3.84%的绝对性能提升，同时推理速度更快。代码和模型见https://github.com/salesforce/ALBEF。\n\n\n论文链接：[https://arxiv.org/abs/2107.07651]\nIntroduction1. 视觉-语言预训练（VLP）的现状与挑战视觉-语言预训练（VLP）旨在通过大规模图像-文本对学习多模态表示，以提升下游任务性能。现有主流方法（如LXMERT、UNITER、OSCAR）依赖预训练的物体检测器提取基于区域的图像特征，并通过多模态编码器融合图像与文本标记。然而，这类方法存在以下关键局限性：  \n\n（1）模态特征未对齐：图像区域特征与文本标记处于不同语义空间，多模态编码器难以有效建模跨模态交互关系。  \n（2）物体检测器的高成本：预训练需边界框标注，推理时依赖高分辨率图像。  \n（3）噪声数据过拟合：主流图像-文本数据集收集自网络，存在大量噪声，传统预训练目标（如掩码语言建模MLM）易过拟合，导致模型泛化能力下降。  \n\n2. ALBEF框架的核心解决方案针对上述问题，论文提出 ALign BEfore Fuse（ALBEF）框架，其核心思路为：  \n\n（1）对齐先行（ALign）：在融合图像-文本特征前，通过图像-文本对比学习（ITC）对齐单模态表示。  \n使用无检测器的图像编码器（ViT）和文本编码器（BERT）独立编码，通过对比损失学习跨模态相似性函数，将图像与文本映射到共享语义空间，降低多模态编码器的建模难度。  \n引入动量编码器维护历史特征队列，通过InfoNCE损失最大化正样本互信息，并利用对比硬负样本挖掘提升样本多样性。  \n\n\n（2）动量蒸馏（Momentum Distillation, MoD）：应对噪声数据过拟合问题。  \n通过动量模型（参数指数滑动平均）生成软伪标签，作为额外监督信号，允许模型学习与噪声标注不同但合理的输出，增强鲁棒性。  \n损失函数结合原始监督信号与伪标签的KL散度，公式为： , 其中，为蒸馏权重。  \n\n\n\n3. 理论分析：互信息最大化视角从互信息（MI）最大化角度解释ALBEF的有效性：  \n\nITC和MLM的视图生成：ITC将图像和文本视为同一数据对的两种“视图”，通过模态分离最大化视图间互信息；MLM通过掩码文本生成视图，利用图像和上下文预测原词，增强跨模态依赖。  \n动量蒸馏的视图增强：动量模型生成语义相似的新视图，迫使基础模型学习对语义保持不变性的表示，进一步提升互信息下界。  \n\n4. 实验效果与创新点\n性能突破：ALBEF在多个下游任务中超越现有SOTA方法：  \n图像-文本检索：在Flickr30K和COCO上超越CLIP、ALIGN，零样本迁移能力显著。  \nVQA和NLVR2：相比SOTA方法VILLA，绝对性能提升分别达2.37%和3.84%，且推理速度快10倍以上。  \n弱监督视觉定位：在RefCOCO+上通过Grad-CAM可视化验证，模型可准确定位物体、属性和关系。  \n\n\n创新优势：  \n无检测器设计：摆脱对物体检测器的依赖，支持低分辨率输入（预训练256×256，微调384×384），降低计算成本。  \n噪声鲁棒性：动量蒸馏有效利用大规模噪声Web数据，提升模型泛化能力。  \n统一单模态与多模态能力：结合对比学习（CLIP类方法）和跨模态推理（Transformer类方法），兼顾检索与复杂推理任务。  \n\n\n\n5. 结论与资源ALBEF通过“对齐-融合”框架、动量蒸馏和互信息理论，实现了视觉-语言表示学习的高效性与鲁棒性，为大规模噪声数据的利用提供了新范式。\nrelated work2.1 视觉-语言表示学习（Vision-Language Representation Learning）现有视觉语言表示学习方法主要分为两类：  \n\n基于多模态编码器的方法  \n代表方法：LXMERT、UNITER、OSCAR等，采用Transformer架构联合建模图像区域特征与文本标记，通过掩码语言建模（MLM）、图像-文本匹配（ITM）等任务学习跨模态交互。  \n优势：在需要复杂推理的任务（如VQA、NLVR²）中表现优异。  \n局限性：依赖预训练物体检测器提取图像区域特征，需高分辨率图像（如600×1000），计算成本高；部分方法（如ViLT）虽移除检测器但性能下降。  \n\n\n基于单模态编码器的对比学习方法  \n代表方法：CLIP、ALIGN，通过对比损失在大规模噪声数据中学习图像和文本的独立嵌入空间，擅长图像-文本检索任务。  \n优势：无需物体检测器，可处理大规模Web数据，零样本迁移能力强。  \n局限性：缺乏对图像-文本复杂交互的建模能力，难以应对需要细粒度推理的任务。  \n\n\n\nALBEF的定位：  \n\n融合两类方法的优势，通过单模态编码器对比对齐（ITC损失）和多模态编码器交互建模（MLM、ITM损失），实现检索与推理任务的平衡。  \n无需物体检测器，采用ViT作为图像编码器，降低计算成本，同时支持低分辨率输入（预训练256×256，微调384×384）。  \n\n2.2 知识蒸馏（Knowledge Distillation）\n传统知识蒸馏：从预训练的“教师模型”向“学生模型”迁移知识，通常通过匹配输出概率（如KL散度）实现，适用于模型压缩或跨任务迁移。  \n在线蒸馏：同时训练多个模型，利用模型集合作为教师，如深度互学习。  \n动量蒸馏（MoD）的创新：  \n属于在线自蒸馏，通过维护模型参数的指数滑动平均（动量模型）生成伪标签，作为额外监督信号。  \n与半监督学习（如Mean Teacher）和对比学习（如MoCo）相关，但首次将动量蒸馏应用于视觉-语言预训练，缓解噪声数据过拟合问题。  \n理论与实验表明，动量蒸馏可提升模型在噪声数据中的鲁棒性，且适用于多种下游任务（包括干净标注数据）。 \n\n\n\nALBEF Pre-training3.1 模型架构ALBEF的架构由三个核心模块组成，如图所示：\n\n图像编码器：\n结构：采用12层视觉Transformer（ViT-B/16），输入图像尺寸为256×256（预训练）或384×384（微调），通过16×16的Patch划分，最终输出序列嵌入，其中是用于全局表示的[CLS]标记，是Patch嵌入。  \n初始化：权重来自ImageNet-1k预训练的ViT模型，提升图像语义理解能力\n\n\n文本编码器：\n结构：6层Transformer，基于BERT-base的前6层初始化，输入文本通过Tokenization生成标记序列，输出嵌入，其中  为文本全局表示。\n\n\n多模态编码器：\n结构：6层Transformer，基于BERT-base的后6层初始化，接收图像编码器的输出和文本编码器的输出，通过 跨注意力机制（Cross-Attention）逐层融合图像与文本特征，实现跨模态交互。\n核心操作：在每一层，图像特征作为键（Key）和值（Value），文本特征作为查询（Query），通过跨注意力学习文本对图像的细粒度对齐（如定位图像中的物体对应文本描述）。\n\n\n\n\n\n\n\n\n\n\n为什么文本编码器使用 BERT-base 的前六层初始化，而多模态编码器使用 BERT-base 后六层来初始化？\nALBEF 的文本编码器和多模态编码器的分层初始化策略，旨在分离单模态理解与跨模态交互的能力，具体原因如下：\n\nBERT 的层功能分工：\nBERT 的前几层更擅长捕捉单模态文本的局部语义和语法结构（如词级关联、短语结构），适合作为文本编码器的初始化，专注于文本单模态表示的学习。\nBERT 的后几层更擅长建模全局语义依赖和跨标记交互（如句子级语义整合），与多模态编码器需要处理图像 - 文本跨模态交互的需求更匹配。\n\n\n单模态与多模态的功能解耦：\n文本编码器：仅处理文本输入，需强化单模态文本理解能力，因此复用 BERT 前六层的文本建模能力。\n多模态编码器：需融合图像与文本特征，其核心是跨注意力机制（Cross-Attention），而 BERT 后六层的自注意力机制（Self-Attention）已具备建模复杂交互的能力，微调后可适配跨模态场景。\n\n\n参数效率与迁移学习：\n利用 BERT 预训练的权重初始化，避免从头训练带来的不稳定，同时通过分模块初始化（前六层 vs. 后六层）实现单模态到多模态的渐进式能力扩展。\n实验表明，这种分层初始化策略在 VQA、NLVR² 等需要跨模态推理的任务中，比随机初始化或全层复用性能更优。\n\n\n\n\n\n3.2 预训练目标（Pre-training Objectives）ALBEF采用三个预训练目标，联合优化单模态对齐与多模态交互：  \n3.2.1 图像-文本对比学习（Image-Text Contrastive Learning, ITC）\n目标：在融合前对齐图像和文本的单模态表示，学习跨模态相似性函数，其中  是将全局嵌入映射到256维空间的线性层。  \n动量编码器与队列机制：  \n维护两个队列存储动量编码器的历史特征（受MoCo启发），动量编码器参数通过指数滑动平均（EMA）更新：, 其中  为动量系数，为当前模型参数，为动量模型参数。  \n队列中存储最近  个图像-文本对的动量特征  和 。  \n\n\n相似度计算：  \n图像到文本相似度：\n文本到图像相似度：\n\n\n对比损失（InfoNCE）：\n\n\n\n其中为one-hot真实标签，为可学习的温度参数，为交叉熵损失。 \n3.2.2 掩码语言建模（Masked Language Modeling, MLM）\n目标：利用图像信息预测文本中被掩码的标记，增强跨模态语义关联。  \n操作：随机掩码文本标记（15%概率），其中80%用[MASK]替换，10%用随机词替换，10%保持不变。  \n损失函数：其中为掩码文本，为模型预测的标记概率，为真实标记的one-hot向量。  \n\n3.2.3 图像-文本匹配（Image-Text Matching, ITM）\n目标：判断图像-文本对是否匹配，引入对比硬负样本挖掘提升训练难度。  \n硬负样本挖掘：在批次内根据对比相似度分布采样硬负样本：对每个图像，选择与该图像相似度最高的非匹配文本作为硬负样本；对每个文本，选择相似度最高的非匹配图像作为硬负样本。  \n损失函数： , 其中为匹配概率，为真实标签。  \n\n总损失函数：\n3.3 动量蒸馏（Momentum Distillation, MoD）\n动机：缓解Web数据噪声导致的过拟合，利用动量模型生成软伪标签作为额外监督。  \n核心思想：当前模型输出与动量模型的伪标签对齐，通过KL散度最小化实现知识蒸馏。  \n公式推导：  \nITC的动量蒸馏损失： , 其中为动量模型计算的软伪概率，为蒸馏权重。  \nMLM的动量蒸馏损失： , 其中 为动量模型预测的掩码标记概率。  \n\n\n效果:  伪标签允许模型学习与噪声标注不同但语义合理的输出，增强鲁棒性（如图所示，伪标签覆盖了真实文本未描述的视觉概念）。\n\n\n3.4 预训练数据集（Pre-training Datasets）\n数据集组成：  \nWeb数据：Conceptual Captions (295万图像)、SBU Captions (86万图像)、Conceptual12M (1006万图像，噪声较大)。  \n领域内数据：COCO (11.3万图像)、Visual Genome (10万图像)。  \n\n\n数据规模：  \n基础数据集：400万图像，510万图像-文本对。  \n扩展数据集：1410万图像（含Conceptual12M），用于验证模型对大规模噪声数据的适应性。  3.5 实现细节（Implementation Details）\n\n\n训练配置：  \n8块NVIDIA A100 GPU，批次大小512，训练30 epoch。  \n优化器：AdamW，权重衰减0.02，学习率余弦衰减（初始，热身1000步）。  \n\n\n图像预处理：随机裁剪256×256，应用RandAugment（不含颜色变换，避免与文本颜色信息冲突）。  \n微调细节：图像分辨率提升至384×384，通过插值适配ViT的位置编码。  \n\nA mutual Information Maximization Perspective4.1 互信息（MI）与视觉-语言表示学习核心思想：互信息衡量两个随机变量的依赖程度，最大化互信息可使模型学习到更具语义关联的跨模态表示。在视觉-语言任务中，图像-文本对的不同“视图”（View）可视为随机变量，通过最大化视图间的互信息，模型能捕捉更鲁棒的语义不变性。数学定义：设随机变量  和  为图像-文本对的两种视图，互信息  表示为：  最大化  可通过最小化InfoNCE损失实现，该损失是互信息的一个下界：  其中  为视图  和  的相似度得分， 包含正样本  和负样本集合。\n4.2 ITC损失的互信息解释ITC的视图定义：将图像  和文本  视为同一对的两个独立视图，通过对比学习最大化它们的互信息。公式推导：ITC损失可重写为对称的InfoNCE损失：  \n\n正样本：真实匹配的图像-文本对 。  \n负样本：队列中存储的  个不匹配样本  和 。  \n物理意义：通过对比学习，迫使匹配对的视图在特征空间中接近，非匹配对远离，从而最大化图像与文本视图的互信息。4.3 MLM损失的互信息解释MLM的视图定义：将“掩码词”与“图像+掩码文本”视为两种视图，通过预测掩码词最大化其依赖关系。公式推导：MLM损失可重构为：  \n查询向量：掩码词的真实标记  通过嵌入层映射为 。  \n键向量：多模态编码器输出的掩码上下文表示 ，包含图像和未掩码文本的信息。  \n物理意义：将掩码词预测视为从上下文视图中检索正确标记，通过最大化掩码词与上下文的互信息，增强跨模态语义关联。4.4 动量蒸馏（MoD）的互信息增强核心机制：动量模型生成的伪目标相当于引入新的“语义相似视图”，迫使基础模型学习对视图变化不变的表示。公式分析：以ITC的动量蒸馏损失为例：   , 其中伪目标  由动量模型的相似度  计算得到：  \n新视图生成：动量模型通过历史参数平均生成更平滑的特征表示， 对应不同于当前模型的视图相似度，引入语义等价但细节不同的负样本。\n互信息提升：最小化  迫使当前模型匹配动量模型的视图分布，相当于最大化当前模型与动量模型生成的新视图之间的互信息，增强表示的不变性。4.5 统一理论视角：视图生成与不变性学习\n\n\n\n\n\n任务\n视图生成方式\n互信息优化目标\n\n\n\n\nITC\n模态分离（图像vs.文本）\n最大化单模态视图间的互信息\n\n\nMLM\n词掩码（完整文本vs.掩码文本+图像）\n最大化掩码词与上下文视图的互信息\n\n\nMoD\n动量模型生成语义相似视图\n最大化当前模型与动量视图的互信息\n\n\n\n\n下游视觉-语言任务与实验结果ALBEF预训练模型被适配到五类下游任务，展示了其在检索、推理、生成和定位任务中的泛化能力。以下是各任务的详细分析：\n5.1 图像-文本检索（Image-Text Retrieval）\n任务目标：实现图像与文本的相互检索，包括图像到文本检索（TR）和文本到图像检索（IR）。  \n数据集：  \nFlickr30K：29k训练图像，1k验证/测试图像，每图像对应5句文本。  \nCOCO：113k训练图像，5k验证/测试图像，每图像对应5句文本。  \n\n\n模型调整：  \n微调时联合优化 ITC损失（单模态对齐）和 ITM损失（多模态匹配）。  \n推理时先用ITC计算特征相似度筛选Top-k候选，再用ITM进行细粒度排序，大幅提升推理速度（仅计算少量候选的ITM分数）。  \n\n\n关键结果：  \n零样本迁移：在Flickr30K上，仅用COCO训练的模型实现TR/IR平均召回率94.1%/82.8%，超越CLIP和ALIGN。  \n微调性能：14M预训练图像下，Flickr30K的TR/IR平均召回率达98.70%/94.07%，COCO上达97.2%/90.5%，显著优于UNITER、OSCAR等方法。  \n\n\n\n\n5.2 视觉蕴含（Visual Entailment, SNLI-VE）任务\n任务目标：判断图像与文本的关系（蕴含、中立、矛盾），属于三分类问题。\n数据集：SNLI-VE，基于SNLI文本和Flickr30K图像构建，含29.8k训练样本。\n模型调整：使用多模态编码器的[CLS]标记输出，通过多层感知机（MLP）分类。\n关键结果：\n14M预训练图像下，测试集准确率达80.91%，相比VILLA提升1.88%，验证细粒度视觉推理能力。\n\n\n\n\n5.3 视觉问答（Visual Question Answering, VQA）\n任务目标：给定图像和问题，生成自然语言答案。\n数据集：VQA2.0，含83k训练图像，41k验证图像，81k测试图像，问题涉及物体、属性、关系等。\n模型调整：\n附加6层Transformer解码器，通过自回归生成答案，解码器权重由多模态编码器初始化。\n限制答案生成范围为3,192个候选词，确保与现有方法可比。\n\n\n关键结果：\n14M预训练图像下，test-std准确率76.04%，相比VILLA提升2.37%，推理速度快10倍以上。\nGrad-CAM可视化显示，模型能准确聚焦问题相关区域。\n\n\n\n\n5.4 自然语言视觉推理（NLVR²）\n任务目标：判断文本是否同时描述两张图像，属于二分类问题。  \n数据集：NLVR²，含50k训练图像对，8.5k验证/测试图像对。  \n模型调整：  \n多模态编码器每层复制两个Transformer块，分别处理两张图像的嵌入，共享跨注意力参数。  \n预训练阶段新增文本分配（TA）任务：将文本匹配到两张图像之一或都不匹配，提升图像对推理能力。  \n\n\n关键结果：  \n14M预训练图像下，test-P准确率83.14%，相比VILLA提升3.84%，验证多图像推理的有效性。  5.5 弱监督视觉定位（Weakly-Supervised Visual Grounding）\n\n\n任务目标：在无边界框标注的情况下，定位图像中与文本描述对应的区域。  \n数据集：RefCOCO+，含19,992张COCO图像，141,564条指代表达（如“穿绿衬衫的女孩”）。  \n模型调整：\n仅使用图像-文本对监督，通过Grad-CAM生成注意力热图，定位文本提及的区域。  \nITC vs. ITM对比：  \nITC：基于单模态相似度，定位物体整体（如“大象”）。  \nITM：基于多模态交互，定位细粒度属性和关系（如“卷鼻子的大象”）。  \n\n\n\n\n\n\n\n关键结果：\nALBEF-ITM在RefCOCO+的TestB子集准确率46.25%，远超ARN、CCL等基线方法（32.13%/33.56%）。\n可视化显示，模型能区分“大行李箱”与“小背包”等细微差异，证明跨模态交互的重要性。\n\n\n\n\n","slug":"ALBEF论文精读","date":"2025-07-03T03:20:45.000Z","categories_index":"论文精读","tags_index":"多模态,VLP","author_index":"犬夜叉"},{"id":"8c60d49e88f3e150291142683c6b267b","title":"ViLT 论文精读","content":"摘要\n\n\n\n\n\n\n\n\n视觉语言预训练已提升了各类视觉 - 语言联合下游任务的性能。当前 VLP 方法高度依赖图像特征提取流程，其中大部分涉及区域监督（如目标检测）和卷积架构（如 ResNet）。尽管现有文献未予重视，但我们发现这一模式存在两方面问题：（1）效率 / 速度层面，仅输入特征提取所需的计算量就远超多模态交互步骤；（2）表达能力层面，其上限受限于视觉嵌入器的表达能力及其预定义的视觉词汇表。本文提出一种极简 VLP 模型 —— 视觉语言 Transformer（ViLT），其核心在于将视觉输入的处理大幅简化为与文本输入相同的无卷积模式。实验表明，ViLT 的速度可达以往 VLP 模型的数十倍，同时在下游任务中具备相当或更优的性能。我们的代码和预训练权重可从https://github.com/dandelin/vilt获取。\n论文介绍1. 视觉语言预训练（VLP）的现状与挑战\n主流方法的依赖与问题： 现有 VLP 模型高度依赖基于卷积神经网络（CNN）的图像特征提取（如 ResNet）和区域监督（如目标检测），导致两大核心问题：\n效率瓶颈：特征提取的计算量远超多模态交互步骤（如 UNITER 模型中视觉处理耗时占比超 90%）。\n表达能力受限：依赖预定义的视觉词汇表（如 Visual Genome 的 1600 个物体类别），难以泛化未知物体或场景。\n\n\n学术研究与实际应用的脱节： 学术实验中常通过预缓存区域特征减轻训练时的计算负担，但在实际应用中，实时输入仍需经历耗时的特征提取流程，限制了模型的部署效率。\n\n2. ViLT 的核心创新：极简架构与统一处理\n无卷积的视觉特征嵌入： 受 ViT（Vision Transformer）启发，ViLT 将图像分割为固定大小的补丁（如 32×32 像素），通过线性投影层直接生成视觉嵌入，完全摒弃 CNN 和目标检测模块。这一设计使视觉处理耗时仅为～0.4 ms，参数仅 2.4M，远低于 ResNet 等传统 backbone。\n统一的 Transformer 架构： 视觉和文本输入均通过 Transformer 进行多模态交互，首次实现 模态特定组件计算量（VE+TE）&lt;多模态交互计算量（MI） 的架构（如图 1 中 ViLT 的 MI 耗时～15 ms，远超 VE 的 0.4 ms）。这种 轻嵌入、重交互 的设计显著提升计算效率，同时保持下游任务性能。\n\n3. 性能与效率对比\n速度优势： ViLT 的总推理时间～15 ms，比基于区域特征的模型（如 UNITER，~900 ms）快 60 倍以上，比基于网格特征的 Pixel-BERT（~60 ms）快 4 倍。\n任务表现： 在 NLVR2（视觉推理）、F30K 检索等任务中，ViLT 的准确率与传统模型相当或更优（如 NLVR2 测试集准确率 74.57%，接近 UNITER 的 75.8%），证明无需复杂视觉 backbone 仍可实现有效跨模态建模。\n\n4. 关键贡献与意义\n架构革新： 首次证明 VLP 模型可完全脱离卷积和区域监督，为轻量化多模态模型设计提供新范式。\n方法创新： 引入全词掩码（Whole Word Masking）和图像增强（RandAugment），提升预训练效率和下游任务泛化能力。\n研究启示： 呼吁 VLP 领域从 “单模态特征增强” 转向 “多模态交互优化”，为后续模型（如更大规模的 ViLT-L/H）奠定基础。\n\n\n从论文中的图片可以看出，对于传统模型，一般的处理流程包括：\n\n图像输入后，先通过卷积神经网络提取网格特征，再经目标检测模块生成区域建议（RoI），并通过非极大值抑制（NMS）和 RoI 头处理，最终得到区域特征。\n直接使用 CNN backbone（如 ResNet）输出的网格特征，跳过目标检测步骤，通过线性嵌入输入 Transformer。在这片论文中，图像直接分割为固定大小的补丁（如 32×32 像素），通过线性投影层（Linear Embedding）将每个补丁转换为特征向量，无需 CNN 或目标检测模块。\n\n\n效率优势：ViLT 的总运行时间（~15 ms）仅为 UNITER 的 1.7%，Pixel-BERT 的 25%。\n性能保持：在 NLVR2、F30K 检索等任务中，ViLT 性能接近或超过传统模型，证明去除 CNN 和区域监督不会显著损失表达能力。\n\n研究背景1. 视觉-语言模型的分类体系\n2. 模态交互方式\n单流 vs. 双流：\n单流模型（如 VisualBERT）将图像和文本拼接后输入 Transformer，参数效率更高；\n双流模型（如 ViLBERT）分模态处理，引入额外参数，ViLT 采用单流架构。\n\n\n\n3. 视觉嵌入的瓶颈\n区域特征： 通过 Faster R-CNN 等目标检测器生成，需经历 RPN、NMS 等复杂流程，计算量大（如 UNITER 视觉处理耗时 810 ms），且依赖预定义类别（如 Visual Genome 的 1600 个物体类），泛化能力受限。\n网格特征： 直接使用 CNN 输出（如 Pixel-BERT），虽省去目标检测，但 CNN backbone（如 ResNet）仍为性能瓶颈（Pixel-BERT-R50 视觉处理耗时 45 ms）。\nViLT 的创新： 采用Patch投影，将图像分割为 32×32 像素块，通过线性投影生成嵌入，仅需 0.4 ms 和 2.4M 参数，彻底摒弃 CNN 和目标检测。\n\n4. 关键问题与突破\n传统模型的效率与表达局限： 视觉嵌入的高计算量（如区域特征提取）和预定义视觉词汇限制了模型速度和泛化能力，学术实验中预缓存特征的做法无法解决实际应用中的实时性需求。\nViLT 的设计思路： 受 ViT 启发，利用 Transformer 的自注意力机制直接处理图像补丁和文本 tokens，将计算重点从 “单模态特征提取” 转向 “跨模态深度交互”，实现轻量化与高性能的平衡。\n\nVision-and-Language Transformer一、模型整体架构与核心设计1. 统一的单流Transformer架构ViLT采用单流 Transformer 处理视觉和文本输入，即图像和文本嵌入被拼接为单一序列，通过多层Transformer层进行联合建模。这一设计避免了双流架构的额外参数开销，同时确保深度跨模态交互。\n2. 视觉嵌入：无卷积的Patch投影\n输入处理：图像被分割为固定大小的补丁（如32×32像素），每个补丁被展平为一维向量  为补丁尺寸， 为图像通道数， 为补丁数量）。\n线性投影：通过可学习的线性层  将补丁向量映射到隐藏空间 （H=768 为隐藏层维度），并添加位置嵌入 。\n关键优势：\n仅需 2.4M参数，远小于ResNet-50（25M）等卷积 backbone。\n计算耗时仅 0.4 ms，彻底消除CNN和目标检测的耗时瓶颈。\n\n\n\n3. 文本嵌入：基于BERT的轻量设计\n输入处理：文本通过BERT分词器生成 tokens，包含特殊标记（如[CLS]）和位置嵌入 。\n模态类型嵌入：为区分视觉和文本模态，分别添加模态类型向量  和，避免模态混淆。\n\n4. 多模态交互：Transformer编码器\n输入拼接：视觉嵌入  和文本嵌入  拼接为 。\nTransformer层：通过12层Transformer编码器（每层包含多头自注意力MSA和MLP）进行特征交互，公式如下：， 其中，层归一化采用ViT的“前归一化”策略（先归一化再计算注意力/MLP），与BERT的“后归一化”不同。\n池化输出：最终序列的首个标记  通过线性投影和激活函数生成池化表示 ，用于下游任务分类或检索。\n\n二、与训练目标与技术创新\n\n\n\n\n\n\n实验数据集介绍\n1、 MSCOCO（Microsoft Common Objects in Context）\n\n规模：113K 张图像，567K 条字幕（平均每张图 5 条字幕），字幕长度为 11.81±2.81。\n定位：微软开发的 多任务基准数据集，支持目标检测、场景分割、图像字幕、视觉问答（VQA）等任务。\n\n\n核心特点：\n字幕标注 聚焦场景核心内容（如 “一只狗在公园奔跑”），避免冗余细节，适合训练模型抓重点；\n图像覆盖真实生活场景（如家庭、户外），物体类别丰富（80 类常见物体），是视觉语言任务的 “试金石”。\n\n\n\n2、VG（Visual Genome）\n\n规模：108K 张图像，5.41M 条字幕（平均每张图 50 条 + 字幕），字幕长度为 5.53±1.76。\n定位：斯坦福李飞飞团队推出的 细粒度语义理解数据集，主打 场景图（Scene Graph） 标注（结构化描述物体、属性、关系，如 “人→骑→自行车”）。\n核心特点：\n\n标注极丰富：除字幕，还有 “区域描述”（局部区域的文本解释）、“视觉问答”（针对图像的问答对）；\n分布呈长尾性：常见关系（如 “on” “in”）占比极高，稀有关系极少，后续衍生出 “VG150” 等清洗版本（筛选高频类别）。\n\n3、 GCC（Google Conceptual Captions）\n\n规模：3.01M 张图像，3.01M 条字幕（1:1 配对），字幕长度为 10.66±4.93。\n定位：大规模弱监督数据集，图像和字幕从网络爬取，属于 “弱关联” 标注（无需严格语义对应）。\n核心特点：\n数据极多样：图像风格覆盖更广（如艺术画、表情包），突破 MSCOCO 的场景限制，提升模型泛化能力；\n预训练核心：ViLT 等模型用其预训练，通过海量数据学习 “松散跨模态关联”，弥补强监督数据集的规模不足。\n\n\n\n4、 SBU（SBU Captions）\n\n规模：867K 张图像，867K 条字幕（1:1 配对），字幕长度为 15.0±7.74。\n定位：早期 图像 - 字幕检索数据集，最初用于 “给定图像找匹配字幕” 或反之的检索任务。\n核心特点：\n字幕更长且自由（均值 15 词，长于其他数据集），适合训练模型理解复杂文本描述；\n标注简洁：1 张图对应 1 条字幕，训练成本低，常与其他数据集联合预训练（如 ViLT）。\n\n\n\n\n\n\n\n1. 图像文本匹配\n任务定义: 判断图像-文本对是否匹配，正样本为真实配对，负样本为随机替换图像的配对。\n损失函数: 二元交叉熵损失, 通过线性层将池化表示  映射为匹配概率\n创新: 词-补丁对齐 (Word-Patch Alignment, WPA) 受最有传输理论启发, 引入WPA 目标增强跨模态对齐:\n计算文本子集和视觉子集之间的Wasserstein距离, 使用近似最近点方法优化\n将距离损失按系数0.1加权后加入ITM损失, 提升细粒度语义对齐(如下图的可视化热力图)\n\n\n\n\n\n\n\n\n\n\n词-补丁对齐详解: 基于最优传输的跨模态语义对齐\n\n核心问题：跨模态细粒度对齐的挑战在视觉语言任务中，文本中的单词（如“花朵”）需与图像中的对应区域（如花朵所在的补丁）建立精准关联。传统方法（如点积、注意力）通过特征相似度建模交互，但缺乏对语义结构的显式对齐。例如，文本中的“giraffe”需对应图像中长颈鹿的整体区域，而非零散像素。WPA的目标：通过最优传输理论，将文本单词与图像补丁视为两个分布，计算它们之间的结构化对齐成本，强制模型学习语义级别的跨模态对应关系。\n理论基础：最优传输与Wasserstein距离最优传输（Optimal Transport, OT）：起源于运输问题，旨在找到两个概率分布之间的最优映射路径，使总运输成本最小。例如，将一堆沙子（源分布）移动到另一堆沙子（目标分布）的最小工作量。Wasserstein距离（推土机距离）：衡量两个分布之间的最小运输成本，公式为： ， 其中， 和  分别为文本和视觉特征分布，为两者的联合分布， 为单个样本的运输成本（如特征向量距离）。关键价值：Wasserstein距离不仅衡量整体分布差异，还能捕捉局部结构对应（如单词与补丁的一一映射），适合细粒度语义对齐。\nWPA的技术实现：基于IPOT的近似优化论文采用 近似最近点方法（IPOT） 高效计算Wasserstein距离，避免传统OT的高复杂度。具体步骤如下：步骤1：特征子集提取文本子集 ：从Transformer最后一层输出中提取文本 tokens 的特征（不包含特殊标记如[CLS]）。视觉子集 ：提取图像补丁的特征（不包含补丁分类标记[VCLS]）。形状：假设文本有  个单词，视觉有  个补丁，则特征矩阵分别为 和 为隐藏层维度）。步骤2：运输成本矩阵构建计算每个单词与补丁之间的成对距离，形成成本矩阵 ：  （归一化欧氏距离），分母  为缩放因子，确保数值稳定性。步骤3：IPOT迭代优化通过IPOT算法求解最优传输计划 ，其中  表示单词  与补丁  的对齐强度（概率值）。初始化：均匀分布 。迭代更新：, 其中， 为温度参数，控制对齐的平滑度；迭代次数 （与训练阶段一致）。约束：确保每行（单词）和每列（补丁）的概率和为1，即 ， 。步骤4：Wasserstein距离计算近似Wasserstein距离为：  , 即运输计划与成本矩阵的内积，反映整体对齐成本。步骤5：损失加权与反向传播将Wasserstein距离按系数  加权后加入图像文本匹配（ITM）损失：  ， 该损失引导模型调整Transformer参数，使语义相关的单词与补丁之间的运输成本降低（即对齐强度增加）。\n可视化与效果：语义对齐的直观呈现论文通过上面的热图可视化WPA的对齐结果。例如，文本“flowers”对应图像中花朵区域的补丁，“wall”对应墙壁区域的补丁，透明度越高表示对齐强度）越大。关键发现：  \n\n\n传统模型（如UNITER）依赖目标检测框标注，只能对齐预定义类别；  \nViLT通过WPA实现无显式标注的语义对齐，可泛化至任意物体。消融实验：移除WPA后，NLVR2准确率下降约0.8%，证明其对跨模态推理的重要性。\n\n\n为什么WPA有效？结构化对齐：区别于简单特征融合（如CLIP的点积），WPA显式建模单词与补丁的一对多关系，捕捉复杂语义关联（如“giraffe”对应多个身体部位补丁）。抗局部最优：IPOT的迭代优化避免陷入浅层交互的局部最优解，迫使模型学习全局一致的跨模态映射。轻量化开销：WPA仅在预训练阶段引入约0.4 ms计算量，远低于目标检测的数百毫秒耗时，适合端到端优化。\n\n\n\n2. 掩码语言建模（Masked Language Modeling, MLM）\n任务定义：随机掩码文本 tokens（15%概率），预测原始标记。\n论文创新：全词掩码（whole world masking）\n掩码整个单词的所有子词单元（如“giraffe”掩码为[gi, [MASK], ##fe]），迫使模型依赖图像信息而非局部文本上下文。\n实验表明，全词掩码显著提升NLVR2等推理任务性能（+0.96% test-P准确率）。\n\n\n\n\n3. 图像增强（RandAugment）\n应用场景：仅在微调阶段使用，避免预训练时缓存特征的限制。\n策略选择：采用RandAugment中的非颜色反转和非裁剪策略（保留颜色信息和小物体），超参数设为 。\n效果：结合全词掩码和更长训练步数（200K步），ViLT在VQAv2测试集准确率提升至71.26%。\n\n\n\n\n\n\n\n\n表格解释\n\n视觉嵌入方式：\nRegion：基于目标检测的区域特征。\nGrid：基于 CNN 网格特征。\nLinear：ViLT 的线性补丁投影。\n\n\n任务：\nVQAv2：视觉问答，准确率越高越好。\nNLVR2：视觉推理（二元分类），dev/test-P 准确率越高越好。\n\n\n关键标注：\n†：预训练额外使用 GQA、VQAv2、VG-QA 数据集。\n‡：额外使用 Open Images 数据集。\na：微调时应用 RandAugment 数据增强。\n+：预训练步数延长至 200K 步（默认 100K 步）。\n\n\n\n\n\n三、关键技术对比与性能优势1. 与传统VLP模型的对比\n\n\n\n维度\n传统模型（如UNITER）\nViLT\n\n\n\n\n视觉嵌入\nCNN+目标检测（如ResNet+Faster R-CNN）\n线性补丁投影（无卷积/检测）\n\n\n计算量\n视觉处理占90%以上耗时（~810 ms）\n视觉处理仅0.4 ms，总耗时~15 ms\n\n\n参数规模\n150M+（含CNN权重）\n87.4M（仅Transformer+线性层）\n\n\n下游任务性能\nVQAv2 test-dev 72.7%\nVQAv2 test-dev 71.26%（+RandAugment）\n\n\n推理效率\n需预缓存特征（训练时高效，推理时慢）\n端到端高效（无需预计算）\n\n\n\n\n2. 效率与性能的平衡\n速度优势：ViLT比UNITER快60倍以上，比Pixel-BERT（网格特征）快4倍。\n泛化能力：无需预定义视觉词汇（如Visual Genome的1600类），通过自注意力学习任意图像补丁与文本的关联，适用于未知物体场景。\n\n","slug":"ViLT论文精读","date":"2025-07-02T09:37:29.000Z","categories_index":"论文精读","tags_index":"多模态,VLP","author_index":"犬夜叉"},{"id":"64c2453c0ac7d584fda9903cba47649e","title":"CLIP 论文精读","content":"CLIP论文精读CLIP通过对比学习从大量的图像-文本中学习视觉概念，实现了强大的零样本图像分类能力\n\n\n\n\n\n\n\n\n\n论文地址：Learning Transferable Visual Models From Natural Language Supervision\n\n\n\n\n\n\n论文创新点\n\n它不使用传统计算机视觉任务中常见的、带有固定类别（如“猫”、“狗”）的标签，而是直接从互联网上收集的（图像，文本）对中学习。这种文本描述提供了比单一标签丰富得多的监督信号，涵盖了几乎无限的视觉概念。\n\n\n为了让模型理解图像与文本的关联，CLIP采用了一种名为“对比学习”的自监督方法。其核心思想是：\n\n构建双编码器架构：CLIP包含一个图像编码器（Image Encoder，如ResNet或Vision Transformer）和一个文本编码器（Text Encoder，如Transformer）。\n学习多模态嵌入空间：在训练过程中，模型会接收一批图像和文本。图像编码器将图像转换为特征向量，文本编码器将文本转换为特征向量。CLIP的目标是在这个共享的多模态嵌入空间中，拉近真实的“图像-文本”对的特征向量（正样本），同时推远不匹配的“图像-文本”对的特征向量（负样本）。\n高效的代理任务：通过判断哪个文本与哪个图像配对，这个看似简单的“代理任务”却极其高效地迫使模型学习图像内容和文本语义之间的深刻联系。\n\n\nCLIP最令人瞩目的成果是其强大的零样本学习能力。传统的模型在面对一个新的分类任务时，通常需要进行微调，即在新任务的标注数据上进行再训练，而经过预训练的CLIP无需任何微调即可直接应用于新的视觉分类任务。其实现方式为：\n\n动态构建分类器：对于一个给定的分类任务（例如，区分“猫”和“狗”的图片），CLIP会将类别名称（”cat”, “dog”）转换成标准的提示语，如 “a photo of a cat” 和 “a photo of a dog”。\n相似度匹配预测：将这些提示语通过文本编码器生成文本特征向量。当输入一张待分类的图像时，图像编码器会生成其图像特征向量。最后，模型会计算该图像特征向量与所有类别提示语的文本特征向量之间的余弦相似度，相似度最高者即为预测的类别。\n\n\n\n\n\n1. 核心方法：对比学习CLIP的目标不是像传统模型那样预测一个固定的类别，而是学习一个多模态的嵌入空间，在这个空间里，匹配的图像和文本对的特征向量距离很近，而不匹配的则很远。具体实现如下：\n\n构建批次：在一个训练批次中，包含 N 个匹配的对。\n双塔编码：\n图像编码器 (Image Encoder)：将 N 个图像编码成 N 个图像特征向量 \n文本编码器 (Text Encoder)：将 N 个文本编码成 N 个文本特征向量 \n\n\n计算相似度：计算所有可能的 对的余弦相似度，形成一个 N×N 的相似度矩阵\n定义损失：在这个矩阵中，对角线上的 N 个元素是正样本（匹配的图文对），其余的  个元素都是负样本。CLIP的优化目标是一个对称的交叉熵损失函数，即同时在行和列的方向上进行优化：对于每个图像，模型需要从 N 个文本中找出正确的那个；反之，对于每个文本，也需要从N个图像中找出正确的那个。\n\n\n2. 核心能力：零样本迁移这是CLIP方法论的直接应用，也是其价值的主要体现\n\n动态构建分类器：对于任何一个分类任务（比如对ImageNet的1000类进行分类），CLIP不需要重新训练。而是通过“提示工程”为每个类别创建一个或多个描述性文本。例如，对于类别 “dog”，可以生成文本 “A photo of a dog.”。\n\n\n\n推理与匹配：将待分类的图像输入Image Encoder得到其特征 。然后，将所有类别的提示文本输入Text Encoder得到一组类别特征 ,… 。最后，计算  与每个类别特征  的余弦相似度，相似度最高者即为预测类别。\n提示工程的重要性：精心设计的提示语至关重要。它能解决词义模糊问题（如”boxer”是狗还是拳击手）并提升性能。论文中通过集成80个不同的提示模板，在ImageNet上的准确率提升了3.5%。这一技巧的有效性，使得CLIP的性能得到了显著增强。\n\n\n3. 关键决策：追求最高的训练效率在论文中，作者强调，由于计算资源是有限的，选择一个计算效率最高的预训练方法至关重要。他们对比了三种方法：\n\n方法一：Transformer语言模型 (预测文本)：类似VirTex，用图像作为上下文，生成描述文本。这种方法表现力强，但任务难度大，学习效率最低。\n方法二：词袋模型：不要求生成完整句子，只要求预测文本中的单词。效率比方法一高3倍，但仍不够理想。\n方法三：对比学习：只要求判断图文是否匹配，任务最简单。其训练效率比词袋模型还要高4倍。\n\n\n这个实验结论是CLIP成功的关键之一：在超大规模数据下，一个更简单、更高计算效率的训练目标，能让模型在有限时间内学到更好的表征。\n4. 模型与数据规模\n数据集：OpenAI构建了一个名为WebImageText的私有数据集，包含从互联网上收集的4亿个图文对。\n图像编码器：论文中测试了两种架构：\nResNet-D：对标准ResNet进行了一些修改，如用注意力池化层替换全局平均池化层。共测试了5个不同规模的ResNet。\nVision Transformer (ViT)：共测试了3个不同规模的ViT。实验发现ViT的计算效率比ResNet更高。最终性能最好的模型是 ViT-L/14，并在336x336的分辨率下进行了额外的微调。\n\n\n文本编码器：一个标准的63M参数、12层、512宽、8个注意力头的Transformer模型。\n\n5. 实验结果与分析CLIP的实验部分非常详尽，覆盖了超过30个不同的数据集，主要结论如下：\n\n与全监督模型匹敌：在ImageNet上，Zero-shot CLIP的准确率可以达到76.2%，与一个在ImageNet上经过完整监督训练的ResNet-50效果相当。\n超强的鲁棒性：CLIP最令人印象深刻的是其在自然分布漂移上的表现。在ImageNet-V2, Rendition, Sketch, Adversarial等更具挑战性的数据集上，其性能远超监督模型。最极端的例子是在ImageNet-A上，ResNet101的准确率从76.2%骤降至2.7%，而CLIP仍能达到77.1%，展现了惊人的泛化能力。\n\n\n\n数据重叠检查：为了验证性能不是来自于训练集和测试集的重叠（数据泄露），作者进行了详尽的检查，发现重合率中位数仅为2.2%，且移除这些重叠样本后，模型性能没有显著变化，证明了其强大的泛化能力是真实有效的。\n优秀的特征表示：即使不用于零样本分类，CLIP学习到的特征本身也极为优秀。在标准的线性探查（linear probe）评测中，CLIP的特征在性能和计算效率方面均优于当时的其他自监督方法。\n\n\n上面图片中，左图估算了线性分类器达到零样本 CLIP 性能所需的每类标注样本数，范围从不足 1 到 184，中位数 5.4，均值 20.8。这表明zero - shot transfer 数据效率差异大，部分任务需大量标注，部分几乎无需。图 8 显示zero - shot 与线性探针性能正相关，但zero - shot 普遍低 10%-25%，仅 5 个数据集接近。这说明 CLIP zero - shot 能力与表征质量相关，但仍有提升空间，多数任务距最优有差距。\n\n上图对比了 CLIP 模型与 EfficientNet、MoCo、ViT 等先进计算机视觉模型的线性探针性能。左图为 Kornblith 等研究的 12 个数据集平均分数，右图为 27 个更多样分布数据集的平均分数。结果显示，CLIP 模型，尤其是 ViT 架构的 CLIP-L/14@336px，在两类数据集上均表现出色，其最佳模型平均得分超过现有模型，且 Vision Transformer 比 ResNet 更高效。虚线表示微调或高分辨率评估的模型，体现了 CLIP 在表征学习上的优势与高效性。\n\n","slug":"CLIP论文精读","date":"2025-07-02T08:41:41.000Z","categories_index":"论文精读","tags_index":"多模态,对比学习","author_index":"犬夜叉"},{"id":"290cb712ccbf87b18cb10d019f08a741","title":"ViT 论文精读","content":"\n\n\n\n\n\n摘要\n构建仅使用少量带注释示例即可快速适应新任务的模型，是多模态机器学习研究的一个开放性挑战。我们引入了Flamingo，这是一系列具备此能力的视觉语言模型（VLM）。我们提出了关键的架构创新（i）连接强大的仅视觉和仅语言预训练模型；（ii）处理任意交错的视觉和文本数据序列；（iii）无缝接收图像或视频作为输入。得益于其灵活性，Flamingo模型可在包含任意交错文本和图像的大规模多模态网络语料库上进行训练，这是赋予它们上下文少样本学习能力的关键。我们对模型进行了全面评估，探索并衡量了它们快速适应各种图像和视频任务的能力。这些任务包括开放式任务，如视觉问答（模型被提示一个问题并必须回答）、字幕任务（评估描述场景或事件的能力），以及封闭式任务，如多项选择视觉问答。对于此范围内的任何任务，单个Flamingo模型只需用特定任务的示例提示模型，即可通过少样本学习达到新的最先进水平。在众多基准测试中，Flamingo的表现优于在数千倍更多特定任务数据上微调的模型。\n\n\nVision Transformer的出现标志着 Transformer 架构成功应用于计算机视觉领域，挑战了卷积神经网络在该领域的主导地位。ViT 通过将图像分割成小块 (patches)，并将这些图像块视为序列输入到标准的 Transformer 编码器中，从而实现了对图像的有效处理。这一进展不仅在图像分类等任务上取得了最先进的成果，更重要的是，它为视觉和语言（以及其他模态）提供了一种通用的架构语言——Transformer。这种架构上的统一极大地促进了后续多模态模型（如CLIP、LLaVA等，它们通常采用ViT或其变体作为视觉编码器）的设计和发展，使得不同模态的基于token的表示可以在相似的计算框架内进行交互和融合。 \n\n从数据图中可以看出，在较小的数据集上，Vision Transformer比计算成本相当的ResNets更容易过拟合。例如，ViT-B/32比ResNet50稍快；它在9M子集上的表现要差得多，但在90M+子集上表现更好。ResNet152x2和ViT-L/16也是如此。这个结果强化了这样一种直觉：卷积的归纳偏置对于较小的数据集是有用的，但对于更大的数据集，直接从数据中学习相关模式是足够的，甚至是有益的。\nI. 摘要尽管 Transformer 架构已成为自然语言处理（NLP）任务事实上的标准，但其在计算机视觉领域的应用仍然有限。在视觉领域，注意力机制要么与卷积网络（CNN）结合使用，要么用于替代卷积网络中的某些组件，但整体结构仍然保留。我们证明了这种对 CNN 的依赖并非必要，一个直接应用于图像块序列的纯 Transformer 模型可以在图像分类任务上表现得非常好。 当在大量数据上进行预训练，并迁移到多个中等或小型图像识别基准测试（如 ImageNet, CIFAR-100, VTAB 等）时，Vision Transformer (ViT) 相比于最先进的卷积网络取得了优异的结果，同时训练所需的计算资源也大幅减少。\nII. 创新点范式革新：将图像视为序列处理论文首次证明了一个纯粹的、标准的 Transformer 模型可以直接用于图像分类，而无需依赖卷积神经网络。传统视觉任务长期由 CNN 主导，这篇论文打破了这一惯例。它通过将图像分割成固定大小的图块，并将这些图块的线性嵌入序列作为 Transformer 的输入，成功地将 NLP 领域的成功范式迁移到了视觉领域。\n数据量胜于归纳偏置论文发现，当在超大规模数据集（如 JFT-300M，包含3亿张图片）上进行预训练时，Vision Transformer (ViT) 的性能超越了当前最先进的卷积网络。这揭示了一个重要现象：CNN 中固有的（如局部性、平移不变性）的归纳偏置在数据量较小时非常有效，但当数据量足够大时，模型可以从数据中直接学习到这些空间关系，强大的模型容量和更少的先验限制反而成为优势。\n卓越的计算效率和可扩展性与性能相当的 SOTA 卷积网络相比，ViT 在达到同等甚至更高精度时，所需的预训练计算资源要少得多。例如，ViT-L/16 在 JFT-300M 数据集上预训练后，其性能优于在同一数据集上训练的 BiT-L (一个大型 ResNet 模型)，而训练成本却显著降低。这证明了 Transformer 架构在可扩展性上的巨大潜力。\n简洁而有效的模型设计论文的设计理念是尽可能少地修改原始的 Transformer 架构，使其可以直接利用 NLP 领域成熟的高效实现和可扩展架构。这种简洁性不仅体现在模型结构上，也体现在对图像的处理上，除了初始的图块划分和用于适应不同分辨率的位置编码插值外，几乎没有引入图像特有的归纳偏置。\nIII. 网络原理详解ViT模型概览ViT模型的核心思想是将图像转换为一个序列，然后用标准的Transformer Encoder来处理这个序列。\n图像分块处理 (Image Patching)Transformer接受的输入数据格式是一维的词嵌入序列，为了处理二维图像数据，论文将图像  重塑为一个扁平化的二维图块序列 ，其中：\n\n(H, W) 是原始图像的分辨率\nC 是通道数\n(P, P) 是每个图像图块的分辨率\n 是最终得到的图块数量， 也作为 Transformer 的有效输入序列长度。\n\n\nTransformer 在其所有层中使用恒定的潜在向量大小 D，因此论文将图块扁平化，并通过一个可训练的线性投影映射到 D 维。这个投影的输出称为图块嵌入 (Patch Embeddings)。具体的分块实现可以使用卷积来实现，例如对于 224x224x3 的图像，可使用卷积核大小为 16x16、步长为 16，卷积核数量为 768，将原图像输出为 14x14x768，再将前两个维度展平，得到了最终的 196x768 的张量。\n\n可学习的分类嵌入 (Class Token)原论文在嵌入图块序列的前面添加一个类似于BERT的可学习的嵌入 [CLS] Token，用于对图像进行分类。在进行物体分类任务时，如果不添加Class token，直接把 196 x 768 维的张量输入到编码器（Encode）中，编码器输出的同样是 196 x 768 维的张量，也就是196个 1 x 768 维的向量。但此时面临一个难题：难以确定该选取哪个向量作为最终的输出向量来进行物体分类。为了解决上述问题，在将数据输入编码器之前，添加一个 1 x 768 维的向量，也就是Class token。这个向量会被放置在 196 x 768 维向量的前面。这样一来，编码器输出的向量维度就变成了 197 x 768。之后，只需通过切片操作获取第一个 1 x 768 维的向量，再把它送入分类头进行分类即可。\n融合位置编码 (Positional Encoding)若不添加类似Transformer中的位置编码，那么ViT对于不同顺序的图块会得到相同的结果，这是违反直觉的。Transformer中使用的是正弦位置编码，ViT原始论文中使用的是一维可学习的位置嵌入，因为论文通过实验没有观察到使用二维感知位置嵌入会带来显著的性能提升。在得到经过操作后的 197 x 768 维的张量（由 1 x 768 维的Class token和 196 x 768 维的张量x拼接而成）后，会加上一个维度同样为 197 x 768 的位置编码向量 position Embedding。由于二者维度相同，所以可以直接进行逐元素相加操作，这样就将位置信息融入到了嵌入向量中。最终的输入数据为：\nTransformer EncoderViT的 Encoder 使用的就是 Transformer Encoder 的结构，经过L个encoder结构后，输入维度没有发生变换，仍为 197*768 维。Transformer 的 Encoder 接收输入序列后，先通过词嵌入和位置编码融合语义与位置信息，随后经过多层处理。每层先通过多头自注意力机制计算词与词之间的关联权重，动态聚合上下文信息，再经残差连接和层归一化稳定训练；接着通过前馈神经网络非线性变换特征，同样伴随残差与归一化。最终输出富含上下文信息的序列表示，核心在于自注意力的全局交互和层堆叠的渐进特征抽象。\nMLP Head (分类头)经过encoder结构后，输出的维度为 197*768，此时我们会通过切片的方式提取出Class token的信息，其维度为 1*768。接着会拿这个 1*768 维的Class token经过MLP Head层。ViT中的MLP Head结构非常简洁，它的设计目标是作为一个简单的线性分类器，将从 [CLS] Token中提取到的高度浓缩的图像特征映射到最终的分类结果上。在最常见的实现中，MLP Head仅仅由一个线性层构成。输入维度等于Transformer模型内部的隐藏维度，输出维度等于任务所需的类别总数。在某些论文或实现中（特别是在预训练阶段），这个MLP Head可能会稍微复杂一点，比如包含一个 tanh 激活函数和一个线性层，即 Linear(tanh(Input))。但在将预训练好的模型应用于下游任务时，通常会丢弃预训练的MLP Head，换上一个全新的、符合新任务类别数量的单线性层。ViT整体结构如下图所示：\n\nIV. ViT代码复现1. Patch Embeddingimport torch\nimport torch.nn as nn\nfrom functools import partial\nfrom collections import OrderedDict\n\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    将图像分割成块 (patch) 并进行线性嵌入\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n        # img_size 输入图片大小 | patch_size 图片分块大小 | in_c 输入通道数 | embed_dim 嵌入后的维度\n        super().__init__()\n        img_size = (img_size, img_size)\n        patch_size = (patch_size, patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n        # 使用二维卷积实现分块和嵌入 (B,3,224,224) -> (B,768,14,14)\n        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"输入图像大小{H}*{W}与模型期望大小{self.img_size[0]}*{self.img_size[1]}不匹配\"\n\n        # (B,768,14,14) --flatten--> (B,768,196) --transpose--> (B,196,768)\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x\n\n2. multi-headclass Attention(nn.Module):\n    def __init__(self,\n                 dim,  # 输入的token维度,768\n                 num_heads = 8, # 注意力头数,为8\n                 qkv_bias=False, # 生成QKV的时候是否添加偏置\n                 qk_scale=None, # 用于缩放QK的系数,如果None,则使用1/sqrt(head_dim)\n                 atte_drop_ration=0., # 注意力分数的dropout比率\n                 proj_drop_ration=0.): # 最终投影层的dropout比率\n        super().__init__()\n        self.num_heads = num_heads # 注意力头数\n        head_dim = dim // num_heads  # 每个注意力头数的维度\n        self.scale = qk_scale or head_dim ** -0.5  #qk的缩放因子\n        self.qkv = nn.Linear(dim,dim*3,bisa=qkv_bias) # 通过全连接层生成QKV,为了并行计算,提高计算效率,参数更少\n        \"\"\"\"\n        这是实现多头注意力的一个巧妙且高效的方式。\n        它用一个全连接层，一次性地将输入 x (维度为 dim) 映射到一个维度为 dim * 3 的张量。\n        这个 dim * 3 的张量可以被看作是 Q, K, V 三个部分横向拼接在一起的结果，\n        后续我们只需要对这个大张量进行切分即可。\n        这样做比定义三个独立的线性层(一个给Q,一个给K,一个给V)在计算上更高效。\n        \"\"\"\n        self.atte_drop = nn.Dropout(atte_drop_ration)\n        self.proj_drop = nn.Dropout(proj_drop_ration)\n        self.proj = nn.Linear(dim,dim) # 将每个head得到的输出进行concat拼接,然后通过线性变换映射为原本的嵌入dim\n\n    def forward(self,x):\n        B,N,C = x.shape  # 批大小, 图块数+1(这个1为class_token), 通道数\n        # reshape: B,N,3*C -> B,N,3,num_head,C//num_head\n        # permute: B,N,3,num_head,C//num_head  ->  3,B,num_heads,N,C//self.num_heads\n        #这样一来，Q, K, V就被分开了，并且每个头的计算所需的数据（N 和 head_dim）都排列在一起，非常适合进行批处理矩阵运算\n        qkv = self.qkv(x).reshape(B,N,3,self.num_head,C//self.num_head).permute(2,0,3,1,4)\n        # 用切片拿到q,k,v. 形状都是: [B, num_heads, N, head_dim]\n        q,k,v = qkv[0],qkv[1],qkv[2]\n        # transpose: (B,num_heads,N,C//self.num_heads)  ->  (B,num_heads,C//self.num_heads,N)\n        # 这是一个批处理矩阵乘法。对于 B 个样本中的每一个和 num_heads 个头中的每一个，\n        # 我们都计算一个 [N, head_dim] 的 q 矩阵和一个 [head_dim, N] 的 k 转置矩阵的乘积\n        # 结果是一个 [N, N] 的矩阵，这个矩阵的第 (i, j) 个元素表示序列中第 i 个 token 对第 j 个 token 的注意力分数\n        attn = (q @ k.transpose(-2,-1))*self.scale  #形状为[B,num_heads,N,N] \n        attn = attn.softmax(dim=-1) # 对每个头的注意力分数矩阵的 每一行 进行归一化，使其和为1\n        attn = self.atte_drop(attn) # 应用dropout\n        # 注意力权重对V进行加权求和\n        # attn @ V: B,num_heads,N,C//self.num_heads\n        # transpose(1,2): B,N,num_heads,C//self.num_heads\n        # reshape(B,N,C): 将最后两个维度信息拼接,合并多个头输出,回到总的嵌入维度\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        # 将拼接好的多头输出通过最后一个线性层 self.proj。这一步允许模型学习如何最好地融合来自不同头的信息\n        x = self.proj(x)\n        x = self.proj_drop(x) # 应用最后的 dropout，防止过拟合\n        return x\n\n3. Blockclass MLP(nn.Module):\n    def __init__(self,in_features,hidden_features=None,out_features=None,act_layer=nn.GELU,drop=0.):\n        # in_features输入的维度  hidden_features隐藏层的维度,通常为in_features的4倍  out_features输出的维度,通常与in_features相同\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features,hidden_features)   # 第一个全连接层\n        self.act = act_layer()  # 激活层,默认GELU函数\n        self.fc2 = nn.Linear(hidden_features,out_features)  # 第二个全连接层\n        self.drop = nn.Dropout(drop)   # dropout层\n\n    def forward(self,x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef drop_path(x, drop_prob:float = 0., training: bool = False):\n    \"\"\"\n    实现DropPath的核心功能。\n    以 drop_prob 的概率将输入的整个张量 x 置零。\n    这是 Stochastic Depth 网络中的主要正则化方法。\n    \"\"\"\n    # 如果丢弃概率为0，或者当前不是训练模式，则直接返回原始输入x\n    # 在评估或推理时，我们不希望随机丢弃任何路径\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1-drop_prob   # 计算需要保留的路径的概率 (keep_prob)\n    # 创建一个生成随即掩码的形状元组，shape会是 (batch_size, 1, 1, ...)，1的数量取决于x的维度\n    shape = (x.shape[0],) + (1,)*(x.ndim - 1)\n    # 生成一个随机张量。torch.rand生成[0, 1)之间的均匀分布随机数。\n    # 加上keep_prob后，random_tensor的范围变为 [keep_prob, 1 + keep_prob)\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.devide)\n    # floor_(): 对随机张量进行向下取整\n    # 结果是：原先在 [keep_prob, 1) 区间的数值变为0，原先在 [1, 1 + keep_prob) 区间的数值变为1\n    # 一个数落在 [1, 1+keep_prob) 的概率恰好是 keep_prob\n    # 这样，random_tensor就变成了一个二值掩码（0或1）\n    random_tensor.floor_()\n    # 将输入x除以keep_prob，然后乘以二值掩码\n    # 乘以random_tensor：将一部分样本的整个张量置零（实现DropPath）。\n    # 除以keep_prob：这是一种被称为\"Inverted Dropout\"的技术。通过在训练时放大保留下来的输出，\n    # 可以保证在推理时（此时keep_prob为1，不做任何操作）网络的期望输出与训练时保持一致，无需在推理阶段进行额外的缩放\n    output = x.div(keep_prob)*random_tensor\n    return output\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super(DropPath,self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self,x):\n        return drop_path(x,self.drop_prob,self.training) \n\n\nclass Block(nn.Module):\n    def __init__(self,\n                 dim, # 每个token的维度\n                 num_heads,  #多头自注意力的头数量\n                 mlp_ratio=4,  #计算hidden_features大小 为输入的四倍\n                 qkv_bias=False,  # qkv偏置\n                 qk_scale = None,  # 注意力缩放因子\n                 drop_ratio=0.,  # 多头自注意力机制的最后dropout比例\n                 attn_drop_ratio=0.,  # 生成qkv之后的dropout比例\n                 drop_path_ratio=0., # drop_path比例\n                 act_layer=nn.GELU,  # 激活函数\n                 norm_layer=nn.LayerNorm  # 正则化层\n                 ):\n        super(Block,self).__init__()\n        self.norm1 = norm_layer(dim) # transformer encoder 中的第一个norm层\n        self.attn = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias,qk_scale=qk_scale,\n                              attn_drop_ratio=attn_drop_ratio,proj_drop_ration=drop_ratio)\n        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio else nn.Identity()\n        self.norm2 = norm_layer(dim)  # 定义第二个layer_norm层\n        mlp_hidden_dim = int(dim*mlp_ratio)\n        # 定义mlp层\n        self.mlp = MLP(in_features=dim,hidden_features=mlp_hidden_dim,act_layer=act_layer,drop=drop_ratio)\n\n    def forward(self,x):\n        x = x + self.drop_path(self.attn(self.norm1(x))) # 前向传播部分，输入的x先经过layernorm再经过多头注意力\n        x = x + self.drop_path(self.mlp(self.norm2(x)))  # 将得到的x一次通过layernorm、mlp、drop_path\n\n4. Vision Transformerdef _init_vit_weights(m):\n    # 1. 如果是全连接层\n    if isinstance(m,nn.Linear):\n        # 使用截断正态分布进行初始化，这是原始ViT论文推荐的方法\n        # 截断正态分布可以防止权重值离均值太远，让初始状态更稳定\n        # std=0.02 是一个经验值。带下划线的方法表示这是个in-place（原地）操作\n        nn.init.trunc_normal_(m.weight,std=0.01)\n        # 如果该线性层有偏置(bias)项\n        if m.bias is not None:\n            # 将偏置初始化为0\n            nn.init.zeros_(m.bias)\n    # 2. 如果是2D卷积层\n    elif isinstance(m,nn.Conv2d):\n        # 使用Kaiming正态分布初始化。这是一种非常适合带有ReLU激活函数的卷积层的初始化方法\n        # 它可以有效防止梯度消失或爆炸\n        # mode=\"fan_out\" 表示根据输出通道数来调整方差\n        nn.init.kaiming_normal_(m.weight,mode=\"fan_out\")\n        # 如果该卷积层有偏置项\n        if m.bias is not None:\n            # 将偏置项初始化为0\n            nn.init.zeros_(m.bias)\n    # 3. 如果是层归一化层\n    elif isinstance(m,nn.LayerNorm):\n        # 将偏置初始化为0\n        nn.init.zeros_(m.bias)\n        # 将权重初始化为1\n        nn.init.ones_(m.weight)\n        # 这样做的目的是，当训练刚开始时，LayerNorm层几乎等同于一个恒等映射，不会改变输入的分布，让训练更稳定\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size=224,patch_size=16,in_c=3,num_classes=1000,\n                 embed_dim=768,depth=12,num_head=12,mlp_ratio=4.0,qkv_bias=True,\n                 qk_scale=None,representation_size=None,distilled=False,drop_ratio=0.,\n                 attn_drop_ratio=0.,drop_path_ratio=0.,embed_layer=PatchEmbed,norm_layer=None,\n                 act_layer=None):\n        \"\"\"\n        ViT模型构造函数。\n        Args:\n            img_size (int): 输入图像的尺寸\n            patch_size (int): 每个图像块(patch)的尺寸\n            in_c (int): 输入图像的通道数\n            num_classes (int): 最终分类的类别数\n            embed_dim (int): Token的嵌入维度 (D)\n            depth (int): Transformer Encoder的总层数\n            num_head (int): 多头注意力机制中的头数\n            mlp_ratio (float): Transformer Encoder中MLP层的维度扩展比例\n            qkv_bias (bool): 是否在Q,K,V生成时使用偏置\n            qk_scale (float, optional): QK缩放因子，默认为 1/sqrt(head_dim)\n            representation_size (int, optional): 在最终分类头之前，可选的中间全连接层维度\n            distilled (bool): 是否使用蒸馏模式 (DeiT模型)\n            drop_ratio (float): 全局Dropout比率\n            attn_drop_ratio (float): 注意力权重Dropout比率\n            drop_path_ratio (float): 随机深度的DropPath比率\n            embed_layer (nn.Module): 用于生成Patch Embedding的层\n            norm_layer (nn.Module, optional): 使用的归一化层\n            act_layer (nn.Module, optional): 使用的激活函数层\n        \"\"\"\n        # 调用父类nn.Module的初始化方法\n        super(VisionTransformer,self).__init__()\n        self.num_classes = num_classes # 保存分类数\n        self.num_features = self.embed_dim = embed_dim # 保存嵌入维度\n        # 判断是否使用蒸馏。如果使用，会多一个distillation token，总共2个特殊token；否则只有cls_token，1个\n        self.num_tokens = 2 if distilled else 1\n        # 如果未指定归一化层，则默认使用LayerNorm，并设置eps以增加数值稳定性。\n        # partial用于创建一个预设了部分参数的新函数\n        norm_layer = norm_layer or partial(nn.LayerNorm,eps=1e-6)\n        \"\"\"\n        partial 来自 Python 内置的 functools 模块，它的作用是将一个函数的某些参数“冻结”住，从而创建一个新的、更简单的函数\n        此处的意义是将所有nn.LayerNorm的eps固定为1e-6\n        \"\"\"\n        act_layer = act_layer or nn.GELU() # 如果未指定激活函数，则默认使用GELU\n        # 1. Patch Embedding层：将输入的图片(B, C, H, W)转换为一系列token(B, N, D)\n        self.patch_embed = embed_layer(img_size=img_size,patch_size=patch_size,in_c=in_c,embed_dim=embed_dim)\n        # 获取patch的数量\n        num_patches = self.patch_embed.num_patches  \n        # 2. 定义可学习的 [CLS] token。这个token最终的输出将代表整个图像的特征用于分类\n        # 初始形状为(1, 1, embed_dim)，1个token，维度为embed_dim\n        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n        # 3. 如果是蒸馏模式，定义可学习的 [DIST] token\n        self.dist_token = nn.Parameter(torch.zeros(1,1,embed_dim)) if distilled else None\n        # 4. 定义可学习的位置编码(Positional Embedding)。因为Transformer本身不感知顺序，需要它来提供位置信息\n        # 长度为 patch数量 + 特殊token数量\n        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches+self.num_tokens,embed_dim))\n        # 5. 在位置编码加入后，应用一个Dropout层\n        self.pos_drop = nn.Dropout(p = drop_ratio)\n        # 6. 构建随机深度(Stochastic Depth)的衰减率序列\n        # torch.linspace生成一个从0到drop_path_ratio的等差序列，长度为depth\n        # 这样，越深的Block，其drop_path_ratio越大，被\"丢弃\"的概率也越高\n        dpr = [x.item() for x in torch.linspace(0,drop_path_ratio,depth)]\n        # 7. 构建Transformer Encoder主体，由连续的Block堆叠而成\n        # 使用nn.Sequential将多个Block串联起来\n        self.block = nn.Sequential(*[\n            Block(dim=embed_dim,num_heads=num_head,mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,qk_scale=qk_scale,\n                  drop_ratio = drop_ratio,attn_drop_ratio=attn_drop_ratio,drop_path_ratio=dpr[i],\n                  norm_layer=norm_layer,act_layer=act_layer)\n            for i in range(depth)\n        ])\n        # 8. 经过所有Transformer Block后的最后一个LayerNorm层。\n        self.norm = norm_layer(embed_dim)\n        # 9. 定义分类头之前的一个可选的\"pre-logits\"层 (常用于从JFT等大数据集预训练后迁移学习)\n        if representation_size and not distilled:\n            self.has_logits = True\n            # 更新最终输出的特征维度\n            self.num_features = representation_size\n            # pre_logits是一个包含全连接层和Tanh激活的序列\n            self.pre_logits = nn.Sequential(OrderedDict([\n                (\"fc\",nn.Linear(embed_dim,representation_size)),\n                (\"act\",nn.Tanh())\n            ]))\n        else:\n            self.has_logits=False\n            # 如果不使用，则用一个恒等映射层代替\n            self.pre_logits=nn.Identity()\n        # 10. 定义最终的分类头 (Head)\n        # 将提取的特征映射到最终的分类数\n        self.head = nn.Linear(self.num_features,num_classes) if num_classes>0 else nn.Identity()\n        # 11. 如果是蒸馏模式，为distillation token也定义一个分类头\n        self.head_dist = None\n        if distilled:\n            self.head_dist = nn.Linear(self.embed_dim,self.num_classes) if num_classes>0 else nn.Identity()\n\n        # 12. 权重初始化\n        # 对位置编码、dist_token、cls_token进行截断正态分布初始化\n        nn.init.trunc_normal_(self.pos_embed,std=0.02)\n        if self.dist_token is not None:\n            nn.init.trunc_normal_(self.dist_token,std=0.02)\n        nn.init.trunc_normal_(self.cls_token,std=0.02)\n        # 使用self.apply()方法，将_init_vit_weights函数递归地应用到模型的所有子模块上\n        self.apply(_init_vit_weights)\n\n    def forward_features(self,x):\n        \"\"\"提取特征的前向传播过程，不包括最后的分类头。\"\"\"\n        # x 初始形状: [B, C, H, W]\n        # 1. Patch Embedding: [B, C, H, W] -> [B, num_patches, embed_dim]\n        x = self.patch_embed(x)\n        # 2. 将cls_token在batch维度上进行扩展，以匹配输入x的batch_size\n        # expand()是一个高效的操作，它不会实际复制数据\n        cls_token = self.cls_token.expand(x.shape[0],-1,-1) # [1, 1, D] -> [B, 1, D]\n        # 3. 将特殊token与patch token拼接在一起\n        if self.dist_token is None:\n            # 若没有蒸馏token，拼接: [B, 1, D] 和 [B, N, D] -> [B, N+1, D]\n            x = torch.cat((cls_token,x),dim=1)\n        else:\n            # 蒸馏模式下，拼接cls_token和dist_token\n            x = torch.cat((cls_token,self.dist_token.expand(x.shape[0],-1,-1),x),dim=1)\n        # 4. 加上位置编码，然后应用dropout\n        # pos_embed的[B]维度会自动广播以匹配x\n        x = self.pos_drop(x+self.pos_embed)\n        # 5. 通过Transformer Encoder主干网络\n        x = self.block(x)\n        # 6. 通过最后的LayerNorm\n        x = self.norm(x)\n        # 7. 提取用于分类的token的输出\n        if self.dist_token is None:\n            # 只返回cls_token的输出 (在序列的第0个位置)，并通过pre_logits层\n            return self.pre_logits(x[:,0])\n        else:\n            # 返回cls_token和dist_token的输出\n            return x[:,0],x[:,1]\n\n    def forward(self,x):\n        \"\"\"完整的从输入到输出的前向传播过程。\"\"\"\n        # 1. 首先通过forward_features提取特征\n        x = self.forward_features(x)\n        # 2. 通过最后的分类头得到logits\n        if self.head_dist is not None:\n            # 蒸馏模式下，两个token分别通过各自的分类头\n            # x此时是一个元组 (cls_output, dist_output)\n            x_cls,x_dist = self.head(x[0]),self.head_dist(x[1])\n            if self.training and not torch.jit.is_scripting():\n                # 在训练并且不是在 TorchScript 编译模式下，返回两个头的输出，以便分别计算损失\n                return x_cls,x_dist\n            else:\n                # 在评估时，返回两个头输出的平均值作为最终预测\n                return (x_cls + x_dist) / 2\n        else:\n            # 标准模式下，直接将特征通过分类头\n            x = self.head(x)\n        return x\n\n\n\n\n\n\n\n\n\n\n蒸馏模式与 dist_token代码里的“蒸馏模式”来源于一篇非常重要的论文 DeiT (Data-efficient Image Transformers)。(1)什么是知识蒸馏 (Knowledge Distillation)？这是一种模型压缩和迁移学习的技术，核心思想是让一个强大而复杂的“教师模型”来指导一个轻量级的“学生模型”进行学习。\n\n教师模型: 通常是一个已经在大规模数据集上训练好的、性能非常强的模型（比如一个超大的 ResNet 或者另一个 ViT）。\n学生模型: 我们当前正在训练的模型（比如这个 VisionTransformer）。指导的方式不仅仅是让学生模型学习正确的标签（比如图片是“猫”），还会让它学习教师模型输出的“软标签”。软标签是指教师模型对所有类别的预测概率分布，例如它可能认为图片是“猫”的概率是85%，是“狗”的概率是10%，是“老虎”的… 这个概率分布包含了教师模型“思考过程”的丰富信息。\n\n(2)为什么 ViT 需要蒸馏？原始的 ViT 需要在海量的数据集（如谷歌内部的 JFT-300M，包含3亿张图片）上预训练才能获得优异的性能。如果只在 ImageNet-1k（约120万张图片）这种“中等”规模的数据集上从头训练，效果往往不如经典的 CNN 模型。DeiT 论文发现，通过知识蒸馏，可以让一个 ViT 在仅使用 ImageNet-1k 的情况下，达到甚至超过在 JFT-300M 上预训练的效果，极大地提高了 ViT 的数据效率。\n(3)dist_token 的作用DeiT 论文提出了一种新颖的蒸馏方式，就是通过添加一个专门用于蒸馏的 distillation token（即 dist_token）。\n\ncls_token 的任务: 和原来一样，它的最终输出用来和真实的标签 (ground-truth label) 计算损失，我们称之为“硬标签损失”。\ndist_token 的任务: 它是一个和 cls_token 地位相同的可学习向量，也被拼接到序列中，通过 Transformer 网络。但它的最终输出是专门用来和教师模型的预测（软标签或硬标签） 计算损失的，我们称之为“蒸馏损失”。\n\n通过这种方式，模型在训练时会同时优化两个目标：\n\n让 cls_token 的输出尽可能接近真实答案。\n让 dist_token 的输出尽可能模仿教师模型的答案。这种双重监督机制被证明非常有效，dist_token 就像一个专门负责从教师那里“偷师学艺”的通道，帮助学生模型学得更好。\n\npre-logits 层的作用pre-logits 层，可以理解为一个在最终分类头（self.head）之前的一个特征处理/映射层。它的主要作用是为了更好地进行迁移学习。想象一个场景：\n\n一个机构（比如谷歌）在一个超级庞大的私有数据集（比如 JFT-300M，有18000个类别）上预训练了一个 ViT 模型。\n这个预训练模型的最终分类头是 nn.Linear(embed_dim, 18000)。\n现在你想把这个模型用到你自己的任务上，比如一个只有10个类别的猫狗分类任务。\n\n显然，那个输出18000个维度的分类头对你来说是没用的。但是，它之前的网络层学到的特征提取能力是非常宝贵的。在这种情况下，pre-logits 层就派上用场了：\n\n它通常是一个 nn.Linear(embed_dim, representation_size) 加上一个激活函数（如 Tanh）。\n在预训练时，模型会先将 cls_token 的输出通过这个 pre-logits 层，得到一个固定维度的“特征表示” (representation)，然后再将这个表示送入最终的分类头。\n当你拿到这个预训练模型进行迁移学习时，你可以丢弃掉原有的最终分类头，但保留 pre-logits 层。然后，你只需要在 pre-logits 层的输出后面接上你自己任务的分类头，例如 nn.Linear(representation_size, 10)。\n\n5. 创建应用层def vit_base_patch16_224(num_classes:int =100, pretrained=False):\n    model = VisionTransformer(img_size=224,\n                              patch_size=16,\n                              embed_dim=768,\n                              depth=12,\n                              num_head=12,\n                              representation_size=None,\n                              num_classes=num_classes)\n    return model\n\n参考笔记：ViT讲解\n","slug":"ViT论文精读","date":"2025-07-02T03:32:00.000Z","categories_index":"论文精读","tags_index":"Vision-Transformer,深度学习","author_index":"犬夜叉"}]