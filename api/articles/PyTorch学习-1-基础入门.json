{"title":"PyTorch学习-1.基础入门","uid":"4d6d66bf862bc9de2ae8dbbddfd17c18","slug":"PyTorch学习-1-基础入门","date":"2025-07-07T13:25:37.000Z","updated":"2025-07-08T01:47:05.286Z","comments":true,"path":"api/articles/PyTorch学习-1-基础入门.json","keywords":null,"cover":null,"content":"<h1 id=\"PyTorch-学习笔记-阶段一：基础入门\"><a href=\"#PyTorch-学习笔记-阶段一：基础入门\" class=\"headerlink\" title=\"PyTorch 学习笔记 - 阶段一：基础入门\"></a>PyTorch 学习笔记 - 阶段一：基础入门</h1><h2 id=\"模块-1：PyTorch-环境与简介\"><a href=\"#模块-1：PyTorch-环境与简介\" class=\"headerlink\" title=\"模块 1：PyTorch 环境与简介\"></a>模块 1：PyTorch 环境与简介</h2><h3 id=\"1-什么是-PyTorch？\"><a href=\"#1-什么是-PyTorch？\" class=\"headerlink\" title=\"1. 什么是 PyTorch？\"></a>1. 什么是 PyTorch？</h3><p>PyTorch 是一个由 Facebook AI 研究院主导开发的、基于 Python 的开源机器学习库。它主要用于两个领域：</p>\n<ol>\n<li><strong>替代 NumPy</strong>：可以利用 GPU 的强大计算能力进行张量计算。</li>\n<li><strong>深度学习平台</strong>：提供极大的灵活性和速度来构建和训练深度学习模型。</li>\n</ol>\n<p><strong>核心特点：</strong></p>\n<ul>\n<li><strong>Pythonic</strong>: PyTorch 的 API 设计非常直观，贴近 Python 编程风格，易于上手。</li>\n<li><strong>动态计算图 (Dynamic Computational Graph)</strong>: 这是 PyTorch 最显著的特点之一。计算图是“在运行时定义”（Define-by-Run）的，意味着图的结构会随着代码的执行而动态构建。这使得调试变得异常简单，并且非常适合处理动态结构的网络，如循环神经网络（RNN）。</li>\n<li><strong>强大的社区</strong>: 尤其在学术界，PyTorch 拥有极高的使用率，这意味着你可以轻松找到最新的论文实现和丰富的学习资源。</li>\n</ul>\n<p><strong>应用场景</strong>:<br>PyTorch 广泛应用于计算机视觉 (CV)、自然语言处理 (NLP)、语音识别等前沿领域。许多顶尖的研究成果都是使用 PyTorch 实现的。</p>\n<h3 id=\"2-PyTorch-vs-TensorFlow\"><a href=\"#2-PyTorch-vs-TensorFlow\" class=\"headerlink\" title=\"2. PyTorch vs. TensorFlow\"></a>2. PyTorch vs. TensorFlow</h3><p>这是一个常见的问题。两者都是顶级的深度学习框架，但设计哲学上有所不同。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">特性</th>\n<th style=\"text-align:left\">PyTorch</th>\n<th style=\"text-align:left\">TensorFlow</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"><strong>计算图</strong></td>\n<td style=\"text-align:left\"><strong>动态图</strong></td>\n<td style=\"text-align:left\">静态图 为主，TF 2.x 后也默认采用 Eager Execution</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong>API 风格</strong></td>\n<td style=\"text-align:left\">更贴近 Python，面向对象，直观</td>\n<td style=\"text-align:left\">选项更多，API 略显复杂，但生态系统（如 TFX, TF Serving）更完善</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong>调试</strong></td>\n<td style=\"text-align:left\">非常容易，可使用标准 Python 调试工具 (如 pdb)</td>\n<td style=\"text-align:left\">相对困难，需要特定的调试工具</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong>社区</strong></td>\n<td style=\"text-align:left\"><strong>学术界首选</strong>，研究和快速原型开发非常流行</td>\n<td style=\"text-align:left\"><strong>工业界部署</strong>方案成熟，拥有庞大的开发者基础</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong>学习曲线</strong></td>\n<td style=\"text-align:left\"><strong>平缓</strong>，对初学者友好</td>\n<td style=\"text-align:left\">相对陡峭，概念较多</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>小结</strong>: 对于初学者和研究人员来说，PyTorch 通常是入门的首选，因为它更直观、易于调试。TensorFlow 在工业部署和生产环境方面有其传统优势。不过，随着两个框架的互相借鉴，它们的差距正在逐渐缩小。</p>\n<h3 id=\"3-环境搭建\"><a href=\"#3-环境搭建\" class=\"headerlink\" title=\"3. 环境搭建\"></a>3. 环境搭建</h3><p>为了不污染你主机的 Python 环境，我们强烈建议使用虚拟环境。Conda 是管理数据科学包的绝佳工具。</p>\n<p><strong>使用 Conda 创建和激活环境:</strong></p>\n<pre class=\"line-numbers language-lang-bash\"><code class=\"language-lang-bash\"># 1. 创建一个名为 \"pytorch_env\" 的新环境，并指定 Python 版本（推荐 3.8, 3.9 或 3.10）\nconda create -n pytorch_env python=3.9\n\n# 2. 激活这个新环境\nconda activate pytorch_env\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>安装 PyTorch:</strong></p>\n<p><strong>最重要的一步</strong>: 不要直接 <code>pip install pytorch</code>！请务必访问 <a href=\"https://pytorch.org/get-started/locally/\">PyTorch 官网</a>。官网会根据你的操作系统、包管理器（Conda/Pip）、计算平台（CPU/GPU）自动生成最适合你的安装指令。</p>\n<p><strong>示例 (以 Windows/Linux, Conda, CUDA 11.8 为例):</strong></p>\n<pre class=\"line-numbers language-lang-bash\"><code class=\"language-lang-bash\"># 前往官网，你可能会得到类似这样的指令\n# 这是针对支持 NVIDIA GPU 的版本\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# 如果你没有 NVIDIA GPU，请选择 CPU 版本\n# 指令会是这样：\npip3 install torch torchvision torchaudio\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"4-第一个-PyTorch-程序-“Hello-Tensor-”\"><a href=\"#4-第一个-PyTorch-程序-“Hello-Tensor-”\" class=\"headerlink\" title=\"4. 第一个 PyTorch 程序 - “Hello, Tensor!”\"></a>4. 第一个 PyTorch 程序 - “Hello, Tensor!”</h3><p>安装完成后，让我们来验证一下。在你的虚拟环境中，打开 Python 解释器或创建一个 <code>.py</code> 文件。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># main.py\nimport torch\n\n# 打印 PyTorch 版本\nprint(f\"PyTorch Version: {torch.__version__}\")\n\n# 检查 GPU 是否可用\nis_cuda_available = torch.cuda.is_available()\nprint(f\"CUDA (GPU) Available: {is_cuda_available}\")\nif is_cuda_available:\n    print(f\"Current CUDA device: {torch.cuda.get_device_name(0)}\")\n\n# 创建我们的第一个张量！\nx = torch.tensor([[1, 2], [3, 4]])\nprint(\"\\nHello, Tensor!\")\nprint(x)\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>正常输出：</strong></p>\n<pre><code>PyTorch Version: 2.7.1+cu118\nCUDA (GPU) Available: True\nCurrent CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n\nHello, Tensor!\ntensor([[1, 2],\n        [3, 4]])\n</code></pre><p>运行这个脚本，如果一切正常，它会打印出你的 PyTorch 版本、GPU 信息（如果有）以及一个 2x2 的张量。</p>\n<hr>\n<h2 id=\"模块-2：核心基石-张量-Tensor\"><a href=\"#模块-2：核心基石-张量-Tensor\" class=\"headerlink\" title=\"模块 2：核心基石 - 张量 (Tensor)\"></a>模块 2：核心基石 - 张量 (Tensor)</h2><h3 id=\"1-张量是什么？\"><a href=\"#1-张量是什么？\" class=\"headerlink\" title=\"1. 张量是什么？\"></a>1. 张量是什么？</h3><p><strong>张量 (Tensor)</strong> 是 PyTorch 中最基本的数据结构。你可以简单地把它理解为一个<strong>多维数组</strong>。它是所有模型输入、输出和参数的载体。</p>\n<ul>\n<li><strong>0D 张量 (标量)</strong>: 一个数字。<code>torch.tensor(5)</code></li>\n<li><strong>1D 张量 (向量)</strong>: 一个数组。<code>torch.tensor([1, 2, 3])</code></li>\n<li><strong>2D 张量 (矩阵)</strong>: 一个二维数组。<code>torch.tensor([[1, 2], [3, 4]])</code></li>\n<li><strong>3D 张量</strong>: 通常用于表示图像 (Height ， Width ， Channels) 或序列数据。</li>\n<li><strong>nD 张量</strong>: 更高维度的数据。</li>\n</ul>\n<h3 id=\"2-创建张量\"><a href=\"#2-创建张量\" class=\"headerlink\" title=\"2. 创建张量\"></a>2. 创建张量</h3><p>PyTorch 提供了多种创建张量的方法。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">import torch\nimport numpy as np\n\n# 1. 从 Python 列表或 NumPy 数组创建\ndata = [[1, 2], [3, 4]]\nx_data = torch.tensor(data)\nprint(f\"From list:\\n {x_data}\")\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array) # 使用 from_numpy 效率更高\nprint(f\"From NumPy array:\\n {x_np}\")\n\n# 3. 创建与另一个张量形状和属性相同的张量\nx_ones = torch.ones_like(x_data) # 保留 x_data 的属性 (如 dtype)\nprint(f\"Ones Tensor like x_data:\\n {x_ones}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>From list:\n tensor([[1, 2],\n        [3, 4]])\nFrom NumPy array:\n tensor([[1, 2],\n        [3, 4]])\n</code></pre><pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 2. 创建指定形状的张量\nshape = (2, 3,)\n# 未初始化的张量，值是随机的\nrand_tensor = torch.rand(shape) \n# 全 1 张量\nones_tensor = torch.ones(shape)\n# 全 0 张量\nzeros_tensor = torch.zeros(shape)\n# 从标准正态分布中采样的张量\nrandn_tensor = torch.randn(shape)\n\nprint(f\"Random Tensor:\\n {rand_tensor}\")\nprint(f\"Ones Tensor:\\n {ones_tensor}\")\nprint(f\"Zeros Tensor:\\n {zeros_tensor}\")\nprint(f\"Randn Tensor:\\n {randn_tensor}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>Random Tensor:\n tensor([[0.0645, 0.5513, 0.6257],\n        [0.6369, 0.9466, 0.3795]])\nOnes Tensor:\n tensor([[1., 1., 1.],\n        [1., 1., 1.]])\nZeros Tensor:\n tensor([[0., 0., 0.],\n        [0., 0., 0.]])\nRandn Tensor:\n tensor([[-0.1138,  0.1148,  0.1198],\n        [ 1.4761, -0.5143, -0.7777]])\n</code></pre><pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 3. 创建与另一个张量形状和属性相同的张量\nx_ones = torch.ones_like(x_data) # 保留 x_data 的属性 (如 dtype)\nprint(f\"Ones Tensor like x_data:\\n {x_ones}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<pre><code>Ones Tensor like x_data:\n tensor([[1, 1],\n        [1, 1]])\n</code></pre><h3 id=\"3-张量的数据类型-dtype\"><a href=\"#3-张量的数据类型-dtype\" class=\"headerlink\" title=\"3. 张量的数据类型 (dtype)\"></a>3. 张量的数据类型 (<code>dtype</code>)</h3><p>每个张量都有一个数据类型 (<code>dtype</code>)。在深度学习中，最常用的是 <code>torch.float32</code>（单精度浮点数）和 <code>torch.long</code>（用于标签或索引）。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 默认是 torch.float32\nfloat_tensor = torch.ones(2, 2) \nprint(f\"Default dtype: {float_tensor.dtype}\")\n\n# 指定 dtype\nlong_tensor = torch.tensor([1, 2, 3], dtype=torch.long)\nprint(f\"Long tensor dtype: {long_tensor.dtype}\")\n\n# 转换 dtype\nfloat_tensor_from_long = long_tensor.to(torch.float16) # .float() 是 .to(torch.float32) 的简写\nprint(f\"Converted to float16: {float_tensor_from_long.dtype}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>Default dtype: torch.float32\nLong tensor dtype: torch.int64\nConverted to float16: torch.float16\n</code></pre><h3 id=\"4-张量操作\"><a href=\"#4-张量操作\" class=\"headerlink\" title=\"4. 张量操作\"></a>4. 张量操作</h3><p>这是 PyTorch 的精髓所在，操作与 NumPy 非常相似。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">tensor = torch.ones(4, 4)\n# 将第 1 行（索引为 0）的所有元素变为 2\ntensor[0, :] = 2 \nprint(f\"Original Tensor:\\n {tensor}\\n\")\n\n# 索引和切片 (Slicing)\nfirst_row = tensor[0]\nfirst_col = tensor[:, 0]\nsub_tensor = tensor[1:3, 1:3]\nprint(f\"First Row: {first_row}\")\nprint(f\"First Column: {first_col}\")\nprint(f\"Sub-tensor (2x2):\\n {sub_tensor}\\n\")\n\n# 拼接 (Concatenating)\n# dim=0 按行拼接, dim=1 按列拼接\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(f\"Concatenated Tensor (dim=1):\\n {t1}\\n\")\n\n# 算术运算\n# 矩阵乘法\nmat_mul = tensor.matmul(tensor.T) # tensor.T 是转置\n# 逐元素乘法\nelem_mul = tensor.mul(tensor)\n\nprint(f\"Matrix Multiplication:\\n {mat_mul}\\n\")\nprint(f\"Element-wise Multiplication:\\n {elem_mul}\\n\")\n\n# 数学函数\nsum_val = tensor.sum()\nmean_val = tensor.mean()\n# .item() 用于从只包含一个元素的张量中提取 Python 数值\nprint(f\"Sum of all elements: {sum_val.item()}\")\nprint(f\"Mean of all elements: {mean_val.item()}\")\n\n# 变形 (Reshaping)\n# view() 和 reshape() 功能类似，view() 更高效但对内存连续性有要求\n# reshape() 更灵活\nflat_tensor = tensor.reshape(-1) # -1 表示自动计算该维度的大小\nprint(f\"Flattened tensor: {flat_tensor}\")\nprint(f\"New shape (8x2): \\n{tensor.reshape(8, 2)}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>Original Tensor:\n tensor([[2., 2., 2., 2.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\nFirst Row: tensor([2., 2., 2., 2.])\nFirst Column: tensor([2., 1., 1., 1.])\nSub-tensor (2x2):\n tensor([[1., 1.],\n        [1., 1.]])\n\nConcatenated Tensor (dim=1):\n tensor([[2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n</code></pre><pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 算术运算\n# 矩阵乘法\nmat_mul = tensor.matmul(tensor.T) # tensor.T 是转置\n# 逐元素乘法\nelem_mul = tensor.mul(tensor)\n\nprint(f\"Matrix Multiplication:\\n {mat_mul}\\n\")\nprint(f\"Element-wise Multiplication:\\n {elem_mul}\\n\")\n\n# 数学函数\nsum_val = tensor.sum()\nmean_val = tensor.mean()\n# .item() 用于从只包含一个元素的张量中提取 Python 数值\nprint(f\"Sum of all elements: {sum_val.item()}\")\nprint(f\"Mean of all elements: {mean_val.item()}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>Matrix Multiplication:\n tensor([[16.,  8.,  8.,  8.],\n        [ 8.,  4.,  4.,  4.],\n        [ 8.,  4.,  4.,  4.],\n        [ 8.,  4.,  4.,  4.]])\n\nElement-wise Multiplication:\n tensor([[4., 4., 4., 4.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\nSum of all elements: 20.0\nMean of all elements: 1.25\n</code></pre><pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 变形 (Reshaping)\n# view() 和 reshape() 功能类似，view() 更高效但对内存连续性有要求\n# reshape() 更灵活\nflat_tensor = tensor.reshape(-1) # -1 表示自动计算该维度的大小\nprint(f\"Flattened tensor: {flat_tensor}\")\nprint(f\"New shape (8x2): \\n{tensor.reshape(8, 2)}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>Flattened tensor: tensor([2., 2., 2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\nNew shape (8x2): \ntensor([[2., 2.],\n        [2., 2.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])\n</code></pre><h3 id=\"5-与-NumPy-的交互\"><a href=\"#5-与-NumPy-的交互\" class=\"headerlink\" title=\"5. 与 NumPy 的交互\"></a>5. 与 NumPy 的交互</h3><p>PyTorch 与 NumPy 之间的转换非常高效，因为当张量在 CPU 上时，它们共享底层的内存地址。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># Tensor -> NumPy\nnumpy_array = tensor.numpy()\nprint(f\"Type after .numpy(): {type(numpy_array)}\")\n\n# NumPy -> Tensor\nnew_tensor = torch.from_numpy(numpy_array)\nprint(f\"Type after torch.from_numpy(): {type(new_tensor)}\")\n\n# 重点：共享内存\nnp.add(numpy_array, 1, out=numpy_array) # 修改 NumPy 数组\nprint(f\"Original tensor after modifying NumPy array:\\n {tensor}\")\nprint(f\"New tensor after modifying NumPy array:\\n {new_tensor}\")\n# 输出会显示 tensor 和 new_tensor 的值都改变了\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>Type after .numpy(): &lt;class 'numpy.ndarray'&gt;\nType after torch.from_numpy(): &lt;class 'torch.Tensor'&gt;\nOriginal tensor after modifying NumPy array:\n tensor([[3., 3., 3., 3.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\nNew tensor after modifying NumPy array:\n tensor([[3., 3., 3., 3.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n</code></pre><h3 id=\"6-GPU-加速\"><a href=\"#6-GPU-加速\" class=\"headerlink\" title=\"6. GPU 加速\"></a>6. GPU 加速</h3><p>这是 PyTorch 相比 NumPy 的巨大优势。要使用 GPU，你需要将张量移动到 GPU 设备上。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 1. 检查 GPU 是否可用，并设置设备\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# 2. 创建一个在 CPU 上的张量\ncpu_tensor = torch.ones(2, 2)\nprint(f\"cpu_tensor is on device: {cpu_tensor.device}\")\n\n# 3. 将张量移动到 GPU\n# .to(device) 是推荐的标准做法\ngpu_tensor = cpu_tensor.to(device)\nprint(f\"gpu_tensor is on device: {gpu_tensor.device}\")\n\n# 4. 在 GPU 上执行运算\n# 注意：参与运算的张量必须在同一个设备上！\nresult = gpu_tensor + gpu_tensor\nprint(f\"gpu_tensor + gpu_tensor=\\n{gpu_tensor + gpu_tensor}\")\n\n# 5. 将结果移回 CPU（例如，用于打印或与 NumPy 交互）\nresult_cpu = result.to(\"cpu\")\n# result_cpu.numpy() # 现在可以安全地转换为 NumPy 了\n\n# 尝试在不同设备上的张量进行运算会报错\ncpu_tensor + gpu_tensor # 这行代码会抛出错误\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>Using cuda device\ncpu_tensor is on device: cpu\ngpu_tensor is on device: cuda:0\ngpu_tensor + gpu_tensor=\ntensor([[2., 2.],\n        [2., 2.]], device='cuda:0')\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 23\n     19 result_cpu = result.to(\"cpu\")\n     20 # result_cpu.numpy() # 现在可以安全地转换为 NumPy 了\n     21 \n     22 # 尝试在不同设备上的张量进行运算会报错\n---&gt; 23 cpu_tensor + gpu_tensor # 这行代码会抛出错误\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n</code></pre><hr>\n<h2 id=\"模块-3：自动求导-Autograd\"><a href=\"#模块-3：自动求导-Autograd\" class=\"headerlink\" title=\"模块 3：自动求导 (Autograd)\"></a>模块 3：自动求导 (Autograd)</h2><p><code>torch.autograd</code> 是 PyTorch 的自动求导引擎，它为神经网络的训练提供了动力。</p>\n<h3 id=\"1-计算图-Computational-Graph\"><a href=\"#1-计算图-Computational-Graph\" class=\"headerlink\" title=\"1. 计算图 (Computational Graph)\"></a>1. 计算图 (Computational Graph)</h3><p>当你对张量执行任何操作时，PyTorch 都会在后台构建一个<strong>计算图</strong>。这个图记录了数据（张量）和操作（函数）之间的关系。</p>\n<ul>\n<li><strong>前向传播 (Forward Pass)</strong>: 当你的代码执行时，PyTorch 会记录下所有的操作，构建出这个图。</li>\n<li><strong>反向传播 (Backward Pass)</strong>: 当你调用 <code>.backward()</code> 时，PyTorch 会沿着这个图反向传播，利用<strong>链式法则</strong>自动计算出图中每个参数相对于最终输出的梯度。</li>\n</ul>\n<p>因为这个图是动态的，所以你可以在每次迭代中使用不同的控制流（如 <code>if</code>, <code>for</code>），PyTorch 依然能正确处理。</p>\n<h3 id=\"2-requires-grad-属性\"><a href=\"#2-requires-grad-属性\" class=\"headerlink\" title=\"2. requires_grad 属性\"></a>2. <code>requires_grad</code> 属性</h3><p>这是一个布尔值，用于告诉 PyTorch 是否需要追踪对该张量的操作，以便后续计算梯度。</p>\n<ul>\n<li><strong><code>requires_grad=True</code></strong>: 追踪！这个张量通常是模型的<strong>可学习参数</strong>（权重 <code>w</code> 和偏置 <code>b</code>）。</li>\n<li><strong><code>requires_grad=False</code></strong>: 不追踪。这个张量通常是模型的输入数据、标签或固定的参数。</li>\n</ul>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 模型参数，需要计算梯度\nw = torch.randn(1, requires_grad=True)\nb = torch.randn(1, requires_grad=True)\n\n# 输入数据，不需要计算梯度\nx = torch.tensor([2.0]) \n\n# 前向传播，构建计算图\ny = w * x + b # y 会自动获得 requires_grad=True\n\nprint(f\"w.requires_grad: {w.requires_grad}\")\nprint(f\"x.requires_grad: {x.requires_grad}\")\nprint(f\"y.requires_grad: {y.requires_grad}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>w.requires_grad: True\nx.requires_grad: False\ny.requires_grad: True\n</code></pre><h3 id=\"3-backward-方法\"><a href=\"#3-backward-方法\" class=\"headerlink\" title=\"3. backward() 方法\"></a>3. <code>backward()</code> 方法</h3><p>当你计算出损失（loss，一个标量）后，调用 <code>loss.backward()</code>，autograd 引擎就会自动完成所有梯度计算。</p>\n<ul>\n<li>梯度值会被计算并<strong>累加</strong>到各个参数张量的 <code>.grad</code> 属性中。</li>\n<li><strong>注意</strong>: <code>backward()</code> 只能对标量（0D 张量）调用。</li>\n</ul>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 假设我们有一个损失值\nloss = (y - 5.0) ** 2 # 假设目标值是 5.0\n\n# 反向传播，计算梯度\nloss.backward()\n\n# 查看梯度\n# d(loss)/dw 和 d(loss)/db\nprint(f\"Gradient for w: {w.grad}\")\nprint(f\"Gradient for b: {b.grad}\")\n\n# 注意：梯度是累加的！\n# 如果再执行一次 backward，梯度会翻倍\n# loss.backward() \n# print(f\"Gradient for w after second backward: {w.grad}\")\n# 这就是为什么在训练循环中每次都要清零梯度\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>执行一次：\nGradient for w: tensor([-17.9509])\nGradient for b: tensor([-8.9754])\n执行两次：\nRuntimeError: Trying to backward through the graph a second time (or directly \naccess saved tensors after they have already been freed). Saved intermediate \nvalues of the graph are freed when you call .backward() or autograd.grad(). \nSpecify retain_graph=True if you need to backward through the graph a second time \nor if you need to access saved tensors after calling backward.\n</code></pre><h3 id=\"4-torch-no-grad\"><a href=\"#4-torch-no-grad\" class=\"headerlink\" title=\"4. torch.no_grad()\"></a>4. <code>torch.no_grad()</code></h3><p>在某些情况下，比如模型评估或更新权重时，我们不希望 PyTorch 追踪操作。使用 <code>torch.no_grad()</code> 上下文管理器可以临时禁用梯度追踪。</p>\n<p><strong>好处</strong>:</p>\n<ul>\n<li><strong>节省内存</strong>: 不会存储中间计算结果用于反向传播。</li>\n<li><strong>加快速度</strong>: 避免了构建计算图的开销。</li>\n</ul>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">print(f\"w's grad before no_grad: {w.requires_grad}\")\n\nwith torch.no_grad():\n    # 在这个代码块内，所有操作都不会被追踪\n    new_y = w * x + b\n    print(f\"new_y's grad inside no_grad block: {new_y.requires_grad}\")\n\nprint(f\"w's grad after no_grad: {w.requires_grad}\") # w 本身的属性不受影响\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>w's grad before no_grad: True\nnew_y's grad inside no_grad block: False\nw's grad after no_grad: True\n</code></pre><h3 id=\"5-动手实践：手动实现线性回归\"><a href=\"#5-动手实践：手动实现线性回归\" class=\"headerlink\" title=\"5. 动手实践：手动实现线性回归\"></a>5. 动手实践：手动实现线性回归</h3><p>让我们用目前所学的所有知识，来解决一个最简单的机器学习问题：<code>y = w * x + b</code>。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">import torch\n\n# 1. 准备数据\nX_train = torch.tensor([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n                        [9.779], [6.182], [7.59], [2.167], [7.042], \n                        [10.791], [5.313], [7.997], [3.1]], dtype=torch.float32)\n\ny_train = torch.tensor([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n                        [3.366], [2.596], [2.53], [1.221], [2.827], \n                        [3.465], [1.65], [2.904], [1.3]], dtype=torch.float32)\n\n# 2. 初始化参数\n# 我们希望 PyTorch 帮我们找到最优的 w 和 b，所以设置 requires_grad=True\nw = torch.randn(1, requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\n# 3. 设置超参数\nlearning_rate = 0.01\nepochs = 100\n\n# 4. 训练循环\nfor epoch in range(epochs):\n    # a. 前向传播：计算预测值\n    y_pred = X_train.matmul(w) + b\n\n    # b. 计算损失 (MSE - 均方误差)\n    loss = torch.mean((y_pred - y_train) ** 2)\n\n    # c. 反向传播：计算梯度\n    # 在调用 backward() 前，必须清零上一轮的梯度\n    if w.grad is not None:\n        w.grad.zero_()\n    if b.grad is not None:\n        b.grad.zero_()\n\n    loss.backward()\n\n    # d. 更新权重：手动执行梯度下降\n    # 这里我们不希望更新权重的操作被追踪，所以使用 no_grad()\n    with torch.no_grad():\n        w -= learning_rate * w.grad\n        b -= learning_rate * b.grad\n\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n# 5. 查看学习到的参数\nprint(\"\\nTraining finished!\")\nprint(f\"Learned w: {w.item():.3f}\")\nprint(f\"Learned b: {b.item():.3f}\")\n\n# 真实世界的 w 约等于 0.25, b 约等于 0.75。你的结果应该很接近了\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre><code>Epoch [10/100], Loss: 1.1384\nEpoch [20/100], Loss: 1.1086\nEpoch [30/100], Loss: 1.0803\nEpoch [40/100], Loss: 1.0534\nEpoch [50/100], Loss: 1.0279\nEpoch [60/100], Loss: 1.0036\nEpoch [70/100], Loss: 0.9805\nEpoch [80/100], Loss: 0.9587\nEpoch [90/100], Loss: 0.9379\nEpoch [100/100], Loss: 0.9181\n\nTraining finished!\nLearned w: 0.242\nLearned b: 0.639\n</code></pre><p>这个简单的例子完美地展示了 Autograd 的作用：我们只定义了前向传播和损失函数，PyTorch 就自动为我们处理了复杂的梯度计算，让我们能专注于模型的设计和训练过程。</p>\n","text":"PyTorch 学习笔记 - 阶段一：基础入门模块 1：PyTorch 环境与简介1. 什么是 PyTorch？PyTorch 是一个由 Facebook AI...","permalink":"/post/PyTorch学习-1-基础入门","photos":[],"count_time":{"symbolsCount":"14k","symbolsTime":"13 mins."},"categories":[{"name":"python语法学习","slug":"python语法学习","count":5,"path":"api/categories/python语法学习.json"}],"tags":[{"name":"PyTorch","slug":"PyTorch","count":3,"path":"api/tags/PyTorch.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#PyTorch-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%98%B6%E6%AE%B5%E4%B8%80%EF%BC%9A%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8\"><span class=\"toc-text\">PyTorch 学习笔记 - 阶段一：基础入门</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9D%97-1%EF%BC%9APyTorch-%E7%8E%AF%E5%A2%83%E4%B8%8E%E7%AE%80%E4%BB%8B\"><span class=\"toc-text\">模块 1：PyTorch 环境与简介</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-%E4%BB%80%E4%B9%88%E6%98%AF-PyTorch%EF%BC%9F\"><span class=\"toc-text\">1. 什么是 PyTorch？</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-PyTorch-vs-TensorFlow\"><span class=\"toc-text\">2. PyTorch vs. TensorFlow</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA\"><span class=\"toc-text\">3. 环境搭建</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#4-%E7%AC%AC%E4%B8%80%E4%B8%AA-PyTorch-%E7%A8%8B%E5%BA%8F-%E2%80%9CHello-Tensor-%E2%80%9D\"><span class=\"toc-text\">4. 第一个 PyTorch 程序 - “Hello, Tensor!”</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9D%97-2%EF%BC%9A%E6%A0%B8%E5%BF%83%E5%9F%BA%E7%9F%B3-%E5%BC%A0%E9%87%8F-Tensor\"><span class=\"toc-text\">模块 2：核心基石 - 张量 (Tensor)</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-%E5%BC%A0%E9%87%8F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F\"><span class=\"toc-text\">1. 张量是什么？</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F\"><span class=\"toc-text\">2. 创建张量</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-dtype\"><span class=\"toc-text\">3. 张量的数据类型 (dtype)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#4-%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C\"><span class=\"toc-text\">4. 张量操作</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#5-%E4%B8%8E-NumPy-%E7%9A%84%E4%BA%A4%E4%BA%92\"><span class=\"toc-text\">5. 与 NumPy 的交互</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#6-GPU-%E5%8A%A0%E9%80%9F\"><span class=\"toc-text\">6. GPU 加速</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9D%97-3%EF%BC%9A%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC-Autograd\"><span class=\"toc-text\">模块 3：自动求导 (Autograd)</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-%E8%AE%A1%E7%AE%97%E5%9B%BE-Computational-Graph\"><span class=\"toc-text\">1. 计算图 (Computational Graph)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-requires-grad-%E5%B1%9E%E6%80%A7\"><span class=\"toc-text\">2. requires_grad 属性</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-backward-%E6%96%B9%E6%B3%95\"><span class=\"toc-text\">3. backward() 方法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#4-torch-no-grad\"><span class=\"toc-text\">4. torch.no_grad()</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#5-%E5%8A%A8%E6%89%8B%E5%AE%9E%E8%B7%B5%EF%BC%9A%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92\"><span class=\"toc-text\">5. 动手实践：手动实现线性回归</span></a></li></ol></li></ol></li></ol>","author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"PyTorch学习-2-神经网络核心","uid":"d970942a0c3301df824e3c1c77149df6","slug":"PyTorch学习-2-神经网络核心","date":"2025-07-08T01:46:23.000Z","updated":"2025-07-08T06:05:08.835Z","comments":true,"path":"api/articles/PyTorch学习-2-神经网络核心.json","keywords":null,"cover":null,"text":"PyTorch 学习笔记 - 阶段二：神经网络的核心在前面的章节中，我们已经熟悉了 PyTorch 的基本数据结构——张量（Tensor），以及如何在它们之上执...","permalink":"/post/PyTorch学习-2-神经网络核心","photos":[],"count_time":{"symbolsCount":"17k","symbolsTime":"15 mins."},"categories":[{"name":"python语法学习","slug":"python语法学习","count":5,"path":"api/categories/python语法学习.json"}],"tags":[{"name":"PyTorch","slug":"PyTorch","count":3,"path":"api/tags/PyTorch.json"}],"author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}}},"next_post":{"title":"NumPy 学习笔记：第二阶段 - 核心技能","uid":"b34a2d20ebaf2f1ab87598c5d48f72ca","slug":"NumPy-学习笔记：第二阶段-核心技能","date":"2025-07-07T09:23:26.000Z","updated":"2025-07-07T10:24:13.120Z","comments":true,"path":"api/articles/NumPy-学习笔记：第二阶段-核心技能.json","keywords":null,"cover":null,"text":"这个阶段的目标是掌握 NumPy 更高级的数组操作和核心功能，这是进行数据分析和科学计算的基础...","permalink":"/post/NumPy-学习笔记：第二阶段-核心技能","photos":[],"count_time":{"symbolsCount":"11k","symbolsTime":"10 mins."},"categories":[{"name":"python语法学习","slug":"python语法学习","count":5,"path":"api/categories/python语法学习.json"}],"tags":[{"name":"python","slug":"python","count":2,"path":"api/tags/python.json"},{"name":"Numpy","slug":"Numpy","count":2,"path":"api/tags/Numpy.json"}],"author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}}}}