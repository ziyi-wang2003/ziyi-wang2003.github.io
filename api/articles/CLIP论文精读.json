{"title":"CLIP 论文精读","uid":"64c2453c0ac7d584fda9903cba47649e","slug":"CLIP论文精读","date":"2025-07-02T08:41:41.000Z","updated":"2025-07-06T04:00:47.849Z","comments":true,"path":"api/articles/CLIP论文精读.json","keywords":null,"cover":"https://i.imgur.com/2MusSpQ.png?maxwidth=520&shape=thumb&fidelity=high","content":"<h3 id=\"CLIP论文精读\"><a href=\"#CLIP论文精读\" class=\"headerlink\" title=\"CLIP论文精读\"></a>CLIP论文精读</h3><p>CLIP通过<strong>对比学习</strong>从大量的图像-文本中学习视觉概念，实现了强大的<strong>零样本图像分类能力</strong></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>论文地址：<a href=\"https://arxiv.org/abs/2103.00020\">Learning Transferable Visual Models From Natural Language Supervision</a></p></blockquote>\n<div class=\"custom-quote tip\">\n<span class=\"custom-quote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M20.86 14.13C20 14.7 19.56 15.74 19.77 16.76C20.13 18.55 18.55 20.13 16.76 19.77C15.74 19.57 14.7 20 14.13 20.86C13.12 22.38 10.89 22.38 9.88 20.86C9.3 20 8.26 19.56 7.24 19.77C5.45 20.13 3.87 18.55 4.23 16.76C4.43 15.74 4 14.7 3.14 14.13C1.62 13.12 1.62 10.89 3.14 9.88C4 9.3 4.44 8.26 4.23 7.24C3.87 5.45 5.45 3.87 7.24 4.23C8.26 4.44 9.3 4 9.87 3.14C10.88 1.62 13.11 1.62 14.12 3.14C14.7 4 15.74 4.44 16.76 4.23C18.55 3.87 20.13 5.45 19.77 7.24C19.56 8.26 20 9.3 20.86 9.87C22.38 10.88 22.38 13.12 20.86 14.13Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M12.01 15C12.01 14.5 12.01 14.5 12.01 14.5C12.04 13.75 13 13.46 14.04 12.2C14.41 11.74 14.69 11.41 14.86 10.85C15.15 9.95 14.92 9.18 14.86 9.02C14.8 8.79 14.52 8 13.72 7.46C13.06 7.02 12.42 7 12.14 7C11.9 7 11.36 7 10.78 7.3C10.28 7.56 9.98 7.9 9.83 8.1C9.24 8.82 9.06 9.63 9 10.06\"></path>\n<path stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M11.99 18H12.01\"></path>\n</svg></span>\n<p class=\"custom-quote-title\">论文创新点</p>\n<ul>\n<li>它不使用传统计算机视觉任务中常见的、带有固定类别（如“猫”、“狗”）的标签，而是<strong>直接从互联网上收集的（图像，文本）对中学习</strong>。这种文本描述提供了比单一标签丰富得多的监督信号，涵盖了几乎无限的视觉概念。</li>\n</ul>\n<ul>\n<li><p>为了让模型理解图像与文本的关联，CLIP采用了一种名为“<strong><code>对比学习</code></strong>”的自监督方法。其核心思想是：</p>\n<ul>\n<li><strong>构建双编码器架构</strong>：CLIP包含一个图像编码器（Image Encoder，如ResNet或Vision Transformer）和一个文本编码器（Text Encoder，如Transformer）。</li>\n<li><strong>学习多模态嵌入空间</strong>：在训练过程中，模型会接收一批图像和文本。图像编码器将图像转换为特征向量，文本编码器将文本转换为特征向量。CLIP的目标是在这个共享的多模态嵌入空间中，<strong>拉近</strong>真实的“图像-文本”对的特征向量（正样本），同时<strong>推远</strong>不匹配的“图像-文本”对的特征向量（负样本）。</li>\n<li><strong>高效的代理任务</strong>：通过判断哪个文本与哪个图像配对，这个看似简单的“代理任务”却极其高效地迫使模型学习图像内容和文本语义之间的深刻联系。</li>\n</ul>\n</li>\n<li><p>CLIP最令人瞩目的成果是其强大的<strong>零样本学习能力</strong>。传统的模型在面对一个新的分类任务时，通常需要进行微调，即在新任务的标注数据上进行再训练，而经过预训练的CLIP无需任何微调即可直接应用于新的视觉分类任务。其实现方式为：</p>\n<ul>\n<li><strong>动态构建分类器</strong>：对于一个给定的分类任务（例如，区分“猫”和“狗”的图片），CLIP会将类别名称（”cat”, “dog”）转换成标准的提示语，如 “a photo of a cat” 和 “a photo of a dog”。</li>\n<li><strong>相似度匹配预测</strong>：将这些提示语通过文本编码器生成文本特征向量。当输入一张待分类的图像时，图像编码器会生成其图像特征向量。最后，模型会计算该图像特征向量与所有类别提示语的文本特征向量之间的余弦相似度，相似度最高者即为预测的类别。</li>\n</ul>\n</li>\n</ul>\n\n</div>\n<h4 id=\"1-核心方法：对比学习\"><a href=\"#1-核心方法：对比学习\" class=\"headerlink\" title=\"1. 核心方法：对比学习\"></a>1. 核心方法：对比学习</h4><p>CLIP的目标不是像传统模型那样预测一个固定的类别，而是学习一个多模态的嵌入空间，在这个空间里，匹配的图像和文本对的特征向量距离很近，而不匹配的则很远。<br>具体实现如下：</p>\n<ol>\n<li><strong>构建批次</strong>：在一个训练批次中，包含 N 个匹配的对。</li>\n<li><strong>双塔编码</strong>：<ol>\n<li><strong>图像编码器</strong> <strong>(Image Encoder)</strong>：将 N 个图像编码成 N 个图像特征向量 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.439ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"9.628ex\" height=\"1.984ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -683 4255.5 877\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D43C\" d=\"M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(473,-150) scale(0.707)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g></g><g data-mml-node=\"mo\" transform=\"translate(876.6,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(1321.2,0)\"><path data-c=\"2026\" d=\"M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(2659.9,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/></g><g data-mml-node=\"msub\" transform=\"translate(3104.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D43C\" d=\"M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(473,-150) scale(0.707)\"><path data-c=\"1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"/></g></g></g></g></svg></mjx-container></li>\n<li><strong>文本编码器</strong> <strong>(Text Encoder)</strong>：将 N 个文本编码成 N 个文本特征向量 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.439ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"10.279ex\" height=\"1.971ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -677 4543.5 871\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(617,-150) scale(0.707)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g></g><g data-mml-node=\"mo\" transform=\"translate(1020.6,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(1465.2,0)\"><path data-c=\"2026\" d=\"M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(2803.9,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/></g><g data-mml-node=\"msub\" transform=\"translate(3248.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(617,-150) scale(0.707)\"><path data-c=\"1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"/></g></g></g></g></svg></mjx-container></li>\n</ol>\n</li>\n<li><strong>计算相似度</strong>：计算所有可能的 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.666ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"6.67ex\" height=\"2.363ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -750 2947.9 1044.2\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"msub\" transform=\"translate(389,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D43C\" d=\"M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(473,-150) scale(0.707)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"/></g></g><g data-mml-node=\"mo\" transform=\"translate(1156,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/></g><g data-mml-node=\"msub\" transform=\"translate(1600.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(617,-150) scale(0.707)\"><path data-c=\"1D457\" d=\"M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z\"/></g></g><g data-mml-node=\"mo\" transform=\"translate(2558.9,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g></g></g></svg></mjx-container>对的余弦相似度，形成一个 N×N 的相似度矩阵</li>\n<li><strong>定义损失</strong>：在这个矩阵中，对角线上的 <code>N</code> 个元素是<strong>正样本</strong>（匹配的图文对），其余的 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.186ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"7.894ex\" height=\"2.072ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -833.9 3489.2 915.9\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msup\"><g data-mml-node=\"mi\"><path data-c=\"1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(975.3,363) scale(0.707)\"><path data-c=\"32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"/></g></g><g data-mml-node=\"mo\" transform=\"translate(1601,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(2601.2,0)\"><path data-c=\"1D441\" d=\"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z\"/></g></g></g></svg></mjx-container> 个元素都是<strong>负样本</strong>。CLIP的优化目标是一个对称的交叉熵损失函数，即同时在行和列的方向上进行优化：<strong>对于每个图像，模型需要从 N 个文本中找出正确的那个；反之，对于每个文本，也需要从N个图像中找出正确的那个</strong>。</li>\n</ol>\n<p><img src=\"/post/CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/file-20250702165816506.png\" alt></p>\n<h4 id=\"2-核心能力：零样本迁移\"><a href=\"#2-核心能力：零样本迁移\" class=\"headerlink\" title=\"2. 核心能力：零样本迁移\"></a>2. 核心能力：零样本迁移</h4><p>这是CLIP方法论的直接应用，也是其价值的主要体现</p>\n<ol>\n<li><strong>动态构建分类器</strong>：对于任何一个分类任务（比如对ImageNet的1000类进行分类），CLIP不需要重新训练。而是通过“提示工程”为每个类别创建一个或多个描述性文本。例如，对于类别 “dog”，可以生成文本 “A photo of a dog.”。</li>\n</ol>\n<p><img src=\"/post/CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/file-20250702165921957.png\" alt></p>\n<ol>\n<li><strong>推理与匹配</strong>：将待分类的图像输入Image Encoder得到其特征 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.667ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.063ex\" height=\"2.213ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -683 911.9 978\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D43C\" d=\"M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(473,-150) scale(0.707)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"/></g></g></g></g></svg></mjx-container>。然后，将所有类别的提示文本输入Text Encoder得到一组类别特征 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.667ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"7.384ex\" height=\"2.199ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -677 3263.6 972\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"/></g><g data-mml-node=\"TeXAtom\" transform=\"translate(617,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(550,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g></g></g><g data-mml-node=\"mo\" transform=\"translate(1409.5,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"/></g><g data-mml-node=\"msub\" transform=\"translate(1854.1,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"/></g><g data-mml-node=\"TeXAtom\" transform=\"translate(617,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(550,0)\"><path data-c=\"32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"/></g></g></g></g></g></svg></mjx-container>,… 。最后，计算 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.667ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.063ex\" height=\"2.213ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -683 911.9 978\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D43C\" d=\"M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(473,-150) scale(0.707)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"/></g></g></g></g></svg></mjx-container> 与每个类别特征 <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.667ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"2.941ex\" height=\"2.199ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -677 1299.9 972\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D447\" d=\"M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z\"/></g><g data-mml-node=\"TeXAtom\" transform=\"translate(617,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(550,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"/></g></g></g></g></g></svg></mjx-container> 的余弦相似度，相似度最高者即为预测类别。</li>\n<li><strong>提示工程的重要性</strong>：精心设计的提示语至关重要。它能解决<strong>词义模糊</strong>问题（如”boxer”是狗还是拳击手）并提升性能。论文中通过集成80个不同的提示模板，在ImageNet上的准确率提升了3.5%。这一技巧的有效性，使得CLIP的性能得到了显著增强。</li>\n</ol>\n<p><img src=\"/post/CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/file-20250702170055484.png\" alt></p>\n<h4 id=\"3-关键决策：追求最高的训练效率\"><a href=\"#3-关键决策：追求最高的训练效率\" class=\"headerlink\" title=\"3. 关键决策：追求最高的训练效率\"></a>3. 关键决策：追求最高的训练效率</h4><p>在论文中，作者强调，由于计算资源是有限的，选择一个<strong>计算效率最高</strong>的预训练方法至关重要。他们对比了三种方法：</p>\n<ul>\n<li><strong>方法一：Transformer语言模型 (预测文本)</strong>：类似VirTex，用图像作为上下文，生成描述文本。这种方法表现力强，但任务难度大，学习效率最低。</li>\n<li><strong>方法二：词袋模型</strong>：不要求生成完整句子，只要求预测文本中的单词。效率比方法一高3倍，但仍不够理想。</li>\n<li><strong>方法三：对比学习</strong>：只要求判断图文是否匹配，任务最简单。<strong>其训练效率比词袋模型还要高4倍</strong>。</li>\n</ul>\n<p><img src=\"/post/CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/file-20250702170227937.png\" alt></p>\n<p>这个实验结论是CLIP成功的关键之一：<strong>在超大规模数据下，一个更简单、更高计算效率的训练目标，能让模型在有限时间内学到更好的表征。</strong></p>\n<h4 id=\"4-模型与数据规模\"><a href=\"#4-模型与数据规模\" class=\"headerlink\" title=\"4. 模型与数据规模\"></a>4. 模型与数据规模</h4><ul>\n<li><strong>数据集</strong>：OpenAI构建了一个名为<strong>WebImageText</strong>的私有数据集，包含从互联网上收集的<strong>4亿</strong>个图文对。</li>\n<li><strong>图像编码器</strong>：论文中测试了两种架构：<ul>\n<li><strong>ResNet-D</strong>：对标准ResNet进行了一些修改，如用注意力池化层替换全局平均池化层。共测试了5个不同规模的ResNet。</li>\n<li><strong>Vision Transformer (ViT)</strong>：共测试了3个不同规模的ViT。实验发现ViT的计算效率比ResNet更高。最终性能最好的模型是 <strong>ViT-L/14</strong>，并在336x336的分辨率下进行了额外的微调。</li>\n</ul>\n</li>\n<li><strong>文本编码器</strong>：一个标准的63M参数、12层、512宽、8个注意力头的Transformer模型。</li>\n</ul>\n<h4 id=\"5-实验结果与分析\"><a href=\"#5-实验结果与分析\" class=\"headerlink\" title=\"5. 实验结果与分析\"></a>5. 实验结果与分析</h4><p>CLIP的实验部分非常详尽，覆盖了超过30个不同的数据集，主要结论如下：</p>\n<ul>\n<li><strong>与全监督模型匹敌</strong>：在ImageNet上，Zero-shot CLIP的准确率可以达到76.2%，与一个在ImageNet上经过完整监督训练的ResNet-50效果相当。</li>\n<li><strong>超强的鲁棒性</strong>：CLIP最令人印象深刻的是其在<strong>自然分布漂移</strong>上的表现。在ImageNet-V2, Rendition, Sketch, Adversarial等更具挑战性的数据集上，其性能远超监督模型。最极端的例子是在ImageNet-A上，ResNet101的准确率从76.2%骤降至2.7%，而CLIP仍能达到77.1%，展现了惊人的泛化能力。</li>\n</ul>\n<p><img src=\"/post/CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/file-20250702170557710.png\" alt></p>\n<ul>\n<li><strong>数据重叠检查</strong>：为了验证性能不是来自于训练集和测试集的重叠（数据泄露），作者进行了详尽的检查，发现重合率中位数仅为2.2%，且移除这些重叠样本后，模型性能没有显著变化，证明了其强大的泛化能力是真实有效的。</li>\n<li><strong>优秀的特征表示</strong>：即使不用于零样本分类，CLIP学习到的特征本身也极为优秀。在标准的线性探查（linear probe）评测中，CLIP的特征在性能和计算效率方面均优于当时的其他自监督方法。</li>\n</ul>\n<p><img src=\"/post/CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/file-20250702171459184.png\" alt></p>\n<p>上面图片中，左图估算了线性分类器达到零样本 CLIP 性能所需的每类标注样本数，范围从不足 1 到 184，中位数 5.4，均值 20.8。<strong>这表明zero - shot transfer 数据效率差异大，部分任务需大量标注，部分几乎无需</strong>。图 8 显示zero - shot 与线性探针性能正相关，但zero - shot 普遍低 10%-25%，仅 5 个数据集接近。<strong>这说明 CLIP zero - shot 能力与表征质量相关，但仍有提升空间，多数任务距最优有差距</strong>。</p>\n<p><img src=\"/post/CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/file-20250702171715730.png\" alt></p>\n<p>上图对比了 CLIP 模型与 EfficientNet、MoCo、ViT 等先进计算机视觉模型的线性探针性能。左图为 Kornblith 等研究的 12 个数据集平均分数，右图为 27 个更多样分布数据集的平均分数。结果显示，CLIP 模型，尤其是 ViT 架构的 CLIP-L/14@336px，在两类数据集上均表现出色，其最佳模型平均得分超过现有模型，且 Vision Transformer 比 ResNet 更高效。虚线表示微调或高分辨率评估的模型，体现了 CLIP 在表征学习上的优势与高效性。</p>\n<p><img src=\"/post/CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/0cd76ac0-de6a-11ec-8768-5ecc511d5822.mp4\" alt></p>\n","text":"CLIP是OpenAI开发的多模态模型，能理解图像和文本的关联。它通过对比学习，将图文映射到同一空间，实现出色的零样本图像分类能力。...","permalink":"/post/CLIP论文精读","photos":[],"count_time":{"symbolsCount":"3.4k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"多模态","slug":"多模态","count":5,"path":"api/tags/多模态.json"},{"name":"对比学习","slug":"对比学习","count":1,"path":"api/tags/对比学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#CLIP%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB\"><span class=\"toc-text\">CLIP论文精读</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95%EF%BC%9A%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">1. 核心方法：对比学习</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B%EF%BC%9A%E9%9B%B6%E6%A0%B7%E6%9C%AC%E8%BF%81%E7%A7%BB\"><span class=\"toc-text\">2. 核心能力：零样本迁移</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#3-%E5%85%B3%E9%94%AE%E5%86%B3%E7%AD%96%EF%BC%9A%E8%BF%BD%E6%B1%82%E6%9C%80%E9%AB%98%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%88%E7%8E%87\"><span class=\"toc-text\">3. 关键决策：追求最高的训练效率</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#4-%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%95%B0%E6%8D%AE%E8%A7%84%E6%A8%A1\"><span class=\"toc-text\">4. 模型与数据规模</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#5-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E4%B8%8E%E5%88%86%E6%9E%90\"><span class=\"toc-text\">5. 实验结果与分析</span></a></li></ol></li></ol>","author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"ViLT 论文精读","uid":"8c60d49e88f3e150291142683c6b267b","slug":"ViLT论文精读","date":"2025-07-02T09:37:29.000Z","updated":"2025-07-06T04:03:43.826Z","comments":true,"path":"api/articles/ViLT论文精读.json","keywords":null,"cover":"https://i.imgur.com/HUxHipu.png?maxwidth=520&shape=thumb&fidelity=high","text":"ViLT是一种高效的多模态模型，旨在简化视觉与语言的融合。它无需复杂的独立视觉特征提取器，直接在统一的Transformer中处理图像块和文本，显著提升了模型训练和推理的速度。...","permalink":"/post/ViLT论文精读","photos":[],"count_time":{"symbolsCount":"7.1k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"多模态","slug":"多模态","count":5,"path":"api/tags/多模态.json"},{"name":"VLP","slug":"VLP","count":2,"path":"api/tags/VLP.json"}],"author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}}},"next_post":{"title":"ViT 论文精读","uid":"290cb712ccbf87b18cb10d019f08a741","slug":"ViT论文精读","date":"2025-07-02T03:32:00.000Z","updated":"2025-07-06T01:07:51.128Z","comments":true,"path":"api/articles/ViT论文精读.json","keywords":null,"cover":"https://i.imgur.com/uQIpaQu_d.png?maxwidth=520&shape=thumb&fidelity=high","text":"ViT是谷歌提出的图像分类模型。它创新性地将自然语言处理领域的Transformer架构应用于视觉任务，通过将图像分割成块序列进行处理，挑战了卷积神经网络(CNN)的主导地位。...","permalink":"/post/ViT论文精读","photos":[],"count_time":{"symbolsCount":"22k","symbolsTime":"20 mins."},"categories":[],"tags":[{"name":"Vision-Transformer","slug":"Vision-Transformer","count":1,"path":"api/tags/Vision-Transformer.json"},{"name":"深度学习","slug":"深度学习","count":1,"path":"api/tags/深度学习.json"}],"author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}}}}