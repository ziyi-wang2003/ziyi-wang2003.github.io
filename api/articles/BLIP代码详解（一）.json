{"title":"BLIP代码详解（一）","uid":"31d49180bc69c8b21b5ea22ba59ea29a","slug":"BLIP代码详解（一）","date":"2025-07-06T09:34:34.000Z","updated":"2025-07-07T06:29:00.013Z","comments":true,"path":"api/articles/BLIP代码详解（一）.json","keywords":null,"cover":null,"content":"<h3 id=\"一、项目结构概述\"><a href=\"#一、项目结构概述\" class=\"headerlink\" title=\"一、项目结构概述\"></a>一、项目结构概述</h3><p>BLIP 是一个用于统一视觉语言理解和生成的预训练项目，其代码库结构清晰，功能模块划分明确，以下是对其主要结构的详细介绍：</p>\n<h4 id=\"1-根目录文件\"><a href=\"#1-根目录文件\" class=\"headerlink\" title=\"1. 根目录文件\"></a>1. 根目录文件</h4><ul>\n<li>配置与依赖：cog.yaml 用于定义项目的配置，requirements.txt 列出了项目所需的依赖库，通过 pip install -r requirements.txt 即可安装。</li>\n<li>训练与评估脚本：包含多个训练和评估脚本，如 train_caption.py 用于图像文本字幕任务的训练，eval_nocaps.py 用于在 NoCaps 数据集上评估微调后的模型。</li>\n<li>演示文件：demo.ipynb 是一个交互式演示的 Colab 笔记本，无需 GPU 即可运行，展示了图像字幕、视觉问答等功能。</li>\n</ul>\n<h4 id=\"2-子目录\"><a href=\"#2-子目录\" class=\"headerlink\" title=\"2. 子目录\"></a>2. 子目录</h4><ul>\n<li>configs/：存放各种任务的配置文件，如 caption_coco.yaml、retrieval_coco.yaml 等，用于设置模型参数、数据集路径等。</li>\n<li>transform/：包含randaugment的代码，用于数据增强</li>\n<li>models/：包含项目的核心模型代码，如 blip_pretrain.py、blip_nlvr.py 等，定义了不同任务的模型结构和前向传播方法。</li>\n<li>data/：用于存放数据集或与数据处理相关的代码。</li>\n</ul>\n<h3 id=\"二、models-blip-py详解\"><a href=\"#二、models-blip-py详解\" class=\"headerlink\" title=\"二、models/blip.py详解\"></a>二、models/blip.py详解</h3><p><code>blip.py</code>实现了多模态模型相关功能。定义<code>BLIP_Base</code>和<code>BLIP_Decoder</code>类，前者可提取图像、文本或多模态特征，后者用于图像描述。还包含分词器初始化、视觉编码器创建、加载预训练模型等辅助函数。 blip.py实现了多模态模型相关功能。</p>\n<h4 id=\"2-1-BLIP-Base-函数\"><a href=\"#2-1-BLIP-Base-函数\" class=\"headerlink\" title=\"2.1 BLIP_Base 函数\"></a>2.1 BLIP_Base 函数</h4><h5 id=\"功能介绍\"><a href=\"#功能介绍\" class=\"headerlink\" title=\"功能介绍\"></a>功能介绍</h5><p><code>BLIP_Base</code> 类是一个用于<strong>图像和文本特征提取</strong>的 PyTorch 模块。<code>__init__</code> 方法初始化视觉编码器、分词器和文本编码器，依据 <code>vit</code> 模型大小和图像尺寸创建视觉编码器，加载配置文件设置文本编码器。<br><code>forward</code> 方法根据 <code>mode</code> 参数执行不同操作：当 <code>mode</code> 为 <code>'image'</code> 时，通过视觉编码器提取图像特征并返回；当 <code>mode</code> 为 <code>'text'</code> 时，通过文本编码器提取文本特征并返回；当 <code>mode</code> 为 <code>'multimodal'</code> 时，结合图像和文本信息，返回多模态特征。 </p>\n<h5 id=\"具体代码\"><a href=\"#具体代码\" class=\"headerlink\" title=\"具体代码\"></a>具体代码</h5><pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 定义 BLIP_Base 类，继承自 PyTorch 的 nn.Module 类，用于构建基础的多模态特征提取模型\nclass BLIP_Base(nn.Module):\n    def __init__(self,                 \n                 # 编码器 - 解码器混合模型的配置文件路径，默认为 'configs/med_config.json'\n                 med_config = 'configs/med_config.json',  \n                 # 输入图像的大小，默认为 224\n                 image_size = 224,\n                 # 视觉变换器（Vision Transformer）的模型大小，可选 'base' 或 'large'，默认为 'base'\n                 vit = 'base',\n                 # 是否使用梯度检查点，默认为 False\n                 vit_grad_ckpt = False,\n                 # 梯度检查点的层数，默认为 0\n                 vit_ckpt_layer = 0,                 \n                 ):\n\n        # 调用父类 nn.Module 的构造函数\n        super().__init__()\n\n        # 调用 create_vit 函数创建视觉编码器，并获取视觉编码器的嵌入维度\n        # create_vit 函数根据传入的 vit 型号、图像大小、是否使用梯度检查点等参数创建视觉编码器\n        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n        # 调用 init_tokenizer 函数初始化分词器，用于将文本转换为模型可处理的输入格式\n        self.tokenizer = init_tokenizer()   \n        # 从 JSON 文件中加载 BERT 模型的配置\n        med_config = BertConfig.from_json_file(med_config)\n        # 将 BERT 模型的编码器宽度设置为视觉编码器的嵌入维度\n        med_config.encoder_width = vision_width\n        # 根据配置创建文本编码器，add_pooling_layer=False 表示不使用池化层\n        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)  \n\n    # 定义前向传播方法，根据不同的模式处理图像和文本数据\n    def forward(self, image, caption, mode):\n\n        # 确保 mode 参数为 'image'、'text' 或 'multimodal' 之一，否则抛出异常\n        assert mode in ['image', 'text', 'multimodal'], \"mode parameter must be image, text, or multimodal\"\n        # 使用分词器对输入的文本描述进行分词，并将分词结果转换为 PyTorch 张量\n        # 然后将张量移动到与图像数据相同的设备上（如 GPU 或 CPU）\n        text = self.tokenizer(caption, return_tensors=\"pt\").to(image.device) \n\n        if mode == 'image':    \n            # 当 mode 为 'image' 时，仅使用视觉编码器处理图像数据\n            # 将输入的图像传入视觉编码器，得到图像的嵌入表示\n            image_embeds = self.visual_encoder(image)             \n            # 返回图像的嵌入表示\n            return image_embeds\n\n        elif mode == 'text':\n            # 当 mode 为 'text' 时，仅使用文本编码器处理文本数据\n            # 将分词后的文本输入和注意力掩码传入文本编码器\n            # return_dict = True 表示以字典形式返回输出结果\n            # mode = 'text' 表示使用文本处理模式\n            text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n                                            return_dict = True, mode = 'text')  \n            # 返回文本编码器的最后一层隐藏状态\n            return text_output.last_hidden_state\n\n        elif mode == 'multimodal':\n            # 当 mode 为 'multimodal' 时，同时处理图像和文本数据\n            # 将输入的图像传入视觉编码器，得到图像的嵌入表示\n            image_embeds = self.visual_encoder(image)    \n            # 创建与图像嵌入表示相同形状的注意力掩码，用于在文本编码器中指示哪些位置是有效的图像信息\n            # image_embeds.size()[:-1] 表示取 image_embeds 张量除最后一个维度之外的形状。例如，如果 image_embeds 的形状是 (batch_size, num_patches, embed_dim)，那么 image_embeds.size()[:-1] 就是 `(batch_size, num_patches)`。\n            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)      \n\n            # 将文本输入的第一个 token 设置为编码器的特殊 token “[ENC]”\n            text.input_ids[:,0] = self.tokenizer.enc_token_id\n            # 将文本输入、注意力掩码、图像嵌入表示和图像注意力掩码传入文本编码器\n            # 让文本编码器能够结合图像信息进行处理\n            output = self.text_encoder(text.input_ids,\n                                       attention_mask = text.attention_mask,\n                                       encoder_hidden_states = image_embeds,\n                                       encoder_attention_mask = image_atts,      \n                                       return_dict = True,\n                                      )              \n            # 返回文本编码器的最后一层隐藏状态，即多模态特征\n            return output.last_hidden_state\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h5 id=\"运行逻辑\"><a href=\"#运行逻辑\" class=\"headerlink\" title=\"运行逻辑\"></a>运行逻辑</h5><p>当创建 <code>BLIP_Base</code> 类的实例时，会执行 <code>__init__</code> 方法进行初始化操作，具体步骤如下：</p>\n<ol>\n<li><strong>接收参数</strong>：接收几个参数，包括 <code>med_config</code>（编码器 - 解码器混合模型的配置文件路径）、<code>image_size</code>（输入图像的大小）、<code>vit</code>（视觉变换器的模型大小）、<code>vit_grad_ckpt</code>（是否使用梯度检查点）和 <code>vit_ckpt_layer</code>（梯度检查点的层数）。</li>\n<li><strong>创建视觉编码器</strong>：调用 <code>create_vit</code> 函数，根据传入的 <code>vit</code> 和 <code>image_size</code> 等参数创建视觉编码器 <code>self.visual_encoder</code>，同时获取其嵌入维度 <code>vision_width</code>。</li>\n<li><strong>初始化分词器</strong>：调用 <code>init_tokenizer</code> 函数初始化一个 BERT 分词器 <code>self.tokenizer</code>。</li>\n<li><strong>加载并配置 BERT 模型</strong>：从 JSON 文件中加载 BERT 模型的配置 <code>med_config</code>，并将其 <code>encoder_width</code> 属性设置为视觉编码器的嵌入维度。</li>\n<li><strong>创建文本编码器</strong>：使用配置好的 <code>med_config</code> 创建一个 BERT 模型 <code>self.text_encoder</code>，并设置不添加池化层。</li>\n</ol>\n<p><code>forward</code> 方法用于执行前向传播过程，接收三个参数：<code>image</code>（输入图像）、<code>caption</code>（文本描述）和 <code>mode</code>（指定处理模式），具体步骤如下：</p>\n<ol>\n<li><strong>模式检查</strong>：使用 <code>assert</code> 语句确保 <code>mode</code> 参数的值为 <code>'image'</code>、<code>'text'</code> 或 <code>'multimodal'</code> 之一，否则抛出异常。</li>\n<li><strong>文本编码</strong>：使用分词器对输入的文本描述 <code>caption</code> 进行编码，并将编码后的张量移动到与输入图像相同的设备上。</li>\n<li><strong>根据模式执行不同操作</strong>：<ul>\n<li><strong>模式为 <code>'image'</code></strong>：将输入图像传入视觉编码器 <code>self.visual_encoder</code>，得到图像的嵌入特征 <code>image_embeds</code> 并返回。</li>\n<li><strong>模式为 <code>'text'</code></strong>：将编码后的文本输入到文本编码器 <code>self.text_encoder</code> 中，同时传入注意力掩码，以字典形式返回输出。最后返回文本编码器输出的最后一层隐藏状态。</li>\n<li><strong>模式为 <code>'multimodal'</code></strong>：<ul>\n<li>首先，将输入图像传入视觉编码器，得到图像的嵌入特征 <code>image_embeds</code>，并创建一个与图像嵌入特征形状匹配的全 1 注意力掩码 <code>image_atts</code>。</li>\n<li>然后，将编码后文本的第一个标记替换为 <code>enc_token_id</code>。</li>\n<li>最后，将文本输入、注意力掩码、图像嵌入特征和图像注意力掩码一起传入文本编码器，以字典形式返回输出，并返回其最后一层隐藏状态。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p>综上所述，<code>BLIP_Base</code> 类可以根据不同的模式对输入的图像和文本进行特征提取，支持单独提取图像特征、文本特征以及融合图像和文本的多模态特征。 </p>\n<h4 id=\"2-2-BLIP-Decoder-函数\"><a href=\"#2-2-BLIP-Decoder-函数\" class=\"headerlink\" title=\"2.2 BLIP_Decoder 函数\"></a>2.2 BLIP_Decoder 函数</h4><h5 id=\"功能介绍-1\"><a href=\"#功能介绍-1\" class=\"headerlink\" title=\"功能介绍\"></a>功能介绍</h5><p>这段代码定义了 <code>BLIP_Decoder</code> 类，用于<strong>图像描述生成任务</strong>。  </p>\n<ul>\n<li><strong>初始化</strong>：在 <code>__init__</code> 方法中，接收配置文件路径、图像大小等参数，创建视觉编码器和文本解码器，初始化分词器，并记录提示信息。 </li>\n<li><strong>前向传播</strong>：<code>forward</code> 方法接收图像和描述文本，将图像编码为特征，处理文本并设置目标标签，输入文本解码器计算语言模型损失。 </li>\n<li><strong>生成描述</strong>：<code>generate</code> 方法根据图像生成描述文本，支持核采样和束搜索两种策略。根据配置生成输入序列，调用文本解码器生成输出，最后将输出解码为文本并去除提示信息。</li>\n</ul>\n<h5 id=\"具体代码-1\"><a href=\"#具体代码-1\" class=\"headerlink\" title=\"具体代码\"></a>具体代码</h5><pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">class BLIP_Decoder(nn.Module):\n    def __init__(self,                 \n                 med_config = 'configs/med_config.json',  \n                 image_size = 384,\n                 vit = 'base',\n                 vit_grad_ckpt = False,\n                 vit_ckpt_layer = 0,\n                 prompt = 'a picture of ',\n                 ):\n        \"\"\"\n        初始化 BLIP_Decoder 类。\n\n        Args:\n            med_config (str): 编码器 - 解码器混合模型的配置文件路径\n            image_size (int): 输入图像的大小\n            vit (str): 视觉变换器（Vision Transformer）的模型大小\n            vit_grad_ckpt (bool): 是否使用梯度检查点来减少内存使用\n            vit_ckpt_layer (int): 梯度检查点的层数\n            prompt (str): 用于图像描述生成的提示文本\n        \"\"\"            \n        super().__init__()\n\n        # 创建视觉编码器，并获取其嵌入维度\n        self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer)\n        # 初始化分词器\n        self.tokenizer = init_tokenizer()   \n        # 从 JSON 文件中加载 BERT 模型的配置\n        med_config = BertConfig.from_json_file(med_config)\n        # 设置编码器的宽度为视觉编码器的嵌入维度\n        med_config.encoder_width = vision_width\n        # 创建文本解码器   \n        self.text_decoder = BertLMHeadModel(config=med_config)    \n\n        # 保存提示文本\n        self.prompt = prompt\n        # 计算提示文本的长度（去除特殊标记）\n        self.prompt_length = len(self.tokenizer(self.prompt).input_ids) - 1\n\n    def forward(self, image, caption):\n        \"\"\"\n        前向传播方法，用于计算语言模型的损失。\n        Args:\n            image (torch.Tensor): 输入的图像张量\n            caption (list): 与图像对应的文本描述列表\n\n        Returns:\n            torch.Tensor: 语言模型的损失\n        \"\"\"\n        # 通过视觉编码器提取图像的嵌入特征\n        image_embeds = self.visual_encoder(image) \n        # 创建图像注意力掩码，形状与图像嵌入特征相同\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n\n        # 使用分词器对文本描述进行编码，填充到最长长度，截断到最大长度为 40，并转换为 PyTorch 张量 , 具体的tokenizer代码见下面\n        text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors=\"pt\").to(image.device) \n\n        # 注：text[:, 0] 会选择 text 中每一行的第 0 个元素，最终返回一个一维的 torch.Tensor ， text在这里形状为(batch_size, sequence_length)\n        # 这里是将每一个batch的输入文本的第一个标记设置为开始标记\n        text.input_ids[:, 0] = self.tokenizer.bos_token_id\n\n        # 创建解码器的目标标签，将填充标记替换为 -100\n        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)         \n        # 将提示部分的目标标签也设置为 -100，不参与损失计算\n        # 实现方法为将每一个batch的decoder_targets除了前prompt_length的内容保留下来，其他都变为-100\n        decoder_targets[:, :self.prompt_length] = -100\n\n        # 将输入文本、注意力掩码、图像嵌入特征等输入到文本解码器中\n        decoder_output = self.text_decoder(text.input_ids, \n                                           attention_mask = text.attention_mask, \n                                           encoder_hidden_states = image_embeds,\n                                           encoder_attention_mask = image_atts,    \n                                           labels = decoder_targets,\n                                           return_dict = True,   \n                                          )   \n        # 获取语言模型的损失\n        loss_lm = decoder_output.loss\n\n        return loss_lm\n\n    def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):\n        \"\"\"\n        根据输入图像生成文本描述。\n        Args:\n            image (torch.Tensor): 输入的图像张量\n            sample (bool): 是否使用核采样，默认为 False，即使用束搜索\n            num_beams (int): 束搜索的束数\n            max_length (int): 生成文本的最大长度\n            min_length (int): 生成文本的最小长度\n            top_p (float): 核采样的概率阈值\n            repetition_penalty (float): 重复惩罚因子\n\n        Returns:\n            list: 生成的文本描述列表\n        \"\"\"\n        # 通过视觉编码器提取图像的嵌入特征\n        image_embeds = self.visual_encoder(image)\n\n        if not sample:\n            # 如果不使用采样，将图像嵌入特征重复 num_beams 次\n            image_embeds = image_embeds.repeat_interleave(num_beams, dim=0)\n\n        # 创建图像注意力掩码，形状与图像嵌入特征相同\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n        # 构建传递给文本解码器的额外参数\n        model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\": image_atts}\n\n        # 为每个图像创建提示文本列表\n        prompt = [self.prompt] * image.size(0)\n        # 使用分词器对提示文本进行编码，并转换为 PyTorch 张量\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(image.device) \n        # 将输入提示文本的第一个标记设置为开始标记\n        input_ids[:, 0] = self.tokenizer.bos_token_id\n        # 去除提示文本的最后一个标记\n        input_ids = input_ids[:, :-1] \n\n        if sample:\n            # 核采样\n            outputs = self.text_decoder.generate(input_ids=input_ids,\n                                                  max_length=max_length,\n                                                  min_length=min_length,\n                                                  do_sample=True,\n                                                  top_p=top_p,\n                                                  num_return_sequences=1,\n                                                  eos_token_id=self.tokenizer.sep_token_id,\n                                                  pad_token_id=self.tokenizer.pad_token_id, \n                                                  repetition_penalty=1.1,                                            \n                                                  **model_kwargs)\n        else:\n            # 束搜索\n            outputs = self.text_decoder.generate(input_ids=input_ids,\n                                                  max_length=max_length,\n                                                  min_length=min_length,\n                                                  num_beams=num_beams,\n                                                  eos_token_id=self.tokenizer.sep_token_id,\n                                                  pad_token_id=self.tokenizer.pad_token_id,     \n                                                  repetition_penalty=repetition_penalty,\n                                                  **model_kwargs)            \n\n        # 用于存储生成的文本描述\n        captions = []    \n        for output in outputs:\n            # 对输出的标记序列进行解码，去除特殊标记\n            caption = self.tokenizer.decode(output, skip_special_tokens=True)    \n            # 去除提示文本，只保留生成的描述部分\n            captions.append(caption[len(self.prompt):])\n        return captions\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h5 id=\"运行逻辑-1\"><a href=\"#运行逻辑-1\" class=\"headerlink\" title=\"运行逻辑\"></a>运行逻辑</h5><p>当创建 <code>BLIP_Decoder</code> 类的实例时，会执行 <code>__init__</code> 方法进行初始化操作，具体步骤如下：</p>\n<ol>\n<li><strong>接收参数</strong>：接收几个参数，包括 <code>med_config</code>（编码器 - 解码器混合模型的配置文件路径）、<code>image_size</code>（输入图像的大小）、<code>vit</code>（视觉变换器的模型大小）、<code>vit_grad_ckpt</code>（是否使用梯度检查点）、<code>vit_ckpt_layer</code>（梯度检查点的层数）和 <code>prompt</code>（用于生成描述的提示文本）。</li>\n<li><strong>创建视觉编码器</strong>：调用 <code>create_vit</code> 函数，根据传入的 <code>vit</code> 和 <code>image_size</code> 等参数创建视觉编码器 <code>self.visual_encoder</code>，同时获取其嵌入维度 <code>vision_width</code>。</li>\n<li><strong>初始化分词器</strong>：调用 <code>init_tokenizer</code> 函数初始化一个 BERT 分词器 <code>self.tokenizer</code>。</li>\n<li><strong>加载并配置 BERT 模型</strong>：从 JSON 文件中加载 BERT 模型的配置 <code>med_config</code>，并将其 <code>encoder_width</code> 属性设置为视觉编码器的嵌入维度。</li>\n<li><strong>创建文本解码器</strong>：使用配置好的 <code>med_config</code> 创建一个 <code>BertLMHeadModel</code> 作为文本解码器 <code>self.text_decoder</code>。</li>\n<li><strong>保存提示文本及长度</strong>：保存传入的提示文本 <code>self.prompt</code>，并计算提示文本的长度（去除特殊标记），保存为 <code>self.prompt_length</code>。</li>\n</ol>\n<p><code>forward</code> 方法用于计算语言模型的损失，接收两个参数：<code>image</code>（输入图像）和 <code>caption</code>（图像对应的文本描述），具体步骤如下：</p>\n<ol>\n<li><strong>提取图像特征</strong>：将输入图像传入视觉编码器 <code>self.visual_encoder</code>，得到图像的嵌入特征 <code>image_embeds</code>，并创建一个与图像嵌入特征形状匹配的全 1 注意力掩码 <code>image_atts</code>。</li>\n<li><strong>对文本描述进行编码</strong>：使用分词器对输入的文本描述 <code>caption</code> 进行编码，填充到最长长度，截断到最大长度为 40，并将编码后的张量移动到与输入图像相同的设备上。</li>\n<li><strong>设置起始标记</strong>：将编码后文本的第一个标记替换为开始标记 <code>bos_token_id</code>。</li>\n<li><strong>创建解码器的目标标签</strong>：将编码后文本中的填充标记替换为 -100，并将提示部分的目标标签也设置为 -100，以避免这些部分参与损失计算。</li>\n<li><strong>计算损失</strong>：将文本输入、注意力掩码、图像嵌入特征和图像注意力掩码一起传入文本解码器，同时传入目标标签，以字典形式返回输出，并获取语言模型的损失 <code>loss_lm</code>。</li>\n<li><strong>返回损失</strong>：返回计算得到的语言模型损失 <code>loss_lm</code>。</li>\n</ol>\n<p><code>generate</code> 方法用于根据输入图像生成文本描述，接收几个参数，包括 <code>image</code>（输入图像）、<code>sample</code>（是否使用核采样）、<code>num_beams</code>（束搜索的束数）、<code>max_length</code>（生成文本的最大长度）、<code>min_length</code>（生成文本的最小长度）、<code>top_p</code>（核采样的概率阈值）和 <code>repetition_penalty</code>（重复惩罚因子），具体步骤如下：</p>\n<ol>\n<li><strong>提取图像特征</strong>：将输入图像传入视觉编码器 <code>self.visual_encoder</code>，得到图像的嵌入特征 <code>image_embeds</code>。</li>\n<li><strong>处理束搜索</strong>：如果不使用采样（即使用束搜索），将图像嵌入特征重复 <code>num_beams</code> 次。</li>\n<li><strong>创建图像注意力掩码</strong>：创建一个与图像嵌入特征形状匹配的全 1 注意力掩码 <code>image_atts</code>，并构建传递给文本解码器的额外参数 <code>model_kwargs</code>。</li>\n<li><strong>构建输入提示</strong>：为每个图像创建提示文本列表，并使用分词器对提示文本进行编码，将编码后的第一个标记替换为开始标记 <code>bos_token_id</code>，并去除提示文本的最后一个标记。</li>\n<li><strong>生成文本</strong>：根据 <code>sample</code> 参数的值，选择使用核采样或束搜索进行文本生成。</li>\n<li><strong>解码并处理生成结果</strong>：对生成的标记序列进行解码，去除特殊标记，并去除提示文本，只保留生成的描述部分，将其存储在列表 <code>captions</code> 中。</li>\n<li><strong>返回生成结果</strong>：返回存储生成描述的列表 <code>captions</code>。</li>\n</ol>\n<h4 id=\"2-3-blip-decoder、blip-feature-extractor-函数\"><a href=\"#2-3-blip-decoder、blip-feature-extractor-函数\" class=\"headerlink\" title=\"2.3 blip_decoder、blip_feature_extractor 函数\"></a>2.3 blip_decoder、blip_feature_extractor 函数</h4><h5 id=\"blip-decoder\"><a href=\"#blip-decoder\" class=\"headerlink\" title=\"blip_decoder\"></a><code>blip_decoder</code></h5><p>这个函数的主要功能是创建一个 <code>BLIP_Decoder</code> 模型实例。<code>BLIP_Decoder</code> 模型通常用于图像描述生成任务，它结合了视觉编码器和文本解码器，能够根据输入的图像生成相应的文本描述。如果 <code>pretrained</code> 参数不为空，函数会尝试从指定的路径或 URL 加载预训练模型的权重，并确保所有的模型参数都成功加载。最后返回创建好的模型实例。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 定义一个函数 blip_decoder，用于创建并返回一个 BLIP_Decoder 模型实例\n# pretrained 参数用于指定预训练模型的路径或 URL，默认为空字符串，表示不使用预训练模型\n# **kwargs 是一个可变参数，用于接收其他传递给 BLIP_Decoder 类初始化方法的参数\ndef blip_decoder(pretrained='',**kwargs):\n    # 创建一个 BLIP_Decoder 类的实例，将可变参数传递给该类的初始化方法\n    model = BLIP_Decoder(**kwargs)\n    # 检查 pretrained 参数是否不为空，如果不为空，则表示需要加载预训练模型\n    if pretrained:\n        # 调用 load_checkpoint 函数，传入模型实例和预训练模型的路径或 URL\n        # 该函数会返回加载预训练模型后的模型实例和一个包含加载信息的对象 msg\n        model,msg = load_checkpoint(model,pretrained)\n        # 使用 assert 语句检查加载信息对象 msg 中的 missing_keys 列表长度是否为 0\n        # 如果不为 0，说明有模型参数没有成功加载，会抛出 AssertionError 异常\n        assert(len(msg.missing_keys)==0)\n    # 返回创建好的模型实例\n    return model\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h5 id=\"blip-feature-extractor\"><a href=\"#blip-feature-extractor\" class=\"headerlink\" title=\"blip_feature_extractor\"></a><code>blip_feature_extractor</code></h5><p>这个函数的主要功能是创建一个 <code>BLIP_Base</code> 模型实例。<code>BLIP_Base</code> 模型主要用于图像和文本的特征提取，它可以分别处理图像和文本，也可以处理多模态的输入。同样，如果 <code>pretrained</code> 参数不为空，函数会尝试从指定的路径或 URL 加载预训练模型的权重，并确保所有的模型参数都成功加载。最后返回创建好的模型实例。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 定义一个函数 blip_feature_extractor，用于创建并返回一个 BLIP_Base 模型实例\n# pretrained 参数用于指定预训练模型的路径或 URL，默认为空字符串，表示不使用预训练模型\n# **kwargs 是一个可变参数，用于接收其他传递给 BLIP_Base 类初始化方法的参数\ndef blip_feature_extractor(pretrained='',**kwargs):\n    # 创建一个 BLIP_Base 类的实例，将可变参数传递给该类的初始化方法\n    model = BLIP_Base(**kwargs)\n    # 检查 pretrained 参数是否不为空，如果不为空，则表示需要加载预训练模型\n    if pretrained:\n        # 调用 load_checkpoint 函数，传入模型实例和预训练模型的路径或 URL\n        # 该函数会返回加载预训练模型后的模型实例和一个包含加载信息的对象 msg\n        model,msg = load_checkpoint(model,pretrained)\n        # 使用 assert 语句检查加载信息对象 msg 中的 missing_keys 列表长度是否为 0\n        # 如果不为 0，说明有模型参数没有成功加载，会抛出 AssertionError 异常\n        assert(len(msg.missing_keys)==0)\n    # 返回创建好的模型实例\n    return model\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h4 id=\"2-4-init-tokenizer、create-vit-函数\"><a href=\"#2-4-init-tokenizer、create-vit-函数\" class=\"headerlink\" title=\"2.4 init_tokenizer、create_vit 函数\"></a>2.4 init_tokenizer、create_vit 函数</h4><h5 id=\"init-tokenizer\"><a href=\"#init-tokenizer\" class=\"headerlink\" title=\"init_tokenizer\"></a><code>init_tokenizer</code></h5><p>这个函数的主要功能是初始化一个基于 <code>bert-base-uncased</code> 的 BERT 分词器，并<strong>为其添加特殊标记</strong>。具体来说，它添加了一个开始标记 <code>[DEC]</code> 和一个额外的特殊标记 <code>[ENC]</code>，并将 <code>[ENC]</code> 标记的 ID 存储在分词器的 <code>enc_token_id</code> 属性中。这样做的目的是为了在后续的文本处理中能够正确识别和处理这些特殊标记，例如在图像 - 文本任务中，<code>[ENC]</code> 标记可能用于指示文本的编码开始位置，<code>[DEC]</code> 标记可能用于指示文本的解码开始位置。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 定义一个函数 init_tokenizer，用于初始化一个 BERT 分词器并添加特殊标记\ndef init_tokenizer():\n    # 从预训练的 'bert-base-uncased' 模型加载 BERT 分词器\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    # 向分词器添加一个开始标记（bos_token），标记为 '[DEC]'\n    tokenizer.add_special_tokens({'bos_token':'[DEC]'})\n    # 向分词器添加额外的特殊标记，这里添加了 '[ENC]'\n    tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})\n    # 获取 '[ENC]' 标记的 ID 并存储在分词器的 enc_token_id 属性中\n    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]\n    # 返回初始化好的分词器\n    return tokenizer\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h5 id=\"create-vit\"><a href=\"#create-vit\" class=\"headerlink\" title=\"create_vit\"></a><code>create_vit</code></h5><p>这个函数的主要功能是根据输入的参数创建一个 Vision Transformer 模型。它接收几个参数，包括 <code>vit</code>（指定模型大小）、<code>image_size</code>（指定输入图像的大小）、<code>use_grad_checkpointing</code>（是否使用梯度检查点）、<code>ckpt_layer</code>（梯度检查点的层数）和 <code>drop_path_rate</code>（随机深度的比率）。根据 <code>vit</code> 参数的值，函数会创建一个基础版本或大型版本的视觉变换器模型。基础版本的模型具有 12 层、12 个注意力头和 768 的嵌入维度，而大型版本的模型具有 24 层、16 个注意力头和 1024 的嵌入维度。最后，函数返回创建好的视觉编码器模型和其嵌入维度，以便后续在多模态模型中使用。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 定义一个函数 create_vit，用于创建视觉变换器（Vision Transformer）模型\n# vit 参数指定视觉变换器的模型大小，可选值为 'base' 或 'large'\n# image_size 参数指定输入图像的大小\n# use_grad_checkpointing 参数指定是否使用梯度检查点，默认为 False\n# ckpt_layer 参数指定梯度检查点的层数，默认为 0\n# drop_path_rate 参数指定随机深度（drop path）的比率，默认为 0\ndef create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):\n    # 使用 assert 语句确保 vit 参数的值为 'base' 或 'large'，否则抛出异常\n    assert vit in ['base', 'large'], \"vit parameter must be base or large\"\n    # 如果 vit 参数为 'base'\n    if vit=='base':\n        # 设置视觉编码器的嵌入维度为 768\n        vision_width = 768\n        # 创建一个基础版本的视觉变换器模型\n        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, \n                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n                                           drop_path_rate=0 or drop_path_rate\n                                          )\n    # 如果 vit 参数为 'large'\n    elif vit=='large':\n        # 设置视觉编码器的嵌入维度为 1024\n        vision_width = 1024\n        # 创建一个大型版本的视觉变换器模型\n        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, \n                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n                                           drop_path_rate=0.1 or drop_path_rate\n                                          )\n    # 返回创建好的视觉编码器模型和其嵌入维度\n    return visual_encoder, vision_width\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h4 id=\"2-5-is-url、load-checkpoint-函数\"><a href=\"#2-5-is-url、load-checkpoint-函数\" class=\"headerlink\" title=\"2.5 is_url、load_checkpoint 函数\"></a>2.5 is_url、load_checkpoint 函数</h4><h5 id=\"is-url\"><a href=\"#is-url\" class=\"headerlink\" title=\"is_url\"></a><code>is_url</code></h5><p>函数用于判断一个字符串是一个有效的 URL 还是一个本地文件名。通过 <code>urlparse</code> 函数解析字符串，检查其协议部分是否为 <code>http</code> 或 <code>https</code>，如果是则返回 <code>True</code>，否则返回 <code>False</code>。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 定义一个函数 is_url，用于判断传入的字符串是一个有效的 URL 还是一个本地文件名\n# url_or_filename: 待判断的字符串，可以是 URL 或本地文件路径\ndef is_url(url_or_filename):\n    # 使用 urlparse 函数解析传入的字符串\n    parsed = urlparse(url_or_filename)\n    # 检查解析结果的 scheme（协议）是否为 \"http\" 或 \"https\"\n    # 如果是，则返回 True，表示这是一个 URL；否则返回 False，表示这可能是一个本地文件名\n    return parsed.scheme in (\"http\", \"https\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h5 id=\"load-checkpoint\"><a href=\"#load-checkpoint\" class=\"headerlink\" title=\"load_checkpoint\"></a><code>load_checkpoint</code></h5><p>用于从指定的 URL 或本地文件路径加载预训练模型的权重。具体步骤如下：</p>\n<ul>\n<li>首先调用 <code>is_url</code> 函数判断输入是 URL 还是本地文件，根据不同情况下载或直接加载模型的检查点数据。</li>\n<li>从检查点数据中提取模型的状态字典。</li>\n<li>对视觉编码器的位置嵌入进行插值处理，以适应不同的输入图像大小。</li>\n<li>遍历模型的状态字典，删除形状不匹配的键值对。</li>\n<li>使用 <code>load_state_dict</code> 函数将处理后的状态字典加载到模型中，允许部分加载。</li>\n<li>最后返回加载了权重的模型实例和加载信息。</li>\n</ul>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 定义一个函数 load_checkpoint，用于从指定的 URL 或本地文件路径加载预训练模型的权重\n# model: 要加载权重的模型实例\n# url_or_filename: 预训练模型权重的 URL 或本地文件路径\ndef load_checkpoint(model,url_or_filename):\n    # 调用 is_url 函数判断 url_or_filename 是否为 URL\n    if is_url(url_or_filename):\n        # 如果是 URL，使用 download_cached_file 函数下载文件并缓存\n        # check_hash=False 表示不检查文件的哈希值\n        # progress=True 表示显示下载进度\n        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n        # 使用 torch.load 函数从缓存文件中加载模型的检查点数据\n        # map_location='cpu' 表示将数据加载到 CPU 上\n        checkpoint = torch.load(cached_file, map_location='cpu') \n    # 如果不是 URL，检查它是否是一个本地文件\n    elif os.path.isfile(url_or_filename):        \n        # 如果是本地文件，直接使用 torch.load 函数从文件中加载模型的检查点数据\n        # map_location='cpu' 表示将数据加载到 CPU 上\n        checkpoint = torch.load(url_or_filename, map_location='cpu') \n    # 如果既不是 URL 也不是本地文件，则抛出运行时错误\n    else:\n        raise RuntimeError('checkpoint url or path is invalid')\n\n    # 从检查点数据中提取模型的状态字典\n    state_dict = checkpoint['model']\n\n    # 对视觉编码器的位置嵌入进行插值处理\n    # interpolate_pos_embed 函数用于调整位置嵌入的大小以适应不同的输入图像大小\n    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) \n    # 检查模型的状态字典中是否存在 'visual_encoder_m.pos_embed' 键\n    if 'visual_encoder_m.pos_embed' in model.state_dict().keys():\n        # 如果存在，则对该位置嵌入也进行插值处理\n        state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],\n                                                                         model.visual_encoder_m)    \n    # 遍历模型的状态字典中的所有键\n    for key in model.state_dict().keys():\n        # 检查状态字典中是否存在相同的键\n        if key in state_dict.keys():\n            # 如果存在，检查该键对应的值的形状是否与模型中的形状一致\n            if state_dict[key].shape!=model.state_dict()[key].shape:\n                # 如果形状不一致，则从状态字典中删除该键值对\n                del state_dict[key]\n\n    # 使用 load_state_dict 函数将处理后的状态字典加载到模型中\n    # strict=False 表示允许部分加载，即不要求状态字典中的所有键都与模型中的键完全匹配\n    msg = model.load_state_dict(state_dict,strict=False)\n    # 打印加载检查点的信息\n    print('load checkpoint from %s'%url_or_filename)  \n    # 返回加载了权重的模型实例和加载信息\n    return model,msg\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n","feature":false,"text":"详细讲解 models/blip.py 代码...","permalink":"/post/BLIP代码详解（一）","photos":[],"count_time":{"symbolsCount":"21k","symbolsTime":"19 mins."},"categories":[{"name":"代码详解","slug":"代码详解","count":1,"path":"api/categories/代码详解.json"}],"tags":[{"name":"BLIP","slug":"BLIP","count":2,"path":"api/tags/BLIP.json"},{"name":"代码详解","slug":"代码详解","count":1,"path":"api/tags/代码详解.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">一、项目结构概述</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-%E6%A0%B9%E7%9B%AE%E5%BD%95%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">1. 根目录文件</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-%E5%AD%90%E7%9B%AE%E5%BD%95\"><span class=\"toc-text\">2. 子目录</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BA%8C%E3%80%81models-blip-py%E8%AF%A6%E8%A7%A3\"><span class=\"toc-text\">二、models&#x2F;blip.py详解</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-1-BLIP-Base-%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">2.1 BLIP_Base 函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D\"><span class=\"toc-text\">功能介绍</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E5%85%B7%E4%BD%93%E4%BB%A3%E7%A0%81\"><span class=\"toc-text\">具体代码</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E8%BF%90%E8%A1%8C%E9%80%BB%E8%BE%91\"><span class=\"toc-text\">运行逻辑</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-2-BLIP-Decoder-%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">2.2 BLIP_Decoder 函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D-1\"><span class=\"toc-text\">功能介绍</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E5%85%B7%E4%BD%93%E4%BB%A3%E7%A0%81-1\"><span class=\"toc-text\">具体代码</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E8%BF%90%E8%A1%8C%E9%80%BB%E8%BE%91-1\"><span class=\"toc-text\">运行逻辑</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-3-blip-decoder%E3%80%81blip-feature-extractor-%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">2.3 blip_decoder、blip_feature_extractor 函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#blip-decoder\"><span class=\"toc-text\">blip_decoder</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#blip-feature-extractor\"><span class=\"toc-text\">blip_feature_extractor</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-4-init-tokenizer%E3%80%81create-vit-%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">2.4 init_tokenizer、create_vit 函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#init-tokenizer\"><span class=\"toc-text\">init_tokenizer</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#create-vit\"><span class=\"toc-text\">create_vit</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-5-is-url%E3%80%81load-checkpoint-%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">2.5 is_url、load_checkpoint 函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#is-url\"><span class=\"toc-text\">is_url</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#load-checkpoint\"><span class=\"toc-text\">load_checkpoint</span></a></li></ol></li></ol></li></ol>","author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"NumPy 学习笔记：第一阶段 - 入门基础","uid":"ba03d7c748f55c669d509a2713c39e98","slug":"NumPy-学习笔记：第一阶段-入门基础","date":"2025-07-07T08:45:52.000Z","updated":"2025-07-07T09:19:42.298Z","comments":true,"path":"api/articles/NumPy-学习笔记：第一阶段-入门基础.json","keywords":null,"cover":null,"text":"这个阶段的目标是理解 NumPy 的核心概念，并掌握其最基本的操作...","permalink":"/post/NumPy-学习笔记：第一阶段-入门基础","photos":[],"count_time":{"symbolsCount":"5.8k","symbolsTime":"5 mins."},"categories":[{"name":"python语法学习","slug":"python语法学习","count":5,"path":"api/categories/python语法学习.json"}],"tags":[{"name":"python","slug":"python","count":2,"path":"api/tags/python.json"},{"name":"Numpy","slug":"Numpy","count":2,"path":"api/tags/Numpy.json"}],"author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}}},"next_post":{"title":"ViLT 论文精读","uid":"8c60d49e88f3e150291142683c6b267b","slug":"ViLT论文精读","date":"2025-07-02T09:37:29.000Z","updated":"2025-07-06T10:27:59.134Z","comments":true,"path":"api/articles/ViLT论文精读.json","keywords":null,"cover":"https://i.imgur.com/HUxHipu.png?maxwidth=520&shape=thumb&fidelity=high","text":"ViLT是一种高效的多模态模型，旨在简化视觉与语言的融合。它无需复杂的独立视觉特征提取器，直接在统一的Transformer中处理图像块和文本，显著提升了模型训练和推理的速度。...","permalink":"/post/ViLT论文精读","photos":[],"count_time":{"symbolsCount":"7.1k","symbolsTime":"6 mins."},"categories":[{"name":"论文精读","slug":"论文精读","count":6,"path":"api/categories/论文精读.json"}],"tags":[{"name":"多模态","slug":"多模态","count":5,"path":"api/tags/多模态.json"},{"name":"VLP","slug":"VLP","count":2,"path":"api/tags/VLP.json"}],"author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}}}}