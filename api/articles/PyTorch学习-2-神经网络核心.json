{"title":"PyTorch学习-2-神经网络核心","uid":"d970942a0c3301df824e3c1c77149df6","slug":"PyTorch学习-2-神经网络核心","date":"2025-07-08T01:46:23.000Z","updated":"2025-07-08T06:05:08.835Z","comments":true,"path":"api/articles/PyTorch学习-2-神经网络核心.json","keywords":null,"cover":null,"content":"<h2 id=\"PyTorch-学习笔记-阶段二：神经网络的核心\"><a href=\"#PyTorch-学习笔记-阶段二：神经网络的核心\" class=\"headerlink\" title=\"PyTorch 学习笔记 - 阶段二：神经网络的核心\"></a>PyTorch 学习笔记 - 阶段二：神经网络的核心</h2><p>在前面的章节中，我们已经熟悉了 PyTorch 的基本数据结构——张量（Tensor），以及如何在它们之上执行操作。现在，是时候将这些基础知识组合起来，构建、训练和评估一个真正能学习的“大脑”——神经网络。</p>\n<p>本章的目标是让你对使用 PyTorch 构建一个完整神经网络的流程了如指掌。我们将像玩乐高积木一样，一块一块地搭建起复杂的模型。我们将深入探索 <code>torch.nn</code> 模块，它是我们所有模型的“积木盒”；我们还将掌握 <code>torch.optim</code>，它是驱动模型学习的“引擎”。学完本章，你将能自信地将 <code>nn.Module</code>、损失函数和优化器这“三驾马车”协同起来，解决实际问题。</p>\n<hr>\n<h3 id=\"模块-4：torch-nn-神经网络的“乐高”\"><a href=\"#模块-4：torch-nn-神经网络的“乐高”\" class=\"headerlink\" title=\"模块 4：torch.nn - 神经网络的“乐高”\"></a><strong>模块 4：<code>torch.nn</code> - 神经网络的“乐高”</strong></h3><p><code>torch.nn</code> 是 PyTorch 专门为神经网络设计的命名空间。如果你把整个 PyTorch 库想象成一个庞大的工具箱，那么 <code>torch.nn</code> 就是其中专门存放预制零件的区域，里面有螺丝（权重）、横梁（层）、甚至预组装好的模块，你只需要按照蓝图将它们组合起来。</p>\n<h4 id=\"4-1-nn-Module-所有模型的基类\"><a href=\"#4-1-nn-Module-所有模型的基类\" class=\"headerlink\" title=\"4.1 nn.Module - 所有模型的基类\"></a><strong>4.1 <code>nn.Module</code> - 所有模型的基类</strong></h4><p>在 PyTorch 的世界里，所有神经网络模型，无论简单还是复杂，都应该继承自 <code>nn.Module</code>。</p>\n<p><strong>核心理念：一种规范，而非仅仅一个类</strong></p>\n<p>将 <code>nn.Module</code> 理解为一个简单的 Python 类是远远不够的。它是一种精心设计的 <strong>规范</strong> 和 <strong>容器</strong>。当你遵循这个规范构建模型时，<code>nn.Module</code> 会在幕后为你处理大量繁琐的工作，其中最核心的一项就是 <strong>自动追踪</strong>。</p>\n<p>什么是自动追踪？当你将一个 <code>nn.Module</code> 的子模块（比如一个卷积层 <code>nn.Conv2d</code>）或者一个可学习的参数（<code>nn.Parameter</code>）定义为你模型的属性时，<code>nn.Module</code> 会自动将它们“注册”到内部的列表中。这意味着你无需手动维护一个包含模型所有权重和偏置的列表，<code>nn.Module</code> 会帮你打理好一切。这为后续的参数访问、优化、保存和加载提供了巨大的便利。</p>\n<p>让我们通过两个核心方法来解构 <code>nn.Module</code> 的工作方式：<code>__init__(self)</code> 和 <code>forward(self, x)</code>。</p>\n<p><strong>1. <code>__init__(self)</code> 方法详解</strong></p>\n<ul>\n<li><strong>职责</strong>：定义和初始化模型中所有需要用到的、<strong>带有可学习参数的组件</strong>（我们称之为“层”，Layers）。</li>\n<li><strong>操作</strong>：在这个方法中，你会实例化所有你需要的层，并将它们赋值给类的属性（例如 <code>self.conv1</code>）。这些层本身就是 <code>nn.Module</code> 的子类实例。这就像在开始拼乐高之前，先把所有需要的积木块从盒子中拿出来，摆在面前。</li>\n</ul>\n<p><strong>2. <code>forward(self, x)</code> 方法详解</strong></p>\n<ul>\n<li><strong>职责</strong>：定义数据在前向传播过程中的“流动路径”。它接收输入数据 <code>x</code>，然后明确地描述 <code>x</code> 应该如何依次通过 <code>__init__</code> 中定义的各个层，最终得到模型的输出。</li>\n<li><strong>操作</strong>：在这里，你将 <code>__init__</code> 中定义的层当作函数来调用，将数据传递给它们。例如 <code>x = self.conv1(x)</code>。这就像是乐高的拼装说明书，告诉你第一步用哪块积木，第二步用哪块，如何将它们连接起来。</li>\n</ul>\n<p><strong>代码示例：构建一个简单的网络</strong></p>\n<p>让我们构建一个非常简单的网络，它包含一个线性层和一个 ReLU 激活函数，来直观地理解 <code>__init__</code> 和 <code>forward</code> 的分工。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# 1. 定义一个继承自 nn.Module 的类\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        # 必须首先调用父类的 __init__ 方法\n        super(SimpleNet, self).__init__()\n\n        # --- “采购”阶段 ---\n        # 在这里定义我们需要的层。这些层包含了可学习的参数。\n        # self.layer1 是一个 nn.Linear 实例，它接受 10 个输入特征，输出 5 个特征。\n        # PyTorch 会自动追踪 self.layer1 的权重和偏置。\n        self.layer1 = nn.Linear(in_features=10, out_features=5)\n        self.layer2 = nn.Linear(in_features=5, out_features=2)\n\n    def forward(self, x):\n        # --- “拼装”阶段 ---\n        # 定义数据流动的路径。\n        # x 的初始形状: [batch_size, 10]\n\n        # 1. 通过第一个线性层\n        x = self.layer1(x)\n        # x 的形状现在是: [batch_size, 5]\n\n        # 2. 应用 ReLU 激活函数\n        # 注意这里我们使用了 F.relu，这是一个函数式调用。后面会详细解释。\n        x = F.relu(x)\n\n        # 3. 通过第二个线性层\n        x = self.layer2(x)\n        # x 的形状现在是: [batch_size, 2]\n\n        return x\n\n# 实例化我们的模型\nmodel = SimpleNet()\n\n# 打印模型结构，可以看到我们定义的层\nprint(model)\n\n# 创建一个假的输入数据来测试前向传播\n# 假设我们有一个批次，包含 3 个样本，每个样本有 10 个特征\ndummy_input = torch.randn(3, 10)\noutput = model(dummy_input) # 直接调用 model(input) 就会执行 forward 方法\n\nprint(\"\\n输入尺寸:\", dummy_input.shape)\nprint(\"输出尺寸:\", output.shape)\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>输出</strong>：</p>\n<pre><code>SimpleNet(\n  (layer1): Linear(in_features=10, out_features=5, bias=True)\n  (layer2): Linear(in_features=5, out_features=2, bias=True)\n)\n\n输入尺寸: torch.Size([3, 10])\n输出尺寸: torch.Size([3, 2])\n</code></pre><p><strong><code>torch.nn</code> 与 <code>torch.nn.functional</code> 的关系</strong></p>\n<p>你可能已经注意到，在 <code>forward</code> 方法中我们使用了 <code>F.relu</code> 而不是 <code>nn.ReLU</code>。这是一个重要的区别。</p>\n<ul>\n<li><strong><code>torch.nn</code> 模块（如 <code>nn.Linear</code>, <code>nn.Conv2d</code>, <code>nn.ReLU</code>）</strong>：通常是<strong>有状态的</strong>。它们是 <code>nn.Module</code> 的子类，内部可以包含可学习的参数（如 <code>nn.Linear</code> 的权重）或持久化的状态（如 <code>nn.BatchNorm</code> 的均值和方差）。因此，<strong>它们必须在 <code>__init__</code> 中实例化并作为模型的属性</strong>。</li>\n<li><strong><code>torch.nn.functional</code>（通常简写为 <code>F</code>）</strong>：通常是<strong>无状态的</strong>。它们是纯粹的函数式操作，不包含任何可学习的参数。例如，<code>relu</code>、<code>max_pool2d</code>、<code>sigmoid</code> 等操作，它们只是对输入张量进行一次计算，没有自己的权重需要维护。</li>\n</ul>\n<p><strong>使用场景与最佳实践：</strong></p>\n<ul>\n<li><strong>对于有可学习参数的层</strong>（如卷积层、线性层），<strong>必须</strong>使用 <code>nn.Module</code> 的形式，并在 <code>__init__</code> 中定义。</li>\n<li><strong>对于没有可学习参数的操作</strong>（如激活函数、池化操作），你可以自由选择：<ul>\n<li><strong>函数式 <code>F.relu</code></strong>：更简洁，通常推荐在 <code>forward</code> 方法中直接使用。</li>\n<li><strong>模块式 <code>nn.ReLU()</code></strong>：需要在 <code>__init__</code> 中定义 <code>self.relu = nn.ReLU()</code>，然后在 <code>forward</code> 中调用 <code>x = self.relu(x)</code>。这样做的好处是，当使用 <code>print(model)</code> 时，这个激活层会明确地显示在网络结构中，有时能让结构更清晰。</li>\n</ul>\n</li>\n</ul>\n<p>在实践中，大多数人倾向于对激活函数和池化使用函数式接口，因为它减少了 <code>__init__</code> 中的代码量。</p>\n<p><strong><code>model.parameters()</code> 和 <code>model.state_dict()</code></strong></p>\n<p><code>nn.Module</code> 的自动追踪能力体现在这两个极其有用的方法上。</p>\n<ul>\n<li><p><strong><code>model.parameters()</code></strong>:</p>\n<ul>\n<li><strong>作用</strong>：返回一个包含模型所有<strong>可学习参数</strong>（<code>requires_grad=True</code> 的张量，主要是权重和偏置）的<strong>迭代器</strong>。</li>\n<li><strong>用途</strong>：这是连接模型和优化器的桥梁。在训练时，我们会将这个迭代器传递给优化器（如 <code>optim.SGD</code>），告诉它：“这些就是你需要计算梯度并更新的参数。”</li>\n</ul>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 接着上面的 SimpleNet 例子\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nprint(\"\\n模型的可学习参数:\")\nfor name, param in model.named_parameters(): # named_parameters() 更常用，因为它同时返回参数名\n    if param.requires_grad:\n        print(f\"名称: {name}, 尺寸: {param.size()}\")\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ul>\n<p><strong>输出：</strong></p>\n<pre><code>模型的可学习参数:\n名称: layer1.weight, 尺寸: torch.Size([5, 10])\n名称: layer1.bias, 尺寸: torch.Size([5])\n名称: layer2.weight, 尺寸: torch.Size([2, 5])\n名称: layer2.bias, 尺寸: torch.Size([2])\n</code></pre><ul>\n<li><p><strong><code>model.state_dict()</code></strong>:</p>\n<ul>\n<li><strong>作用</strong>：返回一个 Python <strong>字典</strong>（<code>OrderedDict</code>），它将模型的每个层映射到其参数张量。它包含了模型所有的<strong>状态</strong>，不仅包括可学习参数，还包括一些非可学习的缓冲区（比如 <code>BatchNorm</code> 层的 <code>running_mean</code>）。</li>\n<li><strong>用途</strong>：这是<strong>保存和加载模型权重</strong>的标准方式。你可以轻松地将这个字典保存到磁盘，并在之后加载它来恢复一个已经训练好的模型的状态。</li>\n</ul>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 获取模型的状态字典\nstate_dict = model.state_dict()\nprint(\"\\n模型的状态字典 (部分键):\")\nfor key in state_dict.keys():\n    print(key)\n\n# 保存模型权重\ntorch.save(state_dict, 'simple_net_weights.pth')\n\n# 加载模型权重\n# 1. 首先需要创建一个相同结构的模型实例\nnew_model = SimpleNet()\n# 2. 加载状态字典\nnew_model.load_state_dict(torch.load('simple_net_weights.pth'))\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>注意</strong>：<code>state_dict</code> 只保存参数，不保存模型结构。加载时，你必须先有一个与保存时结构完全相同的模型实例。</p>\n</li>\n</ul>\n<p><strong>输出：</strong></p>\n<pre><code>模型的状态字典 (部分键):\nlayer1.weight\nlayer1.bias\nlayer2.weight\nlayer2.bias\n&lt;All keys matched successfully&gt;\n</code></pre><h4 id=\"4-2-常用层-Layers-分类与理解\"><a href=\"#4-2-常用层-Layers-分类与理解\" class=\"headerlink\" title=\"4.2 常用层 (Layers) - 分类与理解\"></a><strong>4.2 常用层 (Layers) - 分类与理解</strong></h4><p>现在我们来认识一下 <code>torch.nn</code> 积木盒里最常用的一些积木。</p>\n<p><strong>1. 卷积层 (Convolutional Layers)</strong></p>\n<ul>\n<li><p><strong><code>nn.Conv2d</code></strong>: 计算机视觉任务的基石。它通过在输入图像（或特征图）上滑动一个小的卷积核（filter）来提取局部特征。</p>\n<ul>\n<li><strong>核心参数</strong>:<ul>\n<li><code>in_channels</code> (int): 输入特征图的通道数。对于第一层，这通常是图像的通道数（RGB为3，灰度为1）。</li>\n<li><code>out_channels</code> (int): 输出特征图的通道数。这决定了该层要学习多少种不同的特征。</li>\n<li><code>kernel_size</code> (int or tuple): 卷积核的大小。<code>3</code> 代表 <code>(3, 3)</code> 的方阵。</li>\n<li><code>stride</code> (int or tuple, optional, default=1): 卷积核在图像上滑动的步长。</li>\n<li><code>padding</code> (int or tuple, optional, default=0): 在输入图像的边界周围添加的“填充”层数。<code>padding</code> 对于控制输出特征图的尺寸至关重要，特别是当你想保持输入输出尺寸相同时。</li>\n</ul>\n</li>\n</ul>\n<p><strong>输出尺寸计算公式</strong>:<br><code>Output_size = floor( (Input_size - Kernel_size + 2 * Padding) / Stride ) + 1</code></p>\n</li>\n</ul>\n<p><strong>代码示例</strong>:</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">    # 假设输入是一个 32x32 的 RGB 图像，批次大小为 1\n    # 输入形状: [1, 3, 32, 32] (N, C_in, H, W)\n    input_image = torch.randn(1, 3, 32, 32)\n\n    # 定义一个卷积层\n    # 输入通道为3，输出通道为16（学习16种特征），卷积核大小为3x3，步长为1，填充为1\n    conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n\n    output_feature_map = conv_layer(input_image)\n\n    print(\"卷积层输入尺寸:\", input_image.shape)\n    print(\"卷积层输出尺寸:\", output_feature_map.shape)\n    # 计算: H_out = floor((32 - 3 + 2*1)/1) + 1 = 32. 尺寸不变是因为 padding=1 抵消了 kernel_size=3 的缩小效应。\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>输出：</strong></p>\n<pre><code>卷积层输入尺寸: torch.Size([1, 3, 32, 32])\n卷积层输出尺寸: torch.Size([1, 16, 32, 32])\n</code></pre><p><strong>2. 池化层 (Pooling Layers)</strong></p>\n<ul>\n<li><strong><code>nn.MaxPool2d</code></strong>: 通常跟在卷积层和激活函数之后，用于<strong>降维</strong>和<strong>特征不变性</strong>。它将特征图划分为不重叠的区域，并从每个区域中取出最大值。<ul>\n<li><strong>作用</strong>:<ol>\n<li>减少特征图的空间尺寸，从而减少后续层的参数数量和计算量。</li>\n<li>提供一定程度的<strong>平移不变性</strong>，即目标在图像中轻微移动，池化后的输出仍能保持稳定。</li>\n</ol>\n</li>\n<li><strong>核心参数</strong>:<ul>\n<li><code>kernel_size</code> (int or tuple): 池化窗口的大小。</li>\n<li><code>stride</code> (int or tuple, optional, default=kernel_size): 池化窗口的滑动步长。通常设置为与 <code>kernel_size</code> 相同，以实现不重叠的池化。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>代码示例</strong>:</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">    # 使用上一步的输出作为输入\n    # 输入形状: [1, 16, 32, 32]\n\n    # 定义一个最大池化层\n    # 池化窗口为 2x2，步长为 2\n    pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    pooled_output = pool_layer(output_feature_map)\n\n    print(\"\\n池化层输入尺寸:\", output_feature_map.shape)\n    print(\"池化层输出尺寸:\", pooled_output.shape)\n    # 尺寸减半，因为 kernel_size=2, stride=2\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>输出</strong>：</p>\n<pre><code>池化层输入尺寸: torch.Size([1, 16, 32, 32])\n池化层输出尺寸: torch.Size([1, 16, 16, 16])\n</code></pre><p><strong>3. 线性层 (Linear/Fully-Connected Layers)</strong></p>\n<ul>\n<li><p><strong><code>nn.Linear</code></strong>: 对输入进行线性变换 (<code>y = Wx + b</code>)。它将每个输入神经元连接到每个输出神经元，因此也叫全连接层。通常用在网络的末端，将前面提取的特征整合起来，用于最终的分类或回归。</p>\n<ul>\n<li><strong>核心参数</strong>:<ul>\n<li><code>in_features</code> (int): 每个输入样本的特征数量。</li>\n<li><code>out_features</code> (int): 每个输出样本的特征数量。对于分类任务，这通常是类别的数量。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>与 <code>nn.Flatten</code> 的配合</strong>:<br>卷积/池化层的输出是多维的（如 <code>[N, C, H, W]</code>），而线性层期望的输入是二维的（<code>[N, in_features]</code>）。因此，在从卷积/池化层过渡到线性层之前，必须将特征图“展平”（Flatten）。</p>\n<ul>\n<li><code>nn.Flatten()</code>: 一个方便的层，可以插入到 <code>nn.Sequential</code> 或在 <code>forward</code> 中调用，它会自动将除了批次维度（<code>N</code>）之外的所有维度展平成一个维度。</li>\n<li><code>x.view(-1, num_features)</code>: 一种更手动的展平方式。<code>-1</code> 告诉 PyTorch 自动计算该维度的大小。</li>\n</ul>\n</li>\n</ul>\n<p><strong>代码示例</strong>:</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">    # 使用上一步的池化输出\n    # 输入形状: [1, 16, 16, 16]\n    flattened_output = pooled_output.view(1, -1) # 手动展平\n    # 或者 flattened_output = nn.Flatten()(pooled_output)\n\n    num_features = flattened_output.shape[1] # 1 * 16 * 16 * 16 = 4096\n    print(f\"\\n展平后尺寸: {flattened_output.shape}, 特征数量: {num_features}\")\n\n    # 定义一个线性层，假设我们要进行10分类\n    fc_layer = nn.Linear(in_features=num_features, out_features=10)\n\n    final_output = fc_layer(flattened_output)\n\n    print(\"线性层输出尺寸 (logits):\", final_output.shape)\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>输出</strong>：</p>\n<pre><code>展平后尺寸: torch.Size([1, 4096]), 特征数量: 4096\n线性层输出尺寸 (logits): torch.Size([1, 10])\n</code></pre><p><strong>4. 循环层 (Recurrent Layers)</strong></p>\n<ul>\n<li><strong><code>nn.RNN</code>, <code>nn.LSTM</code>, <code>nn.GRU</code></strong>: 这些是为处理<strong>序列数据</strong>（如文本、语音、时间序列）而设计的。它们的核心思想是拥有一个“记忆”或隐藏状态，可以在处理序列的每个时间步时传递信息。<ul>\n<li><strong><code>nn.LSTM</code> (Long Short-Term Memory)</strong> 是最常用的，因为它通过精巧的“门控”机制（输入门、遗忘门、输出门）有效解决了标准 RNN 的梯度消失/爆炸问题，能够学习长距离依赖。</li>\n<li><strong>输入/输出形状</strong>: 这是一个关键点，也是初学者的常见困惑。PyTorch 中 RNN 层的默认输入形状是 <code>(Sequence_Length, Batch_Size, Input_Size)</code>。<ul>\n<li><code>Sequence_Length</code>: 序列的长度（例如，一个句子中的单词数）。</li>\n<li><code>Batch_Size</code>: 一批处理多少个序列。</li>\n<li><code>Input_Size</code>: 每个时间步的输入特征维度（例如，词嵌入的维度）。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. 辅助层</strong></p>\n<ul>\n<li><p><strong><code>nn.Dropout</code></strong>: 一种非常重要的<strong>正则化</strong>技术，用于防止<strong>过拟合</strong>。</p>\n<ul>\n<li><strong>工作原理</strong>: 在<strong>训练期间</strong>，它会以指定的概率 <code>p</code> 随机地将输入张量中的一部分元素置为零（即“丢弃”神经元），并对剩余元素进行缩放（<code>1/(1-p)</code>），以保持总体的期望值不变。在<strong>评估期间</strong>（调用 <code>model.eval()</code> 后），Dropout 层不起任何作用，直接传递输入。</li>\n<li><strong>作用</strong>: 强迫网络学习冗余的表示，而不是依赖于少数几个特定的神经元，从而提高模型的泛化能力。</li>\n</ul>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># p=0.5 表示每个神经元有 50% 的概率被丢弃\ndropout_layer = nn.Dropout(p=0.5)\n\n# 假设有一个激活后的张量\nactivations = torch.randn(1, 10)\nprint(\"Dropout前:\", activations)\n\n# 在训练模式下应用 Dropout\nmodel_is_training = True\nif model_is_training:\n    output_after_dropout = dropout_layer(activations)\nprint(\"Dropout后 (训练模式):\", output_after_dropout) # 会看到一些元素变为0\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ul>\n<p><strong>输出：</strong></p>\n<pre><code>Dropout前: tensor([[-2.2041,  1.3797,  1.0010, -0.8170,  0.6776, -0.4323,  2.0645,  0.7844,\n          0.7845,  0.8712]])\nDropout后 (训练模式): tensor([[-4.4083,  2.7595,  0.0000, -1.6340,  0.0000, -0.0000,  4.1290,  1.5687,\n          0.0000,  1.7424]])\n</code></pre><h4 id=\"4-3-激活函数-Activation-Functions\"><a href=\"#4-3-激活函数-Activation-Functions\" class=\"headerlink\" title=\"4.3 激活函数 (Activation Functions)\"></a><strong>4.3 激活函数 (Activation Functions)</strong></h4><p><strong>核心作用</strong>：为模型引入<strong>非线性</strong>。如果没有激活函数，一个由多层线性层组成的网络，无论多深，其最终效果都等同于一个单层的线性模型。激活函数打破了这种线性，使得网络能够学习和拟合复杂得多的函数。</p>\n<ul>\n<li><p><strong><code>nn.ReLU</code> (Rectified Linear Unit)</strong>: <code>f(x) = max(0, x)</code></p>\n<ul>\n<li><strong>优点</strong>:<ul>\n<li>计算极其高效。</li>\n<li>在正数区间的梯度为1，有效缓解了梯度消失问题。</li>\n</ul>\n</li>\n<li><strong>缺点</strong>:<ul>\n<li><strong>Dying ReLU 问题</strong>: 如果一个神经元的输入恒为负，那么它的梯度将永远是0，这个神经元将无法再通过梯度下降进行更新。</li>\n</ul>\n</li>\n<li><strong>地位</strong>: 现代深度学习中最常用的激活函数。</li>\n</ul>\n</li>\n<li><p><strong><code>nn.Sigmoid</code></strong>: <code>f(x) = 1 / (1 + exp(-x))</code></p>\n<ul>\n<li><strong>作用</strong>: 将任意实数压缩到 <code>(0, 1)</code> 区间。</li>\n<li><strong>用途</strong>: 常用于二分类问题的输出层，将输出解释为概率。</li>\n<li><strong>缺点</strong>:<ul>\n<li><strong>梯度消失</strong>: 当输入值非常大或非常小时，其导数接近于0，导致梯度在反向传播时迅速减小，难以训练深层网络。</li>\n<li>输出不以0为中心。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong><code>nn.Tanh</code> (Hyperbolic Tangent)</strong>: <code>f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))</code></p>\n<ul>\n<li><strong>作用</strong>: 将任意实数压缩到 <code>(-1, 1)</code> 区间。</li>\n<li><strong>优点</strong>: 输出以0为中心，通常比 Sigmoid 收敛更快。</li>\n<li><strong>缺点</strong>: 仍然存在梯度消失问题。</li>\n</ul>\n</li>\n<li><p><strong><code>nn.Softmax</code></strong>:</p>\n<ul>\n<li><strong>作用</strong>: 将一个向量（通常是网络的原始输出，称为 logits）转换为一个概率分布，其中每个元素都在 <code>(0, 1)</code> 之间，且所有元素之和为1。</li>\n<li><strong>用途</strong>: 专门用于<strong>多分类问题</strong>的输出层。</li>\n<li><strong>注意</strong>: Softmax 通常与交叉熵损失函数配合使用。正如我们将在下一节看到的，<code>nn.CrossEntropyLoss</code> 内部已经包含了 Softmax 操作，所以你<strong>不应该</strong>在模型的最后一层手动添加 <code>nn.Softmax</code>。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"4-4-损失函数-Loss-Functions\"><a href=\"#4-4-损失函数-Loss-Functions\" class=\"headerlink\" title=\"4.4 损失函数 (Loss Functions)\"></a><strong>4.4 损失函数 (Loss Functions)</strong></h4><p><strong>核心作用</strong>：衡量模型预测值 (<code>outputs</code>) 与真实标签 (<code>labels</code>) 之间的差距。这个差距（即损失值）是一个标量，它是反向传播的起点。优化器的目标就是通过调整模型参数来最小化这个损失值。</p>\n<p>在 PyTorch 中，损失函数也是 <code>nn.Module</code> 的子类，通常在使用前实例化。</p>\n<ul>\n<li><p><strong><code>nn.MSELoss</code> (Mean Squared Error Loss)</strong>:</p>\n<ul>\n<li><strong>公式</strong>: <code>Loss = mean((outputs - labels)^2)</code></li>\n<li><strong>用途</strong>: 主要用于<strong>回归任务</strong>，即预测一个连续值（如房价、温度）。</li>\n<li><p><strong>示例</strong>:</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">criterion = nn.MSELoss()\npredicted_prices = torch.tensor([250000.0, 310000.0])\nactual_prices = torch.tensor([260000.0, 300000.0])\nloss = criterion(predicted_prices, actual_prices)\nprint(\"MSE Loss:\", loss.item())\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p><strong><code>nn.BCELoss</code> (Binary Cross Entropy Loss)</strong>:</p>\n<ul>\n<li><strong>用途</strong>: 用于<strong>二分类任务</strong>。</li>\n<li><strong>要求</strong>:<ol>\n<li>模型的输出必须经过 <code>Sigmoid</code> 激活，使其值落在 <code>[0, 1]</code> 区间，代表正类的概率。</li>\n<li>真实标签也应该是 <code>[0, 1]</code> 之间的浮点数。</li>\n</ol>\n</li>\n<li><p><strong>示例</strong>:</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">criterion = nn.BCELoss()\n# 模型输出，已经过 Sigmoid\noutputs = torch.tensor([0.9, 0.2, 0.8]) # 样本1是正类的概率为0.9，...\n# 真实标签\nlabels = torch.tensor([1.0, 0.0, 1.0]) # 样本1是正类，样本2是负类，...\nloss = criterion(outputs, labels)\nprint(\"BCE Loss:\", loss.item())\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n<li><strong><code>nn.BCEWithLogitsLoss</code></strong>: 这是一个更稳定、更推荐的版本，它将 <code>Sigmoid</code> 层和 <code>BCELoss</code> 合二为一。使用它时，你的模型<strong>不需要</strong>在最后一层加 <code>Sigmoid</code>，直接输出原始的 logits 即可。</li>\n</ul>\n</li>\n<li><p><strong><code>nn.CrossEntropyLoss</code></strong>:</p>\n<ul>\n<li><strong>用途</strong>: 用于<strong>多分类任务</strong>。这是最常用的分类损失函数。</li>\n<li><strong>极其重要</strong>: 这个损失函数内部整合了 <code>LogSoftmax</code> 和 <code>NLLLoss</code> (Negative Log Likelihood Loss)。这意味着：<ol>\n<li>你<strong>不应该</strong>在模型的最后一层添加 <code>Softmax</code> 或 <code>LogSoftmax</code> 层。</li>\n<li>模型的输出应该是原始的、未经激活的 <strong>logits</strong>（每个类别的分数）。</li>\n<li>真实标签应该是<strong>类别索引</strong>（<code>LongTensor</code>），而不是 one-hot 编码。</li>\n</ol>\n</li>\n<li><p><strong>示例</strong>:</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\">criterion = nn.CrossEntropyLoss()\n\n# 假设批次大小为3，共有5个类别\n# 模型输出的原始 logits\n# 形状: [batch_size, num_classes]\noutputs = torch.randn(3, 5) \n\n# 真实标签 (类别索引)\n# 形状: [batch_size]\nlabels = torch.tensor([1, 0, 4]) # 第一个样本是第1类，第二个是第0类，...\n\nloss = criterion(outputs, labels)\nprint(\"Cross Entropy Loss:\", loss.item())\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"模块-5：优化器与训练循环-Optimizer-amp-Training-Loop\"><a href=\"#模块-5：优化器与训练循环-Optimizer-amp-Training-Loop\" class=\"headerlink\" title=\"模块 5：优化器与训练循环 (Optimizer & Training Loop)\"></a><strong>模块 5：优化器与训练循环 (Optimizer &amp; Training Loop)</strong></h3><p>我们已经准备好了模型的蓝图 (<code>nn.Module</code>) 和评判标准 (<code>Loss Function</code>)。现在，我们需要一个“工匠”来根据评判结果，实际地去修改模型的参数，让它变得更好。这个工匠就是<strong>优化器 (<code>torch.optim</code>)</strong>。</p>\n<h4 id=\"5-1-优化器-torch-optim\"><a href=\"#5-1-优化器-torch-optim\" class=\"headerlink\" title=\"5.1 优化器 (torch.optim)\"></a><strong>5.1 优化器 (torch.optim)</strong></h4><p><strong>核心角色</strong>：模型参数的“管家”和“更新者”。</p>\n<p>它的工作流程很简单：</p>\n<ol>\n<li>在初始化时，你告诉它要管理哪些参数（通过 <code>model.parameters()</code>）。</li>\n<li>在训练的每一步，当所有参数的梯度（<code>.grad</code>）都已经通过 <code>loss.backward()</code> 计算出来后，你调用优化器的 <code>step()</code> 方法。</li>\n<li><code>step()</code> 方法会根据其内部定义的优化算法（如 SGD、Adam）和计算出的梯度，来更新它所管理的所有参数。</li>\n</ol>\n<p><strong>常用优化器</strong></p>\n<ul>\n<li><p><strong><code>optim.SGD</code> (Stochastic Gradient Descent)</strong>:</p>\n<ul>\n<li>最基础的优化算法。它沿着梯度的反方向更新参数。</li>\n<li><code>param = param - learning_rate * param.grad</code></li>\n<li><strong>关键参数</strong>:<ul>\n<li><code>params</code>: 传入 <code>model.parameters()</code>。</li>\n<li><code>lr</code> (learning rate): 学习率，控制每次更新的步长。这是最重要的超参数。</li>\n<li><code>momentum</code> (动量): 一个非常有用的参数。引入动量可以帮助优化过程：<ol>\n<li><strong>加速收敛</strong>: 如果梯度方向保持一致，动量会累积，使得更新速度加快。</li>\n<li><strong>越过局部最小值</strong>: 就像一个有质量的球滚下山，动量可以帮助它冲过一些小的凹陷（局部最小值）。</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong><code>optim.Adam</code> (Adaptive Moment Estimation)</strong>:</p>\n<ul>\n<li>现代深度学习中最流行和最常用的优化器之一。</li>\n<li>它是一种自适应学习率的算法，会为每个参数维护独立的学习率，并根据梯度的一阶矩（均值）和二阶矩（方差）进行调整。</li>\n<li><strong>优点</strong>: 通常收敛速度比 SGD 快，对学习率的设置不如 SGD 那么敏感，在大多数情况下都能取得很好的效果。是很多任务的“首选默认”优化器。</li>\n<li><strong>关键参数</strong>:<ul>\n<li><code>params</code>, <code>lr</code>: 同 SGD。</li>\n<li><code>betas</code>: 用于计算梯度均值和方差的衰减率，通常使用默认值 <code>(0.9, 0.999)</code>。</li>\n<li><code>eps</code>: 一个很小的数，用于防止分母为零，通常使用默认值 <code>1e-8</code>。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"5-2-构建完整的训练流程-五步法\"><a href=\"#5-2-构建完整的训练流程-五步法\" class=\"headerlink\" title=\"5.2 构建完整的训练流程 - 五步法\"></a><strong>5.2 构建完整的训练流程 - 五步法</strong></h4><p>现在，我们将所有部件组装起来，形成一个完整的训练循环。这个流程是 PyTorch 训练的黄金标准，你必须对每一步都了如指掌。</p>\n<p><strong>准备阶段</strong>: 在任何循环开始之前，我们需要实例化所有必要的组件。</p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 假设我们正在做一个图像分类任务 (e.g., CIFAR-10)\n# 1. 准备数据 (在实际项目中，这里会是 DataLoader)\n# 假设批次大小为 64，图像为 3x32x32，10个类别\ndummy_inputs = torch.randn(64, 3, 32, 32)\ndummy_labels = torch.randint(0, 10, (64,))\n\n# 2. 实例化模型\n# 我们需要一个能处理这种输入的模型\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        # 32x32 -> pool -> 16x16 -> pool -> 8x8\n        # 所以展平后的特征数是 32 * 8 * 8\n        self.fc1 = nn.Linear(32 * 8 * 8, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # 展平所有维度，除了批次维度\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x) # 输出 logits\n        return x\n\nmodel = ConvNet()\n\n# 3. 实例化损失函数\ncriterion = nn.CrossEntropyLoss()\n\n# 4. 实例化优化器\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>训练循环</strong></p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 外层循环 (Epochs): 代表整个数据集被完整训练的轮次\nnum_epochs = 10\nfor epoch in range(num_epochs):\n\n    # 在实际项目中，这里会是: for inputs, labels in train_loader:\n    # 我们用虚拟数据模拟一个批次的处理\n\n    # --- 核心训练五步 ---\n\n    # 1. 清零梯度 (Zero the gradients)\n    # 为什么? PyTorch 的 .backward() 会将新计算的梯度 *累加* 到 .grad 属性中。\n    # 如果不清零，当前批次的梯度会与之前所有批次的梯度叠加，这会导致错误的更新方向。\n    # 所以在每次计算新梯度之前，必须将旧的梯度清零。\n    optimizer.zero_grad()\n\n    # 2. 前向传播 (Forward pass)\n    # 将一批数据输入模型，得到预测结果 (logits)。\n    outputs = model(dummy_inputs)\n\n    # 3. 计算损失 (Compute loss)\n    # 使用损失函数比较预测结果和真实标签，得到一个标量损失值。\n    loss = criterion(outputs, dummy_labels)\n\n    # 4. 反向传播 (Backward pass)\n    # PyTorch 的 Autograd 引擎会从 loss 开始，沿着计算图反向传播，\n    # 自动计算出模型中所有可学习参数 (requires_grad=True) 相对于损失的梯度。\n    # 这些梯度值会被存储在对应参数的 .grad 属性中。\n    loss.backward()\n\n    # 5. 更新权重 (Update weights)\n    # 优化器根据其内部定义的算法 (如 Adam)，使用 .grad 中存储的梯度信息，\n    # 去更新它所管理的所有参数 (在初始化时传入的 model.parameters())。\n    optimizer.step()\n\n    # 打印一些信息来观察训练过程\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>随机输出结果</strong>：</p>\n<pre><code>Epoch [1/10], Loss: 2.2951\nEpoch [2/10], Loss: 2.2638\nEpoch [3/10], Loss: 2.2311\nEpoch [4/10], Loss: 2.1997\nEpoch [5/10], Loss: 2.1695\nEpoch [6/10], Loss: 2.1350\nEpoch [7/10], Loss: 2.0963\nEpoch [8/10], Loss: 2.0478\nEpoch [9/10], Loss: 1.9958\nEpoch [10/10], Loss: 1.9374\n</code></pre><p>这五步——<code>zero_grad()</code>, <code>model()</code>, <code>criterion()</code>, <code>backward()</code>, <code>step()</code>——构成了 PyTorch 训练的核心，你需要将它们刻在脑海里。</p>\n<h4 id=\"5-3-模型评估\"><a href=\"#5-3-模型评估\" class=\"headerlink\" title=\"5.3 模型评估\"></a><strong>5.3 模型评估</strong></h4><p>训练完成后，我们需要在测试集（模型从未见过的数据）上评估其性能。评估过程与训练过程有几个关键区别。</p>\n<p><strong>与训练的区别</strong>：评估的唯一目的是检验模型的性能，这个过程<strong>不应该</strong>更新模型权重，也<strong>不应该</strong>计算梯度。</p>\n<p><strong>两个必须的操作</strong>: <code>model.eval()</code> 和 <code>with torch.no_grad():</code></p>\n<ul>\n<li><p><strong><code>model.eval()</code></strong>:</p>\n<ul>\n<li><strong>必须调用！</strong> 这个方法会将模型切换到“评估模式”。</li>\n<li>这有什么影响？它会关闭像 <code>Dropout</code> 这样的层（因为评估时我们希望使用整个网络的能力），并使像 <code>BatchNorm</code> 这样的层使用在整个训练集上学习到的统计数据，而不是当前批次的统计数据。这确保了评估结果是确定性的和可复现的。</li>\n<li>与之对应的是 <code>model.train()</code>，它会将模型切换回训练模式。在训练循环开始前调用 <code>model.train()</code> 是一个好习惯。</li>\n</ul>\n</li>\n<li><p><strong><code>with torch.no_grad():</code></strong>:</p>\n<ul>\n<li><strong>必须使用！</strong> 这是一个上下文管理器，它会告诉 PyTorch 在这个代码块内<strong>不要计算和存储梯度</strong>。</li>\n<li><strong>好处</strong>:<ol>\n<li><strong>节省大量内存</strong>: PyTorch 在进行前向传播时，如果需要计算梯度，会保存大量的中间激活值，以便在反向传播时使用。<code>no_grad</code> 禁用了这个过程，极大地减少了内存占用。</li>\n<li><strong>加快计算速度</strong>: 省去了构建计算图和梯度计算的开销，使得前向传播更快。</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>评估循环示例</strong></p>\n<pre class=\"line-numbers language-lang-python\"><code class=\"language-lang-python\"># 假设我们有一个测试数据加载器 test_loader\n# 我们用虚拟数据模拟\ndummy_test_inputs = torch.randn(100, 3, 32, 32) # 100个测试样本\ndummy_test_labels = torch.randint(0, 10, (100,))\n\n# 1. 切换到评估模式\nmodel.eval()\n\n# 2. 使用 no_grad 上下文管理器\nwith torch.no_grad():\n    correct = 0\n    total = 0\n\n    # 模拟遍历测试集\n    # 在实际项目中: for images, labels in test_loader:\n    outputs = model(dummy_test_inputs)\n\n    # 计算损失（可选，但通常会监控测试集上的损失）\n    test_loss = criterion(outputs, dummy_test_labels)\n\n    # 计算准确率\n    # outputs 的形状是 [100, 10]，每一行是对应样本在10个类别上的logits\n    # torch.max(outputs.data, 1) 会返回每一行的最大值和其索引\n    # 我们关心的是索引，即模型预测的类别\n    _, predicted = torch.max(outputs.data, 1)\n\n    total += dummy_test_labels.size(0)\n    correct += (predicted == dummy_test_labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f'\\n模型在测试集上的准确率: {accuracy:.2f} %')\nprint(f'模型在测试集上的损失: {test_loss.item():.4f}')\n\n# 如果之后还想继续训练，记得切换回训练模式\nmodel.train()\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>输出：</strong></p>\n<pre><code># 数据随机生成，准确率在10%左右是正常的\n模型在测试集上的准确率: 13.00 %\n模型在测试集上的损失: 2.3072\n</code></pre>","text":"PyTorch 学习笔记 - 阶段二：神经网络的核心在前面的章节中，我们已经熟悉了 PyTorch 的基本数据结构——张量（Tensor），以及如何在它们之上执...","permalink":"/post/PyTorch学习-2-神经网络核心","photos":[],"count_time":{"symbolsCount":"17k","symbolsTime":"15 mins."},"categories":[{"name":"python语法学习","slug":"python语法学习","count":5,"path":"api/categories/python语法学习.json"}],"tags":[{"name":"PyTorch","slug":"PyTorch","count":3,"path":"api/tags/PyTorch.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#PyTorch-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%98%B6%E6%AE%B5%E4%BA%8C%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A0%B8%E5%BF%83\"><span class=\"toc-text\">PyTorch 学习笔记 - 阶段二：神经网络的核心</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9D%97-4%EF%BC%9Atorch-nn-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E2%80%9C%E4%B9%90%E9%AB%98%E2%80%9D\"><span class=\"toc-text\">模块 4：torch.nn - 神经网络的“乐高”</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#4-1-nn-Module-%E6%89%80%E6%9C%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E7%B1%BB\"><span class=\"toc-text\">4.1 nn.Module - 所有模型的基类</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#4-2-%E5%B8%B8%E7%94%A8%E5%B1%82-Layers-%E5%88%86%E7%B1%BB%E4%B8%8E%E7%90%86%E8%A7%A3\"><span class=\"toc-text\">4.2 常用层 (Layers) - 分类与理解</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#4-3-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation-Functions\"><span class=\"toc-text\">4.3 激活函数 (Activation Functions)</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#4-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Loss-Functions\"><span class=\"toc-text\">4.4 损失函数 (Loss Functions)</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9D%97-5%EF%BC%9A%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF-Optimizer-amp-Training-Loop\"><span class=\"toc-text\">模块 5：优化器与训练循环 (Optimizer &amp; Training Loop)</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#5-1-%E4%BC%98%E5%8C%96%E5%99%A8-torch-optim\"><span class=\"toc-text\">5.1 优化器 (torch.optim)</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#5-2-%E6%9E%84%E5%BB%BA%E5%AE%8C%E6%95%B4%E7%9A%84%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B-%E4%BA%94%E6%AD%A5%E6%B3%95\"><span class=\"toc-text\">5.2 构建完整的训练流程 - 五步法</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#5-3-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0\"><span class=\"toc-text\">5.3 模型评估</span></a></li></ol></li></ol></li></ol>","author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"PyTorch学习-3-数据处理&项目实战","uid":"524ccad428f7700ee84d802ab1c86ad8","slug":"PyTorch学习-3-数据处理-项目实战","date":"2025-07-08T05:09:24.000Z","updated":"2025-07-08T06:00:11.038Z","comments":true,"path":"api/articles/PyTorch学习-3-数据处理-项目实战.json","keywords":null,"cover":[],"text":"PyTorch学习-3-数据处理-项目实战模块 6：高效数据加载 (Dataset & DataLoader)6.1 为什么需要它们？想象一下，你有一个包含 5...","permalink":"/post/PyTorch学习-3-数据处理-项目实战","photos":[],"count_time":{"symbolsCount":"11k","symbolsTime":"10 mins."},"categories":[{"name":"python语法学习","slug":"python语法学习","count":5,"path":"api/categories/python语法学习.json"}],"tags":[{"name":"PyTorch","slug":"PyTorch","count":3,"path":"api/tags/PyTorch.json"}],"author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}}},"next_post":{"title":"PyTorch学习-1.基础入门","uid":"4d6d66bf862bc9de2ae8dbbddfd17c18","slug":"PyTorch学习-1-基础入门","date":"2025-07-07T13:25:37.000Z","updated":"2025-07-08T01:47:05.286Z","comments":true,"path":"api/articles/PyTorch学习-1-基础入门.json","keywords":null,"cover":null,"text":"PyTorch 学习笔记 - 阶段一：基础入门模块 1：PyTorch 环境与简介1. 什么是 PyTorch？PyTorch 是一个由 Facebook AI...","permalink":"/post/PyTorch学习-1-基础入门","photos":[],"count_time":{"symbolsCount":"14k","symbolsTime":"13 mins."},"categories":[{"name":"python语法学习","slug":"python语法学习","count":5,"path":"api/categories/python语法学习.json"}],"tags":[{"name":"PyTorch","slug":"PyTorch","count":3,"path":"api/tags/PyTorch.json"}],"author":{"name":"犬夜叉","slug":"blog-author","avatar":"https://i.imgur.com/CrgPA5H_d.png?maxwidth=520&shape=thumb&fidelity=high","link":"/","description":"一位喜欢犬夜叉的多模态大模型研究生","socials":{"github":"https://github.com/ziyi-wang2003","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/qq_62954485?spm=1000.2115.3001.5343","juejin":"","customs":{}}}}}